\input{_headers.tex}

\title{Research Statement}

\author{
  Ryan Giordano \\ \texttt{rgiordan@mit.edu }
}

\begin{document}


\subsection*{Overview}

Deriving scientific information from large, complex datasets can motivate large,
complex statistical models, in fields as diverse as astronomy
\citep{regier:2019:cataloging}, economics \citep{meager:2019:microcredit},
phylogenetics \citep{pritchard:2000:inference}, and many more.  As models grow
in complexity, the need to interrogate their assumptions and to propagate
uncertainty among their components grows, as does the computational
cost of doing so using traditional statistical methods. Many classical
procedures designed to address these concerns, such as Markov Chain Monte Carlo
(MCMC), cross validation, or re-estimating a model under a range of
modeling assumptions, can be prohibitively expensive in many modern problems.

To address this gap, my work employs {\em sensitivity analysis}, applied not
merely in the traditional sense of assessing the risk of to imprecise modeling
assumptions (though I do pursue this traditional role as well), but also to
quantify frequentist sampling properties and propagate uncertainty in Bayesian
procedures. Though conceptually unified, my work is diverse in applications,
touching many of the core activities of modern data analysis, from
cross-validation \citep{giordano:2019:ij, giordano2019:hoij}, scalable Bayesian
posterior inference \citep{giordano:2018:covariances,
giordano:2018:bnpsensitivity}, the bootstrap \citep{giordano:2020:bayesij}, and
more. A recurrent theme of my work is adapting classical {\em theoretical tools}
\citep{reeds:1976:thesis, gustafson:1996:localposterior} to {\em modern
computing environments} equipped with scalable, general purpose automatic
differentiation software \citep{baydin:2015:automatic, carpenter:2015:stanmath}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Data sensitivity: cross validation and frequentist variance}

% Frequentist variability is ultimately concerned with the outcome of an
% estimation procedure if the data were drawn from the same distribution as but
% different from that observed.  Similarly, all forms of cross-validation (CV)
% evaluates a statistic if parts of the observed data had been ablated.  Both of
% these procedures can be treated by sensitivity analysis, where sensitivity
% is to the dataset itself.

\paragraph{Accuracy bounds for approximate cross validation.}

To perform leave-one-out CV (LOO-CV), one re-runs an estimation procedure with
each datapoint left out.  In full, LOO-CV requires as many re-runs procedures as
there are datapoints, and each re-run is expected to be quite close to the
original fit.  Rather than re-running exactly, one can use a Taylor series to
approximate the effect of removing a single data point; since the dataset with
one point left out is, in some sense, ``close'' to the original dataset, the
Taylor series can be expected to perform well.

Prior to our work, this idea had been suggested both in the machine learning
literature \citep{rad:2018:scalableloo, koh:2017:blackbox} as well as in the
classical statistical literature under the name ``infinitesimal jackknife''
\citep{jaeckel:1972:infinitesimal, shao:2012:jackknife}.  However, the machine
learning work developed independently of the statistical precedent, and both
treatments required unrealistic theoretical conditions for the accuracy of the
Taylor series: specifically, that the gradients of the objective function be
uniformly bounded, a condition that is rarely satisfied in scientific practice,
even in the simplest possible example of using maximum likelihood to estimate
the sample mean of a normal distribution.

In \citet{giordano:2019:ij}, we synthesized the classical statistics and machine
learning developments, providing a more realistic set complexity condition under
which the Taylor series is accurate, eschewing the need for bounded gradients.
Unlike previous work, our theory was purely finite sample, implying the
asymptotic results of prior work as a corollary.  We demonstrated the accuracy
of the technique on an unsupervised clustering problem from genomics
\citep{shoemaker:2015:ultrasensitive}.


\paragraph{Sensitivity to removal of a small fraction of the data.} Classical
frequentist standard errors estimate the variability in an estimator that would
result from the rarefied thought experiment of re-sampling datasets from the same
distribution that gave rise to the observed data.  In the social sciences, this
rarefied experiment rarely closely corresponds to reality, and one might be
concerned if substantive conclusions could be overturned by other minor
perturbations to the data.

In \citet{giordano:2020:amip}, we provide an easily-computed approximation to
quantify the effect of ablating a small proportion of a dataset, with
open-source software and finite-sample accuracy bounds for ordinary least
squares and instrumental variables regression. We find that problems with small
signal-to-noise ratio but large datasets will be particularly non-robust to the
removal of a small proportion of the data. Such a situation that obtains
commonly in econometrics, and we find that the sign and statistical significance
of estimated effects in a number of large, prominent econometric studies can be
overturned by dropping only a small number of datapoints
\citep{angelucci:2009:indirect, finkelstein:2012:oregon, meager:2019:microcredit}.


\paragraph{Frequentist variability of Bayesian posteriors.}

Bayesian statistics provides powerful tools for coherently treating uncertainty
in complex problems, though, when the model is misspecified, the estimated
posterior uncertainty may not be meaningful.  In principle, however, one might
always compute the frequentist sampling variability of a Bayesian posterior
quantity, and such a quantity always remains meaningful, even if conceptually
distinct from a posterior uncertainty \citep{waddell:2002:bayesphyloboot,
kleijn:2006:misspecification}.  However, standard tools for evaluating
frequentist uncertainty, such as the bootstrap \citep{huggins:2019:bayesbag},
are extremely computationally intensive, as they typically require re-running an
MCMC procedure hundreds of times.

By combining the frequentist IJ \citep{jaeckel:1972:infinitesimal,
shao:2012:jackknife, giordano:2019:ij} approach to frequentist variance with the
MCMC-based measures of sensitivity \citep{gustafson:2012:localrobustnessbook,
giordano:2018:covariances}, we derive the Bayesian infinitesimal jackknife (IJ),
which can be used to compute the frequentist variability of Bayesian posterior
means without bootstrapping or computing a maximum a-posteriori (MAP) estimate.
In a work in progress \citep{giordano:2020:bayesij}, we extend the Bayesian
central limit theorem of \citet{lehman:1983:pointestimation,
kass:1990:posteriorexpansions} to prove the consistency of the Bayesian IJ and
show its accuracy as an approximation to the bootstrap for a larger number of
examples, effectively allowing estimation of frequentist covariances orders of
magnitude faster than the bootstrap.  We demonstrate the accuracy of our method
on datasets from election modeling \citep{economist:2020:election}, ecology
\citep{kery:2011:bayesian}, and most of the models from \citep{gelman:2006:arm,
stan-examples:2017}.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Sensitivity for Bayesian analysis}

% Bayesian techniques allows analysts to reason coherently about unknown
% parameters, but only if the user specifies a complete generating process for the
% parameters and data, including both prior distributions for the parameters and
% precise likelihoods for the data.  Often, aspects of this model are at best a
% considered simplification, and at worst chosen only for computational
% convenience.  It is critical to ask whether the analysis would have changed
% substantively had different modeling choices been made.

\paragraph{Propagation of uncertainty in scalable Bayesian inference}

One popular technique to scale Bayesian inference to massive problems is mean
field variational Bayes (MFVB) \citep{wainwright:2008:graphical,
blei:2017:variational, regier:2019:cataloging}.  However, MFVB provides
notoriously innacurate posterior uncertainty estimates, even in situations when
it estimates the posterior means accurately. In
\citep{giordano:2018:covariances}, we develop a method to recover accurate
posterior uncertainties from MFVB approximations without needing to fit a more
complex model or run MCMC. Computing the LRVB covariance requires solving a
linear system, which in scientific applications is often sparse and can be
solved using iterative techniques such as conjugate gradient \citep[Chapter
5]{nocedal:2006:numerical}.  We compare LRVB covariances to MCMC on a large
number of real-world datasets, including logistic regression on an internet
advertising dataset \citep{criteo:2014:dataset}, the Cormack-Jolly-Seber model
from ecology \citep{kery:2011:bayesian}, and hierarchical generalized linear
models from the social sciences \citep{gelman:2006:arm}, demonstrating accurate
posterior covariances computed over an order of magnitude faster than MCMC.


\paragraph{Hyperparameter sensitivity for MCMC.}

MCMC is arguably the most commonly used computational tool to estimate Bayesian
posteriors, and modern black-box MCMC tools such as \texttt{Stan} \citep{rstan,
carpenter:2017:stan}.  However, MCMC still often takes a long time to run, and
systematically exploring alternative prior parameterizations by re-running MCMC
would be computationally prohibitive for all but the simplest models. A
classical result from Bayesian robustness states that the sensitivity of a
posterior expectation is given by a particular posterior covariance
\citep{gustafson:1996:localposterior, basu:1996:local}, though the result has
not been widely used, arguably due in part to the lack of an automatic
implementation. In my software package, \citet{giordano:2020:rstansensitivity},
I take advantage of the automatic differentiation capacities of
\texttt{Stan} to provide automatic hyperparameter sensitivity for
generic Stan models.  In examples in the package \texttt{git} repository,
I demonstrate the efficacy of the package in detecting excess prior
sensitivity, particularly in a social sciences model taken from
\citet[Chapter 13.5]{gelman:2006:arm}.


\paragraph{Bayesian nonparametrics.}

A commonly question in unsupervised clustering is how many distinct clusters are
present in a dataset.  Discrete Bayesian nonparametrics (BNP) allows the answer
to be inferred using Bayesian inference, but one must specify a prior on how
distinct clusters are generated \citep{ghosh:2003:bnp,
gershman:2012:bnptutorial}.  A particularly common modeling choice is the
stick-breaking representation of a Dirichlet process prior
\citep{sethuraman:1994:constructivedp}, a mathematical abstraction which is
arguably better justified by its computational convenience than its realism. Our
workshop paper, \citet{giordano:2018:bnpsensitivity}, fits a BNP model with
variational Bayes \citep{blei:2006:dirichletbnp} using the standard,
computationally convenient stick-breaking prior, but then uses sensitivity
analysis to allow the user to explore alternative functional forms an order of
magnitude faster than would be possible with refitting. In work currently in
progress, we apply our method to a human genome dataset in phylogenetics taken
from \citep{huang:2011:haplotype}, and find that our method accurately discovers
real excess prior sensitivity in a BNP version of the model
\texttt{fastSTRUCTURE} \citep{raj:2014:faststructure}.


% \paragraph{Partial pooling in meta-analysis.}
%
% A popular form of meta-analysis in econometrics is to place a hierarchial model
% on a set of related experimental results, which both ``shrinks'' the individual
% estimates towards a common mean, potentially decreasing mean squared error, and
% allowing direct estimation of the average effect and diversity of effects
% \citep{rubin:1981:estimation,gelman:1992:inference}. These advantages come at
% the cost of positing a precise generative process for the effects in question,
% and it is reasonable to interrogate whether the estimation procedure is robust
% to varaibility in these effects.  In \citet{giordano:2016:microcredit}, we apply
% sensitivity analysis to a published meta-analysis of the effectiveness of
% microcredit interventions in seven developing countries
% \citep{meager:2019:microcredit}.  We find that the conclusion are highly
% sensitive to the assumed covariance structure between the base level of business
% profitability and the microcredit effect, a covariance which is {\em a priori}
% difficult to ascertain, automatically diagnosing an important source of
% epistemic uncertainty not captured by the Bayesian posterior.


%
%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\subsection*{Selected Future work}

My research is driven by the needs of my scientific collaborators, and
so my future work will be determined to a large part by my colleagues.
Here, I will discuss a few directions that I find promising and interesting,
and which I believe could be applicable to a diverse set of problems.

\paragraph{Bootstrapping and bootstrapping after bootstrapping.}

We extend the ideas of \citet{giordano:2020:bayesij} to higher-order Taylor
series approximations in \citet{giordano2019:hoij}, provding a family of
estimators which we collectively call the higher-order infintiesimal jackknife
(HOIJ).  In addition to providing higher-quality approximations to CV and
extending our results to k-fold CV, the higher-order approach promises to
provide a scalable alternative to the bootstrap, a procedure that estimates
frequentist variability by repeatedly re-evaluating a model at datasets drawn
with replacement from the observed data. The bootstrap is known to enjoy
higher-order accuracy in certain circumstances \citet{hall:2013:bootstrap}, and
the HOIJ can approach the bootstrap at a rate faster than the bootstrap
approaches the truth.  The HOIJ thus promises to make bootstrap inference
available to models which are differentiable but too expensive to re-evaluate
(e.g. simulation-based models \citep{gourieroux:1993:simulation}), but also to
allow efficient bootstrap-after-bootstrap procedures which that are currently
out of reach for all but the simplest statsitics \citet{efron:1994:bootstrap}.


\paragraph{Preprocessing Sensitivity.}

Analyses in genomics often begin with a pre-processing step in observation units
are clustered together according to ad-hoc measures of similarity across a large
number of feature vectors \citep{xu:2015:identification,
stuart:2019:comprehensive}.  Quickly assessing the sensitivity of these
procedures to the inclusion or exclusion of individual features would allow the
researcher to identify high-leverage observations and avoid imposing structure
via arbitrary modeling assumptions.   Sensitivity anlaysis cannot be applied
directly to such similarity measures, as they are typically non-differentiable.
With a colleague from biology, we are investigating using a probabilisitic
relaxation only of the initial distance measure, drawing random datasets, and
applying the non-differential similarity measure to these random datasets,
taking the average similarity across draws as the output of the procedure. We
can then assess the sensitivity of the original sample's probability to
inclusion or exclusion of particular features, and assess in turn the
sensitivity of the importance sampling estimate of the average output, providing
a sensitivity analysis of the whole non-differentiable procedure.  Conceptually,
approach is attractive because it would allow sensitivity analysis to be applied
to black-box procedures without having to design and validate custom continuous
relaxations.


\paragraph{Partitioned Bayesian inference.}

The ideas of \citep{giordano:2018:covariances} can be naturally extended to
approximately propagate uncertainty amongst separately estimated components of
an inference problem.  For example, astronomical catalogues are customarily
produced with MFVB-like algorithms \citep{lang:2016:tractor,
regier:2019:cataloging}, which take inputs such as the sky background
and optical point spread function as fixed inputs, though these quantities
are themselves inferred with uncertainty.  Viewing all the separate inference
procedures as a sequential quasi-MFVB objective, one could directly apply
the techniques of LRVB to propagate the uncertainty from the modeling inputs
to the astronomical catalogue's uncertainty.  Doing so would require the
approximate solution of a very large, but very sparse, linear system,
which is itself an interesting computational challenge.







\bibliography{references}
\bibliographystyle{plainnat}

\end{document}
