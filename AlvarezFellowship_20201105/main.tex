\input{_headers.tex}

\usepackage{enumitem}
\setlist{nolistsep}

\title{Research Statement}

\author{
  Ryan Giordano \\ \texttt{rgiordan@mit.edu }
}

\begin{document}

%\maketitle
\section*{Research Statement}

As statistical models grow in size and complexity to service modern scientific
datasets and questions, fundamental data science tasks become more
computationally onerous, due to the need of classical procedures to evaluate or
fit a model multiple times.  My research uses {\em sensitivity analysis} to
provide fast, accurate approximations to such fundamental data science tasks,
providing modern theory and practical software, often giving good accuracy and
orders-of-magnitude speedup over classical methods.

Consider the following data science tasks.
%
\begin{itemize}
%
        \item Cross validation (CV) is used ubiquitously in machine learning to
        evaluate model predictive performance and tune hyperparameters, but
        requires fitting a model multiple times with different data subsets left
        out.
        % For example, when clustering the time series profiles of mice gene
        % expression data, one may need to choose how much to smooth the time
        % series prior to clustering, and one would typically choose the smoothing
        % with the best predictive performance as evaluated by CV
        % \citep{friedman:2001:esl, shoemaker:2015:ultrasensitive}.
        %
        \item  Prior specification is a necessary step in Bayesian statistics, a
        statistical paradigm that provides interpretable, coherent uncertainty
        quantification for scientific questions.  But Bayesian inference can be
        sensitive to the prior specification, and evaluating the model for
        multiple plausible prior choices can be computationally prohibitive.
        % For
        % example, one might use Bayesian nonparametrics to ask how many distinct
        % genotypes are found in a human genome dataset
        % \citep{huang:2011:haplotype, raj:2014:faststructure}; forming the
        % estimate for a single prior can take hours.
        %
        \item Uncertainty propagation, i.e., allowing the inferential
        uncertainty in one modeling quantity to inform the inferential
        uncertainty in another, is a key advantage of Bayesian statistics.
        However, the classical tool for Bayesian estimation, Markov Chain Monte
        Carlo (MCMC), requires evaluating a statistical model many times, and so
        can be prohibitively computationally expensive.
        % For example, in the
        % inference of astronomical catalogues, the non-scalability of MCMC leads
        % researchers to turn to scalable ``variational Bayes'' posterior
        % approximations, which are computationally tractable, but known to fail
        % to propagate uncertainty amongst components of the model
        % \citep{turner:2011:two, regier:2019:cataloging}.
%
\end{itemize}
%

% The more complicated a model is, the more likely it is that
% it may overfit the data (remedied by CV), the more difficult it is
% to reason subjectively about the prior (remedied by experimenting with
% a range of priors), and the more likely it is to model multiple, mutually
% dependent, uncertain quantities (remedied by uncertainty propagation).

These three central data science problems may seem superfically distinct.  But
they share the common property that they are computationally demanding due to
requiring the evaluation or estimation of a statistical model multiple times:
once for each draw of an MCMC chain, once for each prior specification, or once
for each cross validation sample.

I show in my work that this commonality implies that they are all amendable to
{\em sensitivity analysis}, in which the evaluation of a model at alternative
inputs is approximated by a Taylor series, evaluated at a single initial model
fit.

My research employs sensitivity analysis to provide accurate approximations to
uncertainty propagation, prior sensitivity, cross validation and other
fundamental data science problems, typically providing good accuracy and
orders-of-magnitude speedups over classical approaches.  A recurrent theme of my
work is adapting venerable classical theoretical tools build on Taylor Series
expansions  \citep{reeds:1976:thesis, gustafson:1996:localposterior,
opper:2001:advancedmeanfield} to {\em modern computing environments} equipped
with scalable, general purpose automatic differentiation software
\citep{baydin:2015:automatic, carpenter:2015:stanmath}.





\newpage

Consider uncertainty propagation in the construction of astronomical catalogues
from telescopic image data. State-of-the art techniques employ a fast Bayesian
posterior approximation known as ``mean-field variational Bayes''
\citep{regier:2019:cataloging} which is known to fail to propagate uncertainty
and underestimate standard errors \citep{turner:2011:two}; full posterior
inference using classical Markov Chain Monte Carlo (MCMC) techniques being too
computationally demanding for such a large-scale problem.

The problem of prior specification is fundamental to Bayesian statistics. The
Bayesian approach provides powerful tools for quanitifying uncertainty of
complex quantities, such as the number of distinct clusters in a human genomics
dataset \citep{huang:2011:haplotype, raj:2014:faststructure}.  However, in
complicated Bayesian models, it can be hard to know {\em a priori} how sensitive
one's conclusions are to one's prior speicification.  When even a single
posterior approximation is extremely expensive, running for a large number of
alternative prior specifications is prohibitive.

The technique of ``cross validation'' is a ubiquitous machine learning tool
for evaluating predictive performance and tuning hyperparameters, but it
requires evaluating one's model at many slightly different datasets, each with
different datapoints left out \citep{barnard:1974:cvchoicediscussion,
friedman:2001:esl}.  For example, in order to select a smoothing parameter in a
preprocessing step for clustering the time series of gene expression levels in
mice \citep{shoemaker:2015:ultrasensitive}, one would typically choose an amount
of smoothing which gives the best predictive performance according to a large
number of CV fits.

These three problems of uncertainty propagation, prior sensitivity, and cross
validation may seem superfically distinct, other than that all are fundamental
to the practice of data science.  But they share the common property that they
are computationally demanding due to requiring the evaluation or estimation of a
statistical model multiple times: once for each draw of an MCMC chain, once for
each prior specification, or once for each cross validation sample. In turn, I
show in my work that this commonality implies that they are all amendable to
{\em sensitivity analysis}, in which the evaluation of a model at alternative
inputs is approximated by a Taylor series, evaluated at a single initial model
fit.

My research employs sensitivity analysis to provide accurate approximations to
uncertainty propagation, prior sensitivity, cross validation and other
fundamental data science problems, typically providing good accuracy and
orders-of-magnitude speedups over classical approaches.  A recurrent theme of my
work is adapting venerable classical theoretical tools build on Taylor Series
expansions  \citep{reeds:1976:thesis, gustafson:1996:localposterior,
opper:2001:advancedmeanfield} to {\em modern computing environments} equipped
with scalable, general purpose automatic differentiation software
\citep{baydin:2015:automatic, carpenter:2015:stanmath}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Data sensitivity: cross validation and frequentist variance}

% Frequentist variability is ultimately concerned with the outcome of an
% estimation procedure if the data were drawn from the same distribution as but
% different from that observed.  Similarly, all forms of cross-validation (CV)
% evaluates a statistic if parts of the observed data had been ablated.  Both of
% these procedures can be treated by sensitivity analysis, where sensitivity
% is to the dataset itself.

\paragraph{Accuracy bounds for approximate cross validation.}

To perform leave-one-out CV (LOO-CV), one re-runs an estimation procedure with
each datapoint left out.  In full, LOO-CV requires as many re-runs procedures as
there are datapoints, and each re-run is expected to be quite close to the
original fit.  Rather than re-running exactly, one can use a Taylor series to
approximate the effect of removing a single data point; since the dataset with
one point left out is, in some sense, ``close'' to the original dataset, the
Taylor series can be expected to perform well.

In \citet{giordano:2019:ij}, we synthesized the classical statistics and machine
learning treatment of this idea \citep{jaeckel:1972:infinitesimal,
shao:2012:jackknife, rad:2018:scalableloo, koh:2017:blackbox}, adapting the
classical theory to the demands of modern machine learning.  In particular, we
provide finite-sample accuracy bounds on the accuracy of the Taylor series (from
which previous asymptotic results followed as a corollary), and remove the
classical assumption of bounded gradients, a condition that almost never obtains
in practice, but which was required by almost all previous theoretical work. We
demonstrated the accuracy of the technique on an unsupervised clustering problem
from genomics \citep{shoemaker:2015:ultrasensitive}.


\paragraph{Sensitivity to removal of a small fraction of the data.} Classical
frequentist standard errors estimate the variability in an estimator that would
result from the rarefied thought experiment of re-sampling datasets from the same
distribution that gave rise to the observed data.  In the social sciences, this
rarefied experiment rarely closely corresponds to reality, and one might be
concerned if substantive conclusions could be overturned by other minor
perturbations to the data.

In \citet{giordano:2020:amip}, we provide an easily-computed approximation to
quantify the effect of ablating a small proportion of a dataset, with
open-source software and finite-sample accuracy bounds for ordinary least
squares and instrumental variables regression. We find that problems with small
signal-to-noise ratio but large datasets will be particularly non-robust to the
removal of a small proportion of the data. Such a situation that obtains
commonly in econometrics, and we find that the sign and statistical significance
of estimated effects in a number of large, prominent econometric studies can be
overturned by dropping only a small number of datapoints
\citep{angelucci:2009:indirect, finkelstein:2012:oregon, meager:2019:microcredit}.


\paragraph{Frequentist variability of Bayesian posteriors.}

Bayesian statistics provides powerful tools for coherently treating uncertainty
in complex problems, though, when the model is misspecified, the estimated
posterior uncertainty may not be meaningful.  In principle, however, one might
always compute the frequentist sampling variability of a Bayesian posterior
quantity, and such a quantity always remains meaningful, even if conceptually
distinct from a posterior uncertainty \citep{waddell:2002:bayesphyloboot,
kleijn:2006:misspecification}.  However, standard tools for evaluating
frequentist uncertainty, such as the bootstrap \citep{huggins:2019:bayesbag},
are extremely computationally intensive, as they typically require re-running an
MCMC procedure hundreds of times.

In a work in progress \citep{giordano:2020:bayesij}, we derive the Bayesian
infinitesimal jackknife (IJ), which we prove can be used to consistently
estimate the frequentist variability of Bayesian posterior means without
bootstrapping or computing a maximum a-posteriori (MAP) estimate.  Our work
synthesizes results from Bayesian robustness and frequentist von Mises
expansions and extends the Bayesian central limit theorem to the expectation of
data-dependent functions
\citep{jaeckel:1972:infinitesimal,shao:2012:jackknife,giordano:2019:ij,gustafson:2012:localrobustnessbook,giordano:2018:covariances,
lehman:1983:pointestimation, kass:1990:posteriorexpansions}. We demonstrate the
accuracy of the Bayesian IJ on datasets from election modeling
\citep{economist:2020:election}, ecology \citep{kery:2011:bayesian}, and most of
the models from \citep{gelman:2006:arm, stan-examples:2017}, showing that the
Bayesian IJ can reproduce the bootstrap covariance estimates in orders of
magnitude less compute time.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Sensitivity for Bayesian analysis}

% Bayesian techniques allows analysts to reason coherently about unknown
% parameters, but only if the user specifies a complete generating process for the
% parameters and data, including both prior distributions for the parameters and
% precise likelihoods for the data.  Often, aspects of this model are at best a
% considered simplification, and at worst chosen only for computational
% convenience.  It is critical to ask whether the analysis would have changed
% substantively had different modeling choices been made.

\paragraph{Propagation of uncertainty in scalable Bayesian inference}

One popular technique to scale Bayesian inference to massive problems is mean
field variational Bayes (MFVB) \citep{wainwright:2008:graphical,
blei:2017:variational, regier:2019:cataloging}.  However, MFVB provides
notoriously innacurate posterior uncertainty estimates, even in situations when
it estimates the posterior means accurately \citep{turner:2011:two}. In
\citet{giordano:2018:covariances}, we develop a method to recover accurate
posterior uncertainties from MFVB approximations without needing to fit a more
complex model or run MCMC. Computing the LRVB covariance requires solving a
linear system, which in scientific applications is often sparse and can be
solved using iterative techniques such as conjugate gradient \citep[Chapter
5]{nocedal:2006:numerical}.

We compare LRVB covariances to MCMC on a large number of real-world datasets,
including logistic regression on an internet advertising dataset
\citep{criteo:2014:dataset}, the Cormack-Jolly-Seber model from ecology
\citep{kery:2011:bayesian}, and hierarchical generalized linear models from the
social sciences \citep{gelman:2006:arm}, demonstrating accurate posterior
covariances computed over an order of magnitude faster than MCMC.


\paragraph{Hyperparameter sensitivity for MCMC.}

MCMC is arguably the most commonly used computational tool to estimate Bayesian
posteriors, and modern black-box MCMC tools such as \texttt{Stan} \citep{rstan,
carpenter:2017:stan}.  However, MCMC is typically still time-consuming, and
systematically exploring alternative prior parameterizations by re-running MCMC
would be computationally prohibitive for all but the simplest models. A
classical result from Bayesian robustness states that the sensitivity of a
posterior expectation is given by a particular posterior covariance
\citep{gustafson:1996:localposterior, basu:1996:local}, though the result has
not been widely used, arguably due in part to the lack of an automatic
implementation. In my software package, \citet{giordano:2020:rstansensitivity},
I take advantage of the automatic differentiation capacities of \texttt{Stan} to
provide automatic hyperparameter sensitivity for generic Stan models.  In
examples in the package \texttt{git} repository, I demonstrate the efficacy of
the package in detecting excess prior sensitivity, particularly in a social
sciences model taken from \citet[Chapter 13.5]{gelman:2006:arm}.


\paragraph{Bayesian nonparametrics.}

A commonly question in unsupervised clustering is how many distinct clusters are
present in a dataset.  Discrete Bayesian nonparametrics (BNP) allows the answer
to be inferred using Bayesian inference, but one must specify a prior on how
distinct clusters are generated \citep{ghosh:2003:bnp,
gershman:2012:bnptutorial}.  A particularly common modeling choice is the
stick-breaking representation of a Dirichlet process prior
\citep{sethuraman:1994:constructivedp}, a mathematical abstraction which is
arguably better justified by its computational convenience than its realism.

In \citet{giordano:2018:bnpsensitivity}, we fit a BNP model with variational
Bayes \citep{blei:2006:dirichletbnp} using the standard, computationally
convenient stick-breaking prior, but then use sensitivity analysis to allow the
user to explore alternative functional forms an order of magnitude faster than
would be possible with refitting. In work currently in progress, we apply our
method to a human genome dataset in phylogenetics taken from
\citep{huang:2011:haplotype}, and find that our method accurately discovers
meaningful prior sensitivity in a BNP version of the model
\texttt{fastSTRUCTURE} \citep{raj:2014:faststructure}.


% \paragraph{Partial pooling in meta-analysis.}
%
% A popular form of meta-analysis in econometrics is to place a hierarchial model
% on a set of related experimental results, which both ``shrinks'' the individual
% estimates towards a common mean, potentially decreasing mean squared error, and
% allowing direct estimation of the average effect and diversity of effects
% \citep{rubin:1981:estimation,gelman:1992:inference}. These advantages come at
% the cost of positing a precise generative process for the effects in question,
% and it is reasonable to interrogate whether the estimation procedure is robust
% to varaibility in these effects.  In \citet{giordano:2016:microcredit}, we apply
% sensitivity analysis to a published meta-analysis of the effectiveness of
% microcredit interventions in seven developing countries
% \citep{meager:2019:microcredit}.  We find that the conclusion are highly
% sensitive to the assumed covariance structure between the base level of business
% profitability and the microcredit effect, a covariance which is {\em a priori}
% difficult to ascertain, automatically diagnosing an important source of
% epistemic uncertainty not captured by the Bayesian posterior.


%
%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\subsection*{Selected Future work}

My research is driven by the needs of my scientific collaborators, and
so my future work will be determined to a large part by my colleagues.
Here, I will discuss a few directions that I find promising and interesting,
and which I believe could be applicable to a diverse set of problems.

\paragraph{The higher-order infinitesimal jackknife for the bootstrap.}

In the preprint \citet{giordano2019:hoij}, we extend
\citet{giordano:2019:ij} to higher-order Taylor series approximations,
provding a family of estimators which we collectively call the higher-order
infintiesimal jackknife (HOIJ).  In addition to providing higher-quality
approximations to CV and extending our results to k-fold CV, the higher-order
approach promises to provide a scalable alternative to the bootstrap, a
procedure that estimates frequentist variability by repeatedly re-evaluating a
model at datasets drawn with replacement from the observed data. The bootstrap
is known to enjoy higher-order accuracy in certain circumstances
\citet{hall:2013:bootstrap}, and the HOIJ can approach the bootstrap at a rate
faster than the bootstrap approaches the truth.  The HOIJ thus promises to make
bootstrap inference available to models which are differentiable but too
expensive to re-evaluate (e.g. simulation-based models
\citep{gourieroux:1993:simulation}), but also to allow efficient
bootstrap-after-bootstrap procedures which that are currently out of reach for
all but the simplest statsitics \citep{efron:1994:bootstrap}.

% \paragraph{Preprocessing sensitivity.}
%
% Analyses in genomics often begin with a pre-processing step in observation units
% are clustered together according to ad-hoc measures of similarity across a large
% number of feature vectors \citep{xu:2015:identification,
% stuart:2019:comprehensive}.  Quickly assessing the sensitivity of these
% procedures to the inclusion or exclusion of individual features would allow the
% researcher to identify high-leverage observations and avoid imposing structure
% via arbitrary modeling assumptions.   Sensitivity anlaysis cannot be applied
% directly to such similarity measures, as they are typically non-differentiable.
% With a colleague from biology, I am investigating using a probabilisitic
% relaxation only of the initial distance measure, drawing random datasets, and
% applying the non-differential similarity measure to these random datasets,
% taking the average similarity across draws as the output of the procedure. We
% can then assess the sensitivity of the original sample's probability to
% inclusion or exclusion of particular features, and assess in turn the
% sensitivity of the importance sampling estimate of the average output, providing
% a sensitivity analysis of the whole non-differentiable procedure.  Conceptually,
% this approach is attractive because it would allow sensitivity analysis to be
% applied to black-box procedures without having to design and validate custom
% continuous relaxations.

\paragraph{Partitioned Bayesian inference.}

The ideas of \citep{giordano:2018:covariances} can be naturally extended to
approximately propagate uncertainty amongst separately estimated components of
an inference problem.  For example, astronomical catalogues are customarily
produced with MFVB-like algorithms \citep{lang:2016:tractor,
regier:2019:cataloging}, which take inputs such as the sky background
and optical point spread function as fixed inputs, though these quantities
are themselves inferred with uncertainty.  Viewing all the separate inference
procedures as a sequential quasi-MFVB objective, one could directly apply
the techniques of LRVB to propagate the uncertainty from the modeling inputs
to the astronomical catalogue's uncertainty.  Doing so would require the
approximate solution of a very large, but very sparse, linear system,
which is itself an interesting computational challenge.







\bibliography{references}
\bibliographystyle{plainnat}

\end{document}
