\input{_headers.tex}

\title{Research Statement}

\author{
  Ryan Giordano \\ \texttt{rgiordan@mit.edu }
}

\begin{document}


\subsection*{Overview}

Deriving scientific information from large, complex datasets can motivate large,
complex statistical models, in fields as diverse as astronomy
\citep{regier:2019:cataloging}, economics \citep{meager:2019:microcredit},
phylogenetics \citep{pritchard:2000:inference}, and many more.  As models grow
in complexity, the need to interrogate their assumptions and to propagate
uncertainty amongst their components grows, as does the computational
cost of doing so using traditional statistical methods. Many classical
procedures designed to address these concerns, such as Markov Chain Monte Carlo
(MCMC), cross validation, or re-estimating a model under a range of
modeling assumptions, can be prohibitively expensive in many modern problems.

To address this gap, my work employs {\em sensitivity analysis}, applied not
merely in the traditional sense of assessing the risk of to imprecise modeling
assumptions (though I do pursue this traditional role as well), but also to
quantify freqeuntist sampling properties and propagate uncertainty in Bayesian
procedures. Though conceptually unified, my work is diverse in applications,
touching many of the core activities of modern data analysis, from
cross-validation \citep{giordano:2019:ij, giordano2019:hoij}, scalable Bayesian
posterior inference \citep{giordano:2018:covariances,
giordano:2018:bnpsensitivity}, the bootstrap \citep{giordano:2020:bayesij}, and
more. A recurrent theme of my work is adapting classical {\em theoretical tools}
\citep{reeds:1976:thesis, gustafson:1996:localposterior} to {\em modern
computing environments} equipped with scalable, general purpose automatic
differentiation software \citep{baydin:2015:automatic, carpenter:2015:stanmath}.

%   At its core, my methodological work is all
% based on using Taylor series approximations, constructed only from properties of
% a single model fit, computed using either optimization or MCMC, to extrapolate
% to alternatives.
% In my work, provide practical
% approximations to Bayesian posterior uncertainty, CV loss, and model
% sensitivity, all while avoiding expensive model re-estimation.
% Sensitivity analysis is a venerable idea with a rich existing
% literature, and I show that it has wide-ranging and fundamental applications in
% modern, computationally intensive statistical problems.
% Furthermore, I motivate the re-examination of theoretical ideas concerning the
% role of differential approximations in statistical analysis as practical tools,
% particularly given the promulgation of scalable, general purpose automatic
% differentiation software \citep{baydin:2015:automatic, carpenter:2015:stanmath}.

% I will divide my research statement into three parts: first, I will cover a more
% traditional application of sensitivity analysis to the assessment of prior
% sensitivity in Bayesian and variational Bayesian analysis.  I will then discuss
% approximate CV and bootstrapping, which can be viewed as a kind of sensitivity
% analysis.  Finally, I discuss how sensitivity analysis can recover accurate
% posterior covariances from variational Bayesian approximations, tying sensitiity
% analysis to the uniquitous scientific goal of uncertainty propagation.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Data sensitivity: cross validation and frequentist variance}

Frequentist variability is ultimately concerned with the outcome of an
estimation procedure if the data were drawn from the same distribution as but
different from that observed.  Similarly, all forms of cross-validation (CV)
evaluates a statistic if parts of the observed data had been ablated.  Both of
these procedures can be treated by sensitivity analysis, where sensitivity
is to the dataset itself.

\paragraph{Accuracy bounds for approximate cross validation.}

To perform leave-one-out CV (LOO-CV), one re-runs an estimation procedure with
each datapoint left out.  In full, LOO-CV requires as many re-runs procedures as
there are datapoints, and each re-run is expected to be quite close to the
original fit.  Rather than re-running exactly, one can use a Taylor series to
approximate the effect of removing a single data point; since the dataset with
one point left out is, in some sense, ``close'' to the original dataset, the
Taylor series can be expected to perform well.

Prior to our work, this idea had been suggested both in the machine learning
literature \citep{rad:2018:scalableloo, koh:2017:blackbox} as well as in the
classical statistical literature under the name ``infinitesimal jackknife''
\citep{jaeckel:1972:infinitesimal, shao:2012:jackknife}.  However, the ML work
appeared unaware of the statisical precedent, and both treatments required
unrealistic theoretical conditions for the accuracy of the Taylor series:
specifically, that the gradients of the objective function be uniformly bounded,
a condition that is rarely satisfied in scientific practice, even in the
simplest possible example of using maximum likelihood to estimate the sample
mean of a normal distribution.

In \citet{giordano:2019:ij}, we provide a more realistic set complexity
condition under which the Taylor series is accurate, eschewing the need for
bounded gradients, and synthesizing the classical statistics and ML literatures.
Also unlike previous work, our theory was purely finite sample, implying the
asymptotica results of prior work as a corollary.  We demonstrated the accuracy
of the technique on an unsupervised clustering problem from genomics
\citep{shoemaker:2015:ultrasensitive}.


\paragraph{Sensitivity to removal of a small fraction of the data.} Classical
frequentist standard errors estimate the variability in an estimator that would
result from the rarified thought experiment of re-sampling datsets from the same
distribution that gave rise to the observed data.  In the social sciences, this
rarefied experiment rarely closely corresponds to reality, and one might be
concerned if substantive conclusions could be overtuned by other minor
perturbations to the data.  For example, if a top-line conclusion of a study of
the efficacy of cash transfers in a particular country
\citep{angelucci:2009:indirect} could be reversed by removing a small percentage
(say, $0.1\%$) of a dataset, one might hesitate to generalize one's conclusions
to other countries, even if the result was ``statistically significant''
according to classical frequentist standard errors.

In \citet{giordano:2020:amip}, we address this fundamental question, extending
our earlier results in \citet{giordano:2019:ij}.  We find that problems with
small signal-to-noise ratio but large datasets will be particularly non-robust
to the removal of a small proportion of the data. Such a situation that obtains
commonly in econometrics, and we find that the sign and statistical significance
of estimated effects in a number of large, prominent econometric studies can be
overturned by dropping only a small number of datapoints
\citep{angelucci:2009:indirect, finkelstein:2012:oregon}.  Our robustness metric
can be computed easily and automatically for any Z-estimator; we provide
software and tractable finite-sample accuracy bounds for ordinary least squares
and instrumental variables regression.  More broadly, our work points to the
importance of considering ``practical significance'' of effects in the social
sciences rather than mere statistical significance.


\paragraph{Frequentist properties of Bayesian posteriors}

Bayesian measures is a powerful tool for coherently treating uncertainty in
complex problems, but, when the model is misspecified, the estimated  posterior
uncertainty may not be meaningful.  One can, however, always compute the
frequentist sampling variability of a Bayesian posterior quantity, and such a
quantity always remains meaningful, even if conceptually distinct from a
posterior uncertainty \citet{waddell:2002:bayesphyloboot,
kleijn:2006:misspecification, huggins:2019:bayesbag}.

By combining the frequentist IJ \citep{jaeckel:1972:infinitesimal,
shao:2012:jackknife, giordano:2019:ij} approach to frequentist variance with the
MCMC-based measures of sensitivity \citep{gustafson:2012:localrobustnessbook,
giordano:2018:covariances}, we are able to derive the Bayesian infinitesimal
jackknife (IJ), which can be used to compute the frequentist variability of
Bayeisan posterior means without bootstrapping or computing a maximum
a-posteriori (MAP) estimate.  In a work in progress, we extend the Bayesian
central limit theorem of \citet{lehman:1983:pointestimation,
kass:1990:posteriorexpansions} to prove the consistency of the Bayesian IJ and
show its accuracy as an approximation to the bootstrap for a larger number of
examples, effectively allowing estimation of frequentist covariances orders of
magnitude faster than the bootstrap.  We demonstrate the accuracy of our method
on datasets from election modeling \citep{economist:2020:election}, ecology \citep{kery:2011:bayesian}, and most of
the models from \citep{gelman:2006:arm, stan-examples:2017}.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Propagation of uncertainty in scalable Bayesian inference}

One popular technique to scale Bayesian inference to massive problems is mean
field variational Bayes (MFVB) \citep{wainwright:2008:graphical,
blei:2017:variational, regier:2019:cataloging}.  However, MFVB provides
notoriously innacurate posterior uncertainty estimates, even in situations when
it estimates the posterior means accurately. In
\citep{giordano:2018:covariances}, we develop a method to recover accurate
posterior uncertainties from MFVB approximations without needing to fit a more
complex model, or indeed to re-fit the original model. Computing the LRVB
covaraince requires solving a linear system, which in scientific applications is
often sparse and can be solved using iterative techniques such as conjugate
gradient \citep[Chapter 5]{nocedal:2006:numerical}.  We compare LRVB covariances
to MCMC on a large number of real-world datasets, including logistic regression
on an internet advertising dataset \citep{criteo:2014:dataset}, the
Cormack-Jolly-Seber model from ecology \citep{kery:2011:bayesian}, and
hierarchical generalized linear models from the social sciences
\citep{gelman:2006:arm}, demonstrating accurate posterior covariances computed
over an order of magnitude faster than MCMC.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Prior sensitivity in Bayesian analysis}

Bayesian techniques allows analysts to reason coherently about unknown
parameters, but only if the user specifies a complete generating process for the
parameters and data, including both prior distributions for the parameters and
precise likelihoods for the data.  Often, aspects of this model are at best a
considered simplifcation, and at worst chosen only for computational
convenience.  It is critical to ask whether the analysis would have changed
substantively had different modeling choices been made.

\paragraph{Bayesian nonparametrics.}

A commonly question in unsupervised clustering is how many distinct clusters are
present in a dataset.  Discrete Bayesian nonparametrics (BNP) allows the answer
to be inferred using Bayesian inference, but one must specify a prior on how
distinct clusters are generated \citep{ghosh:2003:bnp,
gershman:2012:bnptutorial}.  A particularly common modeling choice is the
stick-breaking representation of a Dirichlet process prior
\citep{sethuraman:1994:constructivedp}, a mathematical abstraction which is
arguably better justified by its computational convenience than its realism. Our
workshop paper, \citet{giordano:2018:bnpsensitivity}, fits a BNP model with
variational Bayes \citep{blei:2006:dirichletbnp} using the standard,
computationally convenient stick-breaking prior, but then uses sensitivity
analysis to allow the user to explore alternative functional forms an order of
magnitude faster than would be possible with refitting. In work currently in
progress, we apply our method to a human genome dataset in phylogenetics taken
from \citep{huang:2011:haplotype}, and find that our method accurately discovers
real excess prior sensitivity in a BNP version of the model
\texttt{fastSTRUCTURE} \citep{raj:2014:faststructure}.


\paragraph{Partial pooling in meta-analysis.}

A popular form of meta-analysis in econometrics is to place a hierarchial model
on a set of related experimental results, which both ``shrinks'' the individual
estimates towards a common mean, potentially decreasing mean squared error, and
allowing direct estimation of the average effect and diversity of effects
\citep{rubin:1981:estimation,gelman:1992:inference}. These advantages come at
the cost of positing a precise generative process for the effects in question,
and it is reasonable to interrogate whether the estimation procedure is robust
to varaibility in these effects.  In \citet{giordano:2016:microcredit}, we apply
sensitivity analysis to a published meta-analysis of the effectiveness of
microcredit interventions in seven developing countries
\citep{meager:2019:microcredit}.  We find that the conclusion are highly
sensitive to the assumed covariance structure between the base level of business
profitability and the microcredit effect, a covariance which is {\em a priori}
difficult to ascertain, automatically diagnosing an important source of
epistemic uncertainty not captured by the Bayesian posterior.


\paragraph{Hyperparameter sensitivity for MCMC.}

MCMC is arguably the most commonly used computational tool to estimate Bayesian
posteriors, and modern black-box MCMC tools such as \texttt{Stan} \citep{rstan,
carpenter:2017:stan}.  However, MCMC still often takes a long time to run, and
systematically exploring alternative prior parameterizations by re-running MCMC
would be computationally prohibitive for all but the simplest models. A
classical result from Bayesian robustness states that the sensitivity of a
posterior expectation is given by a particular posterior covariance
\citep{gustafson:1996:localposterior, basu:1996:local}, though the result has
not been widely used, arguably due in part to the lack of an automatic
implementation. In my software package, \citet{giordano:2020:rstansensitivity},
I take advantage of the automatic differentiation capacities of
\texttt{Stan} to provide automatic hyperparameter sensitivity for
generic Stan models.  In examples in the package \texttt{git} repository,
I demonstrate the efficacy of the package in detecting excess prior
sensitivity, particularly in a social sciences model taken from
\citet[Chapter 13.5]{gelman:2006:arm}.
%
%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% \subsection*{Selected Future work}
%
% My future work will be
%
% \paragraph{Bootstrapping simulation-based inference.}
%
%
% \citet{giordano2019:hoij}
% \citet{efron:1994:bootstrap}
% \citet{hall:2013:bootstrap}
%
%
% \paragraph{Preprocessing Sensitivity.}
%
% Our work on the higher-order infinitesimal jackknife could be applied to other
% random reweighting schemes, particularly the bootstrap.  The bootstrap is known
% to have frequentist properties that are asymptotically more accurate than the
% normal approximation in certain circumstances CITE, but the bootstrap requires
% re-computing an estimator as many time as there are bootstrap samples. However,
% a sufficiently high-order IJ estimate will approach the bootstrap estimator at a
% rate faster than the bootstrap's extra accuracy, strongly suggesting that the IJ
% will inherit all of the bootstrap's attractive properties at a fraction of the
% computational cost.  The IJ is particularly appealing for
% bootstrap-after-bootstrap procedures, which have attractive theoretical
% properties but are computationally prohibitive even on medium-sized statistical
% problems.  It seems plausible that the HOIJ could open up a range of bootstrap
% applications that are presently out of reach.
%
%
% \paragraph{Partitioned Bayesian inference with theoretical bounds.}
%
% Given a large, complicated problem, it is often computationally convenient to
% perform inference in separate computatoinal steps, while still propagating
% uncertainty from one step to another.  For example, in CITE, we first fit spline
% regressions to time series of gene expression data, and then clustered the
% spline fits to group together genes with similar behavior.  The Bayesian ideal,
% which is the simultaneous estimation of the clusters and spline regression, was
% too computationally prohibitive, and would intuitively have given a similar
% result to the sequential analysis, to the extent that the cluster center priors
% did not shrink the spline regressions too much.
%




\bibliography{references}
\bibliographystyle{plainnat}

\end{document}
