\input{_headers.tex}

\usepackage{enumitem}
\setlist{nolistsep}

\title{Research Statement}

\author{
  Ryan Giordano \\ \texttt{rgiordan@mit.edu }
}

\begin{document}

%\maketitle
\section*{Research Statement}

As statistical models grow in size and complexity to serve modern scientific
datasets and questions, fundamental data science tasks become more
computationally onerous due to the need of many classical procedures to evaluate
or fit a model multiple times.  My research uses {\em sensitivity analysis} to
provide fast, accurate approximations to such fundamental data science tasks,
often exhibiting good accuracy and orders-of-magnitude speedup over classical
methods.

Consider, as motivating examples, the following ubiquitous data science tasks.
%
\begin{itemize}
%
        \item Cross validation (CV) is a fundamental tool in machine learning to
        evaluate model predictive performance and tune hyperparameters, but
        requires fitting a model multiple times with different data subsets left
        out.
        % For example, when clustering the time series profiles of mice gene
        % expression data, one may need to choose how much to smooth the time
        % series prior to clustering, and one would typically choose the smoothing
        % with the best predictive performance as evaluated by CV
        % \citep{friedman:2001:esl, shoemaker:2015:ultrasensitive}.
        %
        \item  Prior specification is a necessary step in Bayesian statistics, a
        statistical paradigm that provides interpretable, coherent uncertainty
        quantification for scientific questions.  But Bayesian inference can be
        sensitive to the prior specification, and evaluating the model for
        multiple plausible prior choices can be computationally prohibitive.
        % For
        % example, one might use Bayesian nonparametrics to ask how many distinct
        % genotypes are found in a human genome dataset
        % \citep{huang:2011:haplotype, raj:2014:faststructure}; forming the
        % estimate for a single prior can take hours.
        %
        \item Uncertainty propagation, i.e., allowing the inferential
        uncertainty in one modeling quantity to inform the inferential
        uncertainty in another, is a key advantage of Bayesian statistics.
        However, the classical tool for Bayesian estimation, Markov Chain Monte
        Carlo (MCMC), requires evaluating a statistical model many times, and so
        can be computationally expensive.
        % For example, in the
        % inference of astronomical catalogues, the non-scalability of MCMC leads
        % researchers to turn to scalable ``variational Bayes'' posterior
        % approximations, which are computationally tractable, but known to fail
        % to propagate uncertainty amongst components of the model
        % \citep{turner:2011:two, regier:2019:cataloging}.
%
\end{itemize}
%

% The more complicated a model is, the more likely it is that
% it may overfit the data (remedied by CV), the more difficult it is
% to reason subjectively about the prior (remedied by experimenting with
% a range of priors), and the more likely it is to model multiple, mutually
% dependent, uncertain quantities (remedied by uncertainty propagation).

These three central data science problems may seem superfically distinct.  But
they share the common property that they are computationally demanding due to
requiring the evaluation or estimation of a statistical model multiple times:
once for each cross validation sample, once for each prior specification, or
once for each draw of an MCMC chain.

I show that this commonality implies that all these tasks are amendable to {\em
sensitivity analysis}, in which the evaluation of a model at alternative inputs
is approximated by a Taylor series.  By evaluating the derivatives necessary to
form the Taylor series at a {\em single model estimate}, I avoid the
re-estimation or re-evaluation that makes the above procedures computationally
prohibitive.  In exchange, evaluating the necessary derivatives often requires
solving a large but sparse linear system, a tradeoff that can be quite favorable
in practice.

Sensitivity analysis is a venerable topic, though the breadth of its
applications to conteporary problems is arguably underappreciated.  My work
advances existing work by providing practical implementations of classical
methods, particularly using automatic differentiation
\citep{baydin:2015:automatic}, by updating classical theory to apply in finite
sample and under more realistic conditions, and by drawing connections between
superifically disparate applications of sensitivity analysis. For the remainder
of the essay, I will discuss in more detail how I apply sensitivity analysis to
the above three data science tasks and more, both in practice and and in theory.




\newpage

\paragraph{Approximate cross validation.}

The error or variability of machine learning algorithms is often assessed by
repeatedly re-fitting a model with different weighted versions of the observed
data; cross-validation (CV) and the bootstrap can be thought of as examples of
this technique.

In \citet{giordano:2019:ij}, I use a linear approximation to the
dependence of the fitting procedure on the weights, producing results that can
be faster than repeated re-fitting by an order of magnitude. I provide explicit
finite-sample error bounds for the approximation in terms of a small number of
simple, verifiable assumptions.  My results apply whether the weights and data
are stochastic or deterministic, and so can be used as a tool for proving the
accuracy of the infinitesimal jackknife on a wide variety of problems. As a
corollary, I state mild regularity conditions under which our approximation
consistently estimates true leave-$k$-out cross-validation for any fixed $k$. I
demonstrate the accuracy of the approximation on a range of simulated and real
datasets, including an unsupervised clustering problem from genomics
\citep{Luan:2003:clustering, shoemaker:2015:ultrasensitive}.


\paragraph{Prior sensitivity for Markov Chain Monte Carlo.}

MCMC is arguably the most commonly used computational tool to estimate Bayesian
posteriors, which is made still easier by modern black-box MCMC tools such as
\texttt{Stan} \citep{carpenter:2017:stan, rstan}.  However, a single run of MCMC
typically remains time-consuming, and systematically exploring alternative prior
parameterizations by re-running MCMC would be computationally prohibitive for
all but the simplest models.

My software package, \texttt{rstansensitivity},
\citep{giordano:2020:rstansensitivity, giordano:2018:mcmchyper}, takes advantage
of the automatic differentiation capacities of \texttt{Stan}
\citep{carpenter:2015:stanmath} together with a classical result from  Bayesian
robustness \citep{gustafson:1996:localposterior, basu:1996:local,
giordano:2018:covariances} to provide automatic hyperparameter sensitivity for
generic \texttt{Stan} models from only a single MCMC run.  I demonstrate the
speed and utility of the package in detecting excess prior sensitivity,
particularly in a social sciences model taken from \citet[Chapter
13.5]{gelman:2006:arm}.


\paragraph{Prior sensitivity for discrete Bayesian nonparameterics.}

% From BNP_sensitivity/writing/NIPS_2018_BNP_workshop
A central question in many probabilistic clustering problems is how many
distinct clusters are present in a particular dataset. A Bayesian nonparametric
(BNP) model addresses this question by placing a generative process on cluster
assignment, making the number of distinct clusters present amenable to Bayesian
inference.  However, like all Bayesian approaches, BNP requires the
specification of a prior, and this prior may favor a greater or lesser number of
distinct clusters.
% In practice, it is important to quantitatively establish that
% the prior is not too informative, particularly when---as is often the case in
% BNP---the particular form of the prior is chosen for mathematical convenience
% rather than because of a considered subjective belief.

In \citep{giordano:2018:bnpsensitivity}, I derive prior sensitivity measures for
a truncated variational Bayes approximation using ideas from \citep{gustafson:1996:localposterior,
giordano:2018:covariances}. Unlike previous work on local Bayesian sensitivity
for BNP \citep{Basu:2000:BNP_robustness}, we pay special attention to the
ability of our sensitivity measures to \emph{extrapolate} to different priors,
rather than treating the sensitivity as a measure of robustness \textit{per se}.
In work currently in progress, my co-author and I apply the approximation from
\citep{giordano:2018:bnpsensitivity} to an unsupervised clustering problem on a
human genome dataset \citep{huang:2011:haplotype, raj:2014:faststructure},
demonstrating that the approximate is accurate, orders of magnitude faster than
re-fitting, and capable of detecting meaningful prior sensitivity.


\paragraph{Uncertainty propagation in mean-field variational Bayes.}

Mean-field Variational Bayes (MFVB) is an approximate Bayesian posterior
inference technique that is increasingly popular due to its fast runtimes on
large-scale scientific data sets (e.g., \citet{raj:2014:faststructure,
kucukelbir:2017:advi, regier:2019:cataloging}). However, even when MFVB provides
accurate posterior means for certain parameters, it often mis-estimates
variances and covariances \citep{wang:2005:inadequacy, turner:2011:two} due to
its inability to propagate Bayesian uncertainty between statistical parameters.

In \citet{giordano:2015:linear, giordano:2018:covariances}, I derive a simple
formula for the effect of infinitesimal model perturbations on MFVB posterior
means, thus providing improved covariance estimates and greatly expanding the
practical usefulness of MFVB posterior approximations. The estimates for MFVB
posterior covariances rely on a result from the classical Bayesian robustness
literature that relates derivatives of posterior expectations to posterior
covariances and includes the Laplace approximation as a special case.
% The key condition is that the MFVB
% approximation provides good estimates of a select subset of posterior means---an
% assumption that has been shown to hold in many practical settings.
In our experiments, we demonstrate that our methods are simple, general, and
fast, providing accurate posterior uncertainty estimates and robustness measures
with runtimes that can be an order of magnitude faster than MCMC, including
models from ecology \citep{kery:2011:bayesian}, the social sciences
\citep{gelman:2006:arm}, and on a massive internet advertising dataset
\citep{criteo:2014:dataset}.



\newpage

\paragraph{Sensitivity to removal of a small fraction of the data.} Classical
frequentist standard errors estimate the variability in an estimator that would
result from the rarefied thought experiment of re-sampling datasets from the same
distribution that gave rise to the observed data.  In the social sciences, this
rarefied experiment rarely closely corresponds to reality, and one might be
concerned if substantive conclusions could be overturned by other minor
perturbations to the data.

In \citet{giordano:2020:amip}, we provide an easily-computed approximation to
quantify the effect of ablating a small proportion of a dataset, with
open-source software and finite-sample accuracy bounds for ordinary least
squares and instrumental variables regression. We find that problems with small
signal-to-noise ratio but large datasets will be particularly non-robust to the
removal of a small proportion of the data. Such a situation that obtains
commonly in econometrics, and we find that the sign and statistical significance
of estimated effects in a number of large, prominent econometric studies can be
overturned by dropping only a small number of datapoints
\citep{angelucci:2009:indirect, finkelstein:2012:oregon, meager:2019:microcredit}.


\paragraph{Frequentist variability of Bayesian posteriors.}

Bayesian statistics provides powerful tools for coherently treating uncertainty
in complex problems, though, when the model is misspecified, the estimated
posterior uncertainty may not be meaningful.  In principle, however, one might
always compute the frequentist sampling variability of a Bayesian posterior
quantity, and such a quantity always remains meaningful, even if conceptually
distinct from a posterior uncertainty \citep{waddell:2002:bayesphyloboot,
kleijn:2006:misspecification}.  However, standard tools for evaluating
frequentist uncertainty, such as the bootstrap \citep{huggins:2019:bayesbag},
are extremely computationally intensive, as they typically require re-running an
MCMC procedure hundreds of times.

In a work in progress \citep{giordano:2020:bayesij}, we derive the Bayesian
infinitesimal jackknife (IJ), which we prove can be used to consistently
estimate the frequentist variability of Bayesian posterior means without
bootstrapping or computing a maximum a-posteriori (MAP) estimate.  Our work
synthesizes results from Bayesian robustness and frequentist von Mises
expansions and extends the Bayesian central limit theorem to the expectation of
data-dependent functions
\citep{jaeckel:1972:infinitesimal,shao:2012:jackknife,giordano:2019:ij,gustafson:2012:localrobustnessbook,giordano:2018:covariances,
lehman:1983:pointestimation, kass:1990:posteriorexpansions}. We demonstrate the
accuracy of the Bayesian IJ on datasets from election modeling
\citep{economist:2020:election}, ecology \citep{kery:2011:bayesian}, and most of
the models from \citep{gelman:2006:arm, stan-examples:2017}, showing that the
Bayesian IJ can reproduce the bootstrap covariance estimates in orders of
magnitude less compute time.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Sensitivity for Bayesian analysis}

% Bayesian techniques allows analysts to reason coherently about unknown
% parameters, but only if the user specifies a complete generating process for the
% parameters and data, including both prior distributions for the parameters and
% precise likelihoods for the data.  Often, aspects of this model are at best a
% considered simplification, and at worst chosen only for computational
% convenience.  It is critical to ask whether the analysis would have changed
% substantively had different modeling choices been made.

\paragraph{Propagation of uncertainty in scalable Bayesian inference}

One popular technique to scale Bayesian inference to massive problems is mean
field variational Bayes (MFVB) \citep{wainwright:2008:graphical,
blei:2017:variational, regier:2019:cataloging}.  However, MFVB provides
notoriously innacurate posterior uncertainty estimates, even in situations when
it estimates the posterior means accurately \citep{turner:2011:two}. In
\citet{giordano:2018:covariances}, we develop a method to recover accurate
posterior uncertainties from MFVB approximations without needing to fit a more
complex model or run MCMC. Computing the LRVB covariance requires solving a
linear system, which in scientific applications is often sparse and can be
solved using iterative techniques such as conjugate gradient \citep[Chapter
5]{nocedal:2006:numerical}.

We compare LRVB covariances to MCMC on a large number of real-world datasets,
including logistic regression on an internet advertising dataset
\citep{criteo:2014:dataset}, the Cormack-Jolly-Seber model from ecology
\citep{kery:2011:bayesian}, and hierarchical generalized linear models from the
social sciences \citep{gelman:2006:arm}, demonstrating accurate posterior
covariances computed over an order of magnitude faster than MCMC.



\paragraph{Bayesian nonparametrics.}

A commonly question in unsupervised clustering is how many distinct clusters are
present in a dataset.  Discrete Bayesian nonparametrics (BNP) allows the answer
to be inferred using Bayesian inference, but one must specify a prior on how
distinct clusters are generated \citep{ghosh:2003:bnp,
gershman:2012:bnptutorial}.  A particularly common modeling choice is the
stick-breaking representation of a Dirichlet process prior
\citep{sethuraman:1994:constructivedp}, a mathematical abstraction which is
arguably better justified by its computational convenience than its realism.

In \citet{giordano:2018:bnpsensitivity}, we fit a BNP model with variational
Bayes \citep{blei:2006:dirichletbnp} using the standard, computationally
convenient stick-breaking prior, but then use sensitivity analysis to allow the
user to explore alternative functional forms an order of magnitude faster than
would be possible with refitting. In work currently in progress, we apply our
method to a human genome dataset in phylogenetics taken from
\citep{huang:2011:haplotype}, and find that our method accurately discovers
meaningful prior sensitivity in a BNP version of the model
\texttt{fastSTRUCTURE} \citep{raj:2014:faststructure}.


% \paragraph{Partial pooling in meta-analysis.}
%
% A popular form of meta-analysis in econometrics is to place a hierarchial model
% on a set of related experimental results, which both ``shrinks'' the individual
% estimates towards a common mean, potentially decreasing mean squared error, and
% allowing direct estimation of the average effect and diversity of effects
% \citep{rubin:1981:estimation,gelman:1992:inference}. These advantages come at
% the cost of positing a precise generative process for the effects in question,
% and it is reasonable to interrogate whether the estimation procedure is robust
% to varaibility in these effects.  In \citet{giordano:2016:microcredit}, we apply
% sensitivity analysis to a published meta-analysis of the effectiveness of
% microcredit interventions in seven developing countries
% \citep{meager:2019:microcredit}.  We find that the conclusion are highly
% sensitive to the assumed covariance structure between the base level of business
% profitability and the microcredit effect, a covariance which is {\em a priori}
% difficult to ascertain, automatically diagnosing an important source of
% epistemic uncertainty not captured by the Bayesian posterior.


%
%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\subsection*{Selected Future work}

My research is driven by the needs of my scientific collaborators, and
so my future work will be determined to a large part by my colleagues.
Here, I will discuss a few directions that I find promising and interesting,
and which I believe could be applicable to a diverse set of problems.

\paragraph{The higher-order infinitesimal jackknife for the bootstrap.}

In the preprint \citet{giordano2019:hoij}, we extend
\citet{giordano:2019:ij} to higher-order Taylor series approximations,
provding a family of estimators which we collectively call the higher-order
infintiesimal jackknife (HOIJ).  In addition to providing higher-quality
approximations to CV and extending our results to k-fold CV, the higher-order
approach promises to provide a scalable alternative to the bootstrap, a
procedure that estimates frequentist variability by repeatedly re-evaluating a
model at datasets drawn with replacement from the observed data. The bootstrap
is known to enjoy higher-order accuracy in certain circumstances
\citet{hall:2013:bootstrap}, and the HOIJ can approach the bootstrap at a rate
faster than the bootstrap approaches the truth.  The HOIJ thus promises to make
bootstrap inference available to models which are differentiable but too
expensive to re-evaluate (e.g. simulation-based models
\citep{gourieroux:1993:simulation}), but also to allow efficient
bootstrap-after-bootstrap procedures which that are currently out of reach for
all but the simplest statsitics \citep{efron:1994:bootstrap}.

% \paragraph{Preprocessing sensitivity.}
%
% Analyses in genomics often begin with a pre-processing step in observation units
% are clustered together according to ad-hoc measures of similarity across a large
% number of feature vectors \citep{xu:2015:identification,
% stuart:2019:comprehensive}.  Quickly assessing the sensitivity of these
% procedures to the inclusion or exclusion of individual features would allow the
% researcher to identify high-leverage observations and avoid imposing structure
% via arbitrary modeling assumptions.   Sensitivity anlaysis cannot be applied
% directly to such similarity measures, as they are typically non-differentiable.
% With a colleague from biology, I am investigating using a probabilisitic
% relaxation only of the initial distance measure, drawing random datasets, and
% applying the non-differential similarity measure to these random datasets,
% taking the average similarity across draws as the output of the procedure. We
% can then assess the sensitivity of the original sample's probability to
% inclusion or exclusion of particular features, and assess in turn the
% sensitivity of the importance sampling estimate of the average output, providing
% a sensitivity analysis of the whole non-differentiable procedure.  Conceptually,
% this approach is attractive because it would allow sensitivity analysis to be
% applied to black-box procedures without having to design and validate custom
% continuous relaxations.

\paragraph{Partitioned Bayesian inference.}

The ideas of \citep{giordano:2018:covariances} can be naturally extended to
approximately propagate uncertainty amongst separately estimated components of
an inference problem.  For example, astronomical catalogues are customarily
produced with MFVB-like algorithms \citep{lang:2016:tractor,
regier:2019:cataloging}, which take inputs such as the sky background
and optical point spread function as fixed inputs, though these quantities
are themselves inferred with uncertainty.  Viewing all the separate inference
procedures as a sequential quasi-MFVB objective, one could directly apply
the techniques of LRVB to propagate the uncertainty from the modeling inputs
to the astronomical catalogue's uncertainty.  Doing so would require the
approximate solution of a very large, but very sparse, linear system,
which is itself an interesting computational challenge.







\bibliography{references}
\bibliographystyle{plainnat}

\end{document}
