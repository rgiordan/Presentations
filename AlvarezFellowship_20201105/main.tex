\input{_headers.tex}

\usepackage{enumitem}
\setlist{nolistsep}

\title{Research Statement}

\author{
  Ryan Giordano \\ \texttt{rgiordan@mit.edu }
}

\begin{document}

%\maketitle
\section*{Research Statement}

As statistical models grow in size and complexity to serve modern scientific
datasets and questions, fundamental data science tasks become more
computationally onerous due to the need of many classical procedures to evaluate
or fit a model multiple times.  My research uses {\em sensitivity analysis} to
provide fast, accurate approximations to such fundamental data science tasks,
often exhibiting good accuracy and orders-of-magnitude speedup over classical
methods.

Consider, as motivating examples, the following ubiquitous data science tasks.
%
\begin{itemize}
%
        \item Cross validation (CV) is a fundamental tool in machine learning to
        evaluate model predictive performance and tune hyperparameters, but
        requires fitting a model multiple times with different data subsets left
        out.
        % For example, when clustering the time series profiles of mice gene
        % expression data, one may need to choose how much to smooth the time
        % series prior to clustering, and one would typically choose the smoothing
        % with the best predictive performance as evaluated by CV
        % \citep{friedman:2001:esl, shoemaker:2015:ultrasensitive}.
        %
        \item  Prior specification is a necessary step in Bayesian statistics, a
        statistical paradigm that provides interpretable, coherent uncertainty
        quantification for scientific questions.  But Bayesian inference can be
        sensitive to the prior specification, and evaluating the model for
        multiple plausible prior choices can be computationally prohibitive.
        % For
        % example, one might use Bayesian nonparametrics to ask how many distinct
        % genotypes are found in a human genome dataset
        % \citep{huang:2011:haplotype, raj:2014:faststructure}; forming the
        % estimate for a single prior can take hours.
        %
        \item Uncertainty propagation, i.e., allowing the inferential
        uncertainty in one modeling quantity to inform the inferential
        uncertainty in another, is a key advantage of Bayesian statistics.
        However, the classical tool for Bayesian estimation, Markov Chain Monte
        Carlo (MCMC), requires evaluating a statistical model many times, and so
        can be computationally expensive.
        % For example, in the
        % inference of astronomical catalogues, the non-scalability of MCMC leads
        % researchers to turn to scalable ``variational Bayes'' posterior
        % approximations, which are computationally tractable, but known to fail
        % to propagate uncertainty amongst components of the model
        % \citep{turner:2011:two, regier:2019:cataloging}.
%
\end{itemize}
%

% The more complicated a model is, the more likely it is that
% it may overfit the data (remedied by CV), the more difficult it is
% to reason subjectively about the prior (remedied by experimenting with
% a range of priors), and the more likely it is to model multiple, mutually
% dependent, uncertain quantities (remedied by uncertainty propagation).

These three central data science problems may seem superfically distinct.  But
they share the common property that they are computationally demanding due to
requiring the evaluation or estimation of a statistical model multiple times:
once for each cross validation sample, once for each prior specification, or
once for each draw of an MCMC chain.

I show that this commonality implies that all these tasks are amendable to {\em
sensitivity analysis}, in which the evaluation of a model at alternative inputs
is approximated by a Taylor series.  By evaluating the derivatives necessary to
form the Taylor series at a {\em single model estimate}, I avoid the
re-estimation or re-evaluation that makes the above procedures computationally
prohibitive.  In exchange, evaluating the necessary derivatives often requires
solving a large but sparse linear system, a tradeoff that can be quite favorable
in practice.

Sensitivity analysis is a venerable topic, though the breadth of its
applications to conteporary problems is arguably underappreciated.  My work
advances existing work by providing practical implementations of classical
methods, particularly using automatic differentiation
\citep{baydin:2015:automatic}, by updating classical theory to apply in finite
sample and under more realistic conditions, and by drawing connections between
superifically disparate applications of sensitivity analysis. For the remainder
of the essay, I will discuss in more detail how I apply sensitivity analysis to
the above three data science tasks and more, both in practice and and in theory.




\newpage

\paragraph{Approximate cross validation.}

The error or variability of machine learning algorithms is often assessed by
repeatedly re-fitting a model with different weighted versions of the observed
data; cross-validation (CV) and the bootstrap can be thought of as examples of
this technique.

In \citet{giordano:2019:ij}, I use a linear approximation to the
dependence of the fitting procedure on the weights, producing results that can
be faster than repeated re-fitting by an order of magnitude. I provide explicit
finite-sample error bounds for the approximation in terms of a small number of
simple, verifiable assumptions.  My results apply whether the weights and data
are stochastic or deterministic, and so can be used as a tool for proving the
accuracy of the infinitesimal jackknife on a wide variety of problems. As a
corollary, I state mild regularity conditions under which our approximation
consistently estimates true leave-$k$-out cross-validation for any fixed $k$. I
demonstrate the accuracy of the approximation on a range of simulated and real
datasets, including an unsupervised clustering problem from genomics
\citep{Luan:2003:clustering, shoemaker:2015:ultrasensitive}.


\paragraph{Prior sensitivity for Markov Chain Monte Carlo.}

MCMC is arguably the most commonly used computational tool to estimate Bayesian
posteriors, which is made still easier by modern black-box MCMC tools such as
\texttt{Stan} \citep{carpenter:2017:stan, rstan}.  However, a single run of MCMC
typically remains time-consuming, and systematically exploring alternative prior
parameterizations by re-running MCMC would be computationally prohibitive for
all but the simplest models.

My software package, \texttt{rstansensitivity},
\citep{giordano:2020:rstansensitivity, giordano:2018:mcmchyper}, takes advantage
of the automatic differentiation capacities of \texttt{Stan}
\citep{carpenter:2015:stanmath} together with a classical result from  Bayesian
robustness \citep{gustafson:1996:localposterior, basu:1996:local,
giordano:2018:covariances} to provide automatic hyperparameter sensitivity for
generic \texttt{Stan} models from only a single MCMC run.  I demonstrate the
speed and utility of the package in detecting excess prior sensitivity,
particularly in a social sciences model taken from \citet[Chapter
13.5]{gelman:2006:arm}.


\paragraph{Prior sensitivity for discrete Bayesian nonparameterics.}

% From BNP_sensitivity/writing/NIPS_2018_BNP_workshop
A central question in many probabilistic clustering problems is how many
distinct clusters are present in a particular dataset. A Bayesian nonparametric
(BNP) model addresses this question by placing a generative process on cluster
assignment, making the number of distinct clusters present amenable to Bayesian
inference.  However, like all Bayesian approaches, BNP requires the
specification of a prior, and this prior may favor a greater or lesser number of
distinct clusters.
% In practice, it is important to quantitatively establish that
% the prior is not too informative, particularly when---as is often the case in
% BNP---the particular form of the prior is chosen for mathematical convenience
% rather than because of a considered subjective belief.

In \citep{giordano:2018:bnpsensitivity}, I derive prior sensitivity measures for
a truncated variational Bayes approximation using ideas from \citep{gustafson:1996:localposterior,
giordano:2018:covariances}. Unlike previous work on local Bayesian sensitivity
for BNP \citep{Basu:2000:BNP_robustness}, we pay special attention to the
ability of our sensitivity measures to \emph{extrapolate} to different priors,
rather than treating the sensitivity as a measure of robustness \textit{per se}.
In work currently in progress, my co-author and I apply the approximation from
\citep{giordano:2018:bnpsensitivity} to an unsupervised clustering problem on a
human genome dataset \citep{huang:2011:haplotype, raj:2014:faststructure},
demonstrating that the approximate is accurate, orders of magnitude faster than
re-fitting, and capable of detecting meaningful prior sensitivity.


\paragraph{Uncertainty propagation in mean-field variational Bayes.}

Mean-field Variational Bayes (MFVB) is an approximate Bayesian posterior
inference technique that is increasingly popular due to its fast runtimes on
large-scale scientific data sets (e.g., \citet{raj:2014:faststructure,
kucukelbir:2017:advi, regier:2019:cataloging}). However, even when MFVB provides
accurate posterior means for certain parameters, it often mis-estimates
variances and covariances \citep{wang:2005:inadequacy, turner:2011:two} due to
its inability to propagate Bayesian uncertainty between statistical parameters.

In \citet{giordano:2015:linear, giordano:2018:covariances}, I derive a simple
formula for the effect of infinitesimal model perturbations on MFVB posterior
means, thus providing improved covariance estimates and greatly expanding the
practical usefulness of MFVB posterior approximations. The estimates for MFVB
posterior covariances rely on a result from the classical Bayesian robustness
literature that relates derivatives of posterior expectations to posterior
covariances and includes the Laplace approximation as a special case.
% The key condition is that the MFVB
% approximation provides good estimates of a select subset of posterior means---an
% assumption that has been shown to hold in many practical settings.
In our experiments, we demonstrate that our methods are simple, general, and
fast, providing accurate posterior uncertainty estimates and robustness measures
with runtimes that can be an order of magnitude faster than MCMC, including
models from ecology \citep{kery:2011:bayesian}, the social sciences
\citep{gelman:2006:arm}, and on a massive internet advertising dataset
\citep{criteo:2014:dataset}.



\paragraph{Data ablation.}

In \citet{giordano:2020:amip}, I propose a method to assess the sensitivity of
statistical analyses to the removal of a small fraction of the sample. Analyzing
all possible data subsets of a certain size is computationally prohibitive, so I
provide a finite-sample metric to approximately compute the number (or fraction)
of observations that has the greatest influence on a given result when dropped.
I provide explicit finite-sample error bounds on our approximation for linear
and instrumental variables regressions. At minimal computational cost, the
metric provides an exact finite-sample lower bound on sensitivity for any
estimator, so any non-robustness our metric finds is conclusive. I demonstrate
that non-robustness to data ablation is driven by a low signal-to-noise ratio in
the inference problem, is not reflected in standard errors, does not disappear
asymptotically, and is not a product of misspecification.

The approximation is automatically computable and works for common estimators
(including OLS, IV, GMM, MLE, and variational Bayes), and I provide an
easy-to-use \texttt{R} package to compute the approximation
\citep{zaminfluence}. Several empirical applications based on published
econometric analyses \citep{angelucci:2009:indirect, finkelstein:2012:oregon,
meager:2019:microcredit} show that even 2-parameter linear regression analyses
of randomized trials can be highly sensitive. While I find some applications are
robust, in others the sign of a treatment effect can be changed by dropping less
than 1\% of the sample even when standard errors are small.


\paragraph{Frequentist variability of Bayesian posteriors.}

The frequentist (i.e., sampling) variance of Bayesian posterior expectations
differs in general from the posterior variance even for large datasets,
particularly when the model is misspecified or contains many latent variables
\citep{kleijn:2006:misspecification}.
Knowing the frequentist variance of a posterior expectation can be useful even
to a committed Bayesian, particularly when the data is known to arise from
random sampling and there is a possibility of model misspecification
\citep{waddell:2002:bayesphyloboot}.  However, the
principal existing approach for computing the frequentist variability from MCMC
procedures is the bootstrap, which can be extremely computationally intensive
due to the need to run hundreds of extra MCMC procedures
\citep{huggins:2019:bayesbag}.

In \citep{giordano:2020:bayesij, giordano:2020:stanconbayesij}, I propose an
efficient alternative to bootstrapping an MCMC procedure which is based on the
influence function from sensitivity analysis.  Using results from
\citep{giordano:2018:covariances, giordano:2019:ij}, I show that the influence
function for posterior expectations has the form of a simple posterior
covariance, easily computed from the posterior samples of a single MCMC
procedure. Furthermore, when a Bayesian central limit theorem applies, I show
that the variance estimate derived from the influence function is asymptotically
equivalent to that of the bootstrap.  I demonstrate the accuracy and
computational benefits of the influence function variance estimates on array of
experiments including an election forecasting model
\citep{economist:2020:election}, the Cormack-Jolly-Seber model from ecology
\citep{kery:2011:bayesian}, and a large collection of models and datasets from
the social sciences \citep{gelman:2006:arm}.



\newpage


\paragraph{Frequentist variability of Bayesian posteriors.}

Bayesian statistics provides powerful tools for coherently treating uncertainty
in complex problems, though, when the model is misspecified, the estimated
posterior uncertainty may not be meaningful.  In principle, however, one might
always compute the frequentist sampling variability of a Bayesian posterior
quantity, and such a quantity always remains meaningful, even if conceptually
distinct from a posterior uncertainty \citep{waddell:2002:bayesphyloboot,
kleijn:2006:misspecification}.  However, standard tools for evaluating
frequentist uncertainty, such as the bootstrap \citep{huggins:2019:bayesbag},
are extremely computationally intensive, as they typically require re-running an
MCMC procedure hundreds of times.

In a work in progress \citep{giordano:2020:bayesij}, we derive the Bayesian
infinitesimal jackknife (IJ), which we prove can be used to consistently
estimate the frequentist variability of Bayesian posterior means without
bootstrapping or computing a maximum a-posteriori (MAP) estimate.  Our work
synthesizes results from Bayesian robustness and frequentist von Mises
expansions and extends the Bayesian central limit theorem to the expectation of
data-dependent functions
\citep{jaeckel:1972:infinitesimal,shao:2012:jackknife,giordano:2019:ij,gustafson:2012:localrobustnessbook,giordano:2018:covariances,
lehman:1983:pointestimation, kass:1990:posteriorexpansions}. We demonstrate the
accuracy of the Bayesian IJ on datasets from election modeling
\citep{economist:2020:election}, ecology \citep{kery:2011:bayesian}, and most of
the models from \citep{gelman:2006:arm, stan-examples:2017}, showing that the
Bayesian IJ can reproduce the bootstrap covariance estimates in orders of
magnitude less compute time.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Sensitivity for Bayesian analysis}

% Bayesian techniques allows analysts to reason coherently about unknown
% parameters, but only if the user specifies a complete generating process for the
% parameters and data, including both prior distributions for the parameters and
% precise likelihoods for the data.  Often, aspects of this model are at best a
% considered simplification, and at worst chosen only for computational
% convenience.  It is critical to ask whether the analysis would have changed
% substantively had different modeling choices been made.

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
\subsection*{Selected Future work}

My research is driven by the needs of my scientific collaborators, and
so my future work will be determined to a large part by my colleagues.
Here, I will discuss a few directions that I find promising and interesting,
and which I believe could be applicable to a diverse set of problems.

\paragraph{The higher-order infinitesimal jackknife for the bootstrap.}

In the preprint \citet{giordano2019:hoij}, we extend
\citet{giordano:2019:ij} to higher-order Taylor series approximations,
provding a family of estimators which we collectively call the higher-order
infintiesimal jackknife (HOIJ).  In addition to providing higher-quality
approximations to CV and extending our results to k-fold CV, the higher-order
approach promises to provide a scalable alternative to the bootstrap, a
procedure that estimates frequentist variability by repeatedly re-evaluating a
model at datasets drawn with replacement from the observed data. The bootstrap
is known to enjoy higher-order accuracy in certain circumstances
\citet{hall:2013:bootstrap}, and the HOIJ can approach the bootstrap at a rate
faster than the bootstrap approaches the truth.  The HOIJ thus promises to make
bootstrap inference available to models which are differentiable but too
expensive to re-evaluate (e.g. simulation-based models
\citep{gourieroux:1993:simulation}), but also to allow efficient
bootstrap-after-bootstrap procedures which that are currently out of reach for
all but the simplest statsitics \citep{efron:1994:bootstrap}.

% \paragraph{Preprocessing sensitivity.}
%
% Analyses in genomics often begin with a pre-processing step in observation units
% are clustered together according to ad-hoc measures of similarity across a large
% number of feature vectors \citep{xu:2015:identification,
% stuart:2019:comprehensive}.  Quickly assessing the sensitivity of these
% procedures to the inclusion or exclusion of individual features would allow the
% researcher to identify high-leverage observations and avoid imposing structure
% via arbitrary modeling assumptions.   Sensitivity anlaysis cannot be applied
% directly to such similarity measures, as they are typically non-differentiable.
% With a colleague from biology, I am investigating using a probabilisitic
% relaxation only of the initial distance measure, drawing random datasets, and
% applying the non-differential similarity measure to these random datasets,
% taking the average similarity across draws as the output of the procedure. We
% can then assess the sensitivity of the original sample's probability to
% inclusion or exclusion of particular features, and assess in turn the
% sensitivity of the importance sampling estimate of the average output, providing
% a sensitivity analysis of the whole non-differentiable procedure.  Conceptually,
% this approach is attractive because it would allow sensitivity analysis to be
% applied to black-box procedures without having to design and validate custom
% continuous relaxations.

\paragraph{Partitioned Bayesian inference.}

The ideas of \citep{giordano:2018:covariances} can be naturally extended to
approximately propagate uncertainty amongst separately estimated components of
an inference problem.  For example, astronomical catalogues are customarily
produced with MFVB-like algorithms \citep{lang:2016:tractor,
regier:2019:cataloging}, which take inputs such as the sky background
and optical point spread function as fixed inputs, though these quantities
are themselves inferred with uncertainty.  Viewing all the separate inference
procedures as a sequential quasi-MFVB objective, one could directly apply
the techniques of LRVB to propagate the uncertainty from the modeling inputs
to the astronomical catalogue's uncertainty.  Doing so would require the
approximate solution of a very large, but very sparse, linear system,
which is itself an interesting computational challenge.







\bibliography{references}
\bibliographystyle{plainnat}

\end{document}
