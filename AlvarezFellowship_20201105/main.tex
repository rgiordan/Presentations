\input{_headers.tex}

\title{Research Statement}

\author{
  Ryan Giordano \\ \texttt{rgiordan@mit.edu }
}

\begin{document}


\subsection*{Overview}

Deriving scientific information from large, complex datasets can motivate large,
complex statistical models; below we will give examples from our work on
problems in astronomy, genomics, phylogenetics, econometrics, internet
advertising, and ecology.  As models grow in complexity, the need to interrogate
their assumptions, to propagate uncertainty amongst their components, and to
perform non-parametric checks on their data sensitivity grows commensurately.
However, so does the computational cost of doing so using traditional
statistical methods. Many classical procedures designed to address these
concerns, such as Markov Chain Monte Carlo (MCMC), cross validation (CV), or
re-estimating a model under a range of modeling assumptions, can be
prohibitively expensive in many modern problems.

To help fill this gap, my research focuses on applications of {\em sensitivity
analysis}, applied not merely in the traditional sense of assessing sensitivity
to imprecise modeling assumptions (though I do pursue this traditional role as
well), but also to assess freqeuntist sampling properties and propagate
uncertainty in Bayesian procedures.  At its core, my methodological work is all
based on using Taylor series approximations, constructed only from properties of
a single model fit, computed using either optimization or MCMC, to extrapolate
to alternatives.  In this way, I provide approximations to Bayesian posterior
uncertainty, CV loss, and model sensitivity, while avoiding expensive
re-estimation.  Sensitivity analysis is a venerable idea with a rich existing
literature, and I show that it has wide-ranging and fundamental applications in
modern, computationally intensive statistical problems.  Furthermore, I motivate
the re-examination of theoretical ideas concerning the role of differential
approximations in statistical analysis as practical tools in the age of big data
and big computing.

I will divide my research statement into three parts: first, I will cover a more
traditional application of sensitivity analysis to the assessment of prior
sensitivity in Bayesian and variational Bayesian analysis.  I will then discuss
approximate CV and bootstrapping, which can be viewed as a kind of sensitivity
analysis.  Finally, I discuss how sensitivity analysis can recover accurate
posterior covariances from variational Bayesian approximations, tying sensitiity
analysis to the uniquitous scientific goal of uncertainty propagation.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Prior sensitivity in Bayesian analysis}

Bayesian techniques allows analysts to reason coherently about unknown
parameters, but only if the user specifies a complete generating process for the
parameters and data, including both prior distributions for the parameters and
precise likelihoods for the data.  Often, aspects of this model are at best a
considered simplifcation, and at worst chosen only for computational
convenience.  It is critical to ask whether the analysis would have changed
substantively had different modeling choices been made.

\paragraph{Bayesian nonparametrics.}

A commonly question in unsupervised clustering is how many distinct clusters are
present in a dataset.  Discrete Bayesian nonparametrics (BNP) allows the answer
to be inferred using Bayesian inference, but one must specify a prior on how
distinct clusters are generated \citep{ghosh:2003:bnp,
gershman:2012:bnptutorial}.  A particularly common modeling choice is the
stick-breaking representation of a Dirichlet process prior
\citep{sethuraman:1994:constructivedp}, a mathematical abstraction which is
arguably better justified by its computational convenience than its realism. Our
workshop paper, \citet{giordano:2018:bnpsensitivity}, fits a BNP model with
variational Bayes \citep{blei:2006:dirichletbnp} using the standard,
computationally convenient stick-breaking prior, but then uses sensitivity
analysis to allow the user to explore alternative functional forms an order of
magnitude faster than would be possible with refitting. In work currently in
progress, we apply our method to a human genome dataset in phylogenetics taken
from \citep{huang:2011:haplotype}, and find that our method accurately discovers
real excess prior sensitivity in a BNP version of the model
\texttt{fastSTRUCTURE} \citep{raj:2014:faststructure}.



\paragraph{Partial pooling in meta-analysis.}

A popular form of meta-analysis in econometrics is to place a hierarchial model
on a set of related experimental results, which both ``shrinks'' the individual
estimates towards a common mean, potentially decreasing mean squared error, and
allowing direct estimation of the average effect and diversity of effects
\citep{rubin:1981:estimation,gelman:1992:inference}. These advantages come at
the cost of positing a precise generative process for the effects in question,
and it is reasonable to interrogate whether the estimation procedure is robust
to varaibility in these effects.  In \citet{giordano:2016:microcredit}, we apply
sensitivity analysis to a published meta-analysis of the effectiveness of
microcredit interventions in seven developing countries
\citep{meager:2019:microcredit}.  We find that the conclusion are highly
sensitive to the assumed covariance structure between the base level of business
profitability and the microcredit effect, a covariance which is {\em a priori}
difficult to ascertain, automatically diagnosing an important source of
epistemic uncertainty not captured by the Bayesian posterior.


\paragraph{Hyperparameter sensitivity for MCMC.}

MCMC is arguably the most commonly used computational tool to estimate Bayesian
posteriors, and modern black-box MCMC tools such as \texttt{Stan} \citep{rstan,
carpenter:2017:stan}.  However, MCMC still often takes a long time to run, and
systematically exploring alternative prior parameterizations by re-running MCMC
would be computationally prohibitive for all but the simplest models. A
classical result from Bayesian robustness states that the sensitivity of a
posterior expectation is given by a particular posterior covariance
\citep{gustafson:1996:localposterior, basu:1996:local}, though the result has
not been widely used, arguably due in part to the lack of an automatic
implementation. In my software package, \citet{giordano:2020:rstansensitivity},
I take advantage of the automatic differentiation capacities of
\texttt{Stan} to provide automatic hyperparameter sensitivity for
generic Stan models.  In examples in the package \texttt{git} repository,
I demonstrate the efficacy of the package in detecting excess prior
sensitivity, particularly in a social sciences model taken from
\citet[Chapter 13.5]{gelman:2006:arm}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Data sensitivity: cross validation and frequentist variance}

Frequentist variability is ultimately concerned with the outcome of an
estimation procedure if the data were drawn from the same distribution as but
different from that observed.  Similarly, all forms of cross-validation (CV)
evaluates a statistic if parts of the observed data had been ablated.  Both of
these procedures can be treated by sensitivity analysis, where sensitivity
is to the dataset itself.

\paragraph{Accuracy bounds for approximate cross validation.}

To perform leave-one-out CV (LOO-CV), one re-runs an estimation procedure with
each datapoint left out.  In full, LOO-CV requires as many re-runs procedures as
there are datapoints, and each re-run is expected to be quite close to the
original fit.  Rather than re-running exactly, one can use a Taylor series to
approximate the effect of removing a single data point; since the dataset with
one point left out is, in some sense, ``close'' to the original dataset, the
Taylor series can be expected to perform well.

Prior to our work, this idea had been suggested both in the machine learning
literature \citep{rad:2018:scalableloo, koh:2017:blackbox} as well as in the
classical statistical literature under the name ``infinitesimal jackknife''
\citep{jaeckel:1972:infinitesimal, shao:2012:jackknife}.  However, the ML work
appeared unaware of the statisical precedent, and both treatments required
unrealistic theoretical conditions for the accuracy of the Taylor series:
specifically, that the gradients of the objective function be uniformly bounded,
a condition that is rarely satisfied in scientific practice, even in the
simplest possible example of using maximum likelihood to estimate the sample
mean of a normal distribution.

In \citet{giordano:2019:ij}, we provide a more realistic set complexity
condition under which the Taylor series is accurate, eschewing the need for
bounded gradients, and synthesizing the classical statistics and ML literatures.
Also unlike previous work, our theory was purely finite sample, implying the
asymptotica results of prior work as a corollary.  We demonstrated the accuracy
of the technique on an unsupervised clustering problem from genomics
\citep{shoemaker:2015:ultrasensitive}.


\paragraph{Sensitivity to removal of a small fraction of the data.} Classical
frequentist standard errors estimate the variability in an estimator that would
result from the rarified thought experiment of re-sampling datsets from the same
distribution that gave rise to the observed data.  In the social sciences, this
rarefied experiment rarely closely corresponds to reality, and one might be
concerned if substantive conclusions could be overtuned by other minor
perturbations to the data.  For example, if a top-line conclusion of a study of
the efficacy of cash transfers in a particular country
\citep{angelucci:2009:indirect} could be reversed by removing a small percentage
(say, $0.1\%$) of a dataset, one might hesitate to generalize one's conclusions
to other countries, even if the result was ``statistically significant''
according to classical frequentist standard errors.

In \citet{giordano:2020:amip}, we address this fundamental question, extending
our earlier results in \citet{giordano:2019:ij}.  We find that problems with
small signal-to-noise ratio but large datasets will be particularly non-robust
to the removal of a small proportion of the data. Such a situation that obtains
commonly in econometrics, and we find that the sign and statistical significance
of estimated effects in a number of large, prominent econometric studies can be
overturned by dropping only a small number of datapoints
\citep{angelucci:2009:indirect, finkelstein:2012:oregon}.  Our robustness metric
can be computed easily and automatically for any Z-estimator; we provide
software and tractable finite-sample accuracy bounds for ordinary least squares
and instrumental variables regression.  More broadly, our work points to the
importance of considering ``practical significance'' of effects in the social
sciences rather than mere statistical significance.


\paragraph{Frequentist properties of Bayesian posteriors}

Bayesian measures is a powerful tool for coherently treating uncertainty in
complex problems, but, when the model is misspecified, the estimated  posterior
uncertainty may not be meaningful.  One can, however, always compute the
frequentist sampling variability of a Bayesian posterior quantity, and such a
quantity always remains meaningful, even if conceptually distinct from a
posterior uncertainty \citet{waddell:2002:bayesphyloboot,
kleijn:2006:misspecification, huggins:2019:bayesbag}.

By combining the frequentist IJ \citep{jaeckel:1972:infinitesimal,
shao:2012:jackknife, giordano:2019:ij} approach to frequentist variance with the
MCMC-based measures of sensitivity \citep{gustafson:2012:localrobustnessbook,
giordano:2018:covariances}, we are able to derive the Bayesian infinitesimal
jackknife (IJ), which can be used to compute the frequentist variability of
Bayeisan posterior means without bootstrapping or computing a maximum
a-posteriori (MAP) estimate.  In a work in progress, we extend the Bayesian
central limit theorem of \citet{lehman:1983:pointestimation,
kass:1990:posteriorexpansions} to prove the consistency of the Bayesian IJ and
show its accuracy as an approximation to the bootstrap for a larger number of
examples, effectively allowing estimation of frequentist covariances orders of
magnitude faster than the bootstrap.  We demonstrate the accuracy of our method
on datasets from election modeling \citep{economist:2020:election}, ecology \citep{kery:2011:bayesian}, and most of
the models from \citep{gelman:2006:arm, stan-examples:2017}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Propagation of uncertainty in scalable Bayesian inference}

Complex scientific inference procedures, such as the creation of astronomical
catalogues, often exhibit uncertainty in many aspects of the model.  For
instance, in order to infer whether a handful of pixels on a telescopic image is
a dim star or a distant galaxy, one must know the distortion (aka the point
spread function) of the telescope, the lightness of the sky background, the
noise of the photoreceptors, and the identity of nearby celestial objects, all
of which quantities must themselves be inferred with some uncertainty.

Bayesian procedures coherently propagate uncertainty between all such model
quantities, but classical MCMC procedures do not scale well, and are far beyond
computational reach for astronomical catalogues.  Researchers often turn to
optimization-based mean field Variational Bayes (MFVB) procedures as a scalable
alterative to MCMC, but MFVB does not estimate posterior correlations, and is
known to underestimate marginal posterior uncertainties.
\footnote{The frequentist expectation-maximization, or EM, algorithm,
can be understood as a MFVB procedure, and the present criticism applies
to it as well.}

In CITE, I develop a method to recover accurate posterior uncertainties from
MFVB approximations without needing to fit a more complex model, or indeed to
re-fit the original model.  The idea is to exploit a duality between posterior
covariances the sensitivity of posterior means and use the sensitivity of the
MFVB approximation to infinitesimal perturbations as an estimator of the
posterior covariance.  We call the method ``linear response variational Bayes''
(LRVB) after the idea's progenitor as a method in statistical mechanics for
inferring microscopic intensive thermodynamic quantities from macroscopic
perturbations of extensive quantities.  Computing the LRVB covaraince requires
solving a linear system, which in scientific applications is often sparse and
can be solved using iterative techniques such as conjugate gradient.

We compare LRVB covariances to MCMC on a large number of real-world datasets,
including logistic regression on internet-scale data, the Cormack-Jolly-Seber
model from ecology, and hierarchical generalized linear models from the
social sciences, and demonstrated accurate posterior covariances computed
over an order of magnitude faster than MCMC.


\subsection*{Selected Future work}

There is a lot to be done simply applying the above methodology to
applied problems in conjuction with collaborators with domain expertise.
However, in this section, I will focus instead on new methodological directions
suggested by the above work.

\paragraph{The bootstrap and the bootstrap after the bootstrap.}

Our work on the higher-order infinitesimal jackknife could be applied to other
random reweighting schemes, particularly the bootstrap.  The bootstrap is known
to have frequentist properties that are asymptotically more accurate than the
normal approximation in certain circumstances CITE, but the bootstrap requires
re-computing an estimator as many time as there are bootstrap samples. However,
a sufficiently high-order IJ estimate will approach the bootstrap estimator at a
rate faster than the bootstrap's extra accuracy, strongly suggesting that the IJ
will inherit all of the bootstrap's attractive properties at a fraction of the
computational cost.  The IJ is particularly appealing for
bootstrap-after-bootstrap procedures, which have attractive theoretical
properties but are computationally prohibitive even on medium-sized statistical
problems.  It seems plausible that the HOIJ could open up a range of bootstrap
applications that are presently out of reach.


\paragraph{Bayesian model criticism.}

Essentially all tools for checking the accuracy of Bayesian models are
frequentist in nature --- e.g. checking whether the data is likely under
draws from the prior or posterior, or evaluating its predicive performance
on a held-out dataset.  Predictive model checks, such as leave-one-out
CV are attractive, but currently have few practical implementations that
avoid multiple runs of the MCMC algorithm.  However, the possibility of
forming higher-order expansions of the Bayesian posterior as a function of
the empirical distribtuion could change that.


\paragraph{Partitioned Bayesian inference with theoretical bounds.}

Given a large, complicated problem, it is often computationally convenient to
perform inference in separate computatoinal steps, while still propagating
uncertainty from one step to another.  For example, in CITE, we first fit spline
regressions to time series of gene expression data, and then clustered the
spline fits to group together genes with similar behavior.  The Bayesian ideal,
which is the simultaneous estimation of the clusters and spline regression, was
too computationally prohibitive, and would intuitively have given a similar
result to the sequential analysis, to the extent that the cluster center priors
did not shrink the spline regressions too much.


\paragraph{Incorporating LRVB corrections into MFVB approximations.}



\paragraph{Bootstrapping simulation-based inference.}

\citet{giordano2019:hoij}
\citet{efron:1994:bootstrap}
\citet{hall:2013:bootstrap}



\bibliography{references}
\bibliographystyle{plainnat}

\end{document}
