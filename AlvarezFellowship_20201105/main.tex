\input{_headers.tex}

\title{Research Statement}

\author{
  Ryan Giordano \\ \texttt{rgiordan@mit.edu }
}

\begin{document}

Data analysts ask increasingly sophisticated questions of larger and more
complex datasets, statistical models are becoming both more complex and more
time-consuming to estimate.   For problems as disparate as the production of
astronomical catalogues, analysis of internet user data, to the meta-analysis of
randomized controlled trials in development economics.

As models grow in complexity, the need to interrogate their assumptions,
to propagate uncertainty amongst their components, and to perform non-parametric
checks on their data sensitivity grows commensurately, but so does the
computational cost of doing so.

Classical procedures such as Markov Chain Monte Carlo (MCMC) or the bootstrap,
which require evaluating a model for many distinct parameter values or datasets,
respectively, can be prohibitively expensive.

My research focuses on applications of sensitivity analysis that use only
properties of a single model fit to extrapolate to alternatives without
expensive re-running.  My work is conceptually unified around a single theme,
but disparate in applications.

My guiding motivation is uncertainty quantification, providing easy-to-use,
general tools that allow analysts to reason about both aleatoric and epistemic
uncertainty in their analysis tasks.


\subsection*{Prior Sensitivity in Bayesian Analysis}

Bayesian analysis allows analysts to reason coherently about unknown parameters,
but only if the user specifies a complete generating process for the parameters
and data, including both prior distributions for the parameters and precise
likelihoods for the data.  Often aspects of this model are at best a considered
simplifcation, and at worst chosen only for computational convenience.  It is
critical to ask whether the analysis would have changed substantively had
different modeling choices been made.

\subsubsection*{Bayesian Nonparametrics}

A commonly asked question in unsupervised clustering is how many distinct
clusters are present in a dataset.  Discrete Bayesian nonparametrics allows the
question to be addressed using Bayesian inference, but one must specify a prior
on how distinct clusters are generated.  A particularly common choice is
the stick-breaking representation of a Dirichlet process prior, a mathematical
abstraction arguably better justified by its mathematical convenience than
its realism.  The prior must be specified in terms of random stick lengths
to be broken off successively, the lengths of the sticks determining
the a priori cluster sizes.  The standard approach is to model the stick
lengths with $Beta(1, \alpha)$ distribution, the $\alpha$ being a scalar
tuning parameters.  Other classes of distributions are possible in principle
but hardly ever considered in practice, in part because the Beta distribution
enjoys some computational conveniences.

In CITE, we provide sensitivity measures that allow the user to explore
alternative stick breaking distributions from a single fit using the standard
and convenient Beta prior.  We linearly approximating the dependence of the
optimum on the functional form of a parameterized class of priors.  A natural
parameterized class is the set of $Beta(1, \alpha)$ distributions (parameterized
by $\alpha$), but we also consider arbitrarily functional perturbations.
In current work in progress, we evaluate the worst-case perturbations.
On a real-world clustering problem, a human genome dataset, we find that
the number of distinct inferred populations is in fact quite sensitive
to the prior.


\subsubsection*{Parital Pooling in Econometrics Meta-analysis}


\subsection*{Cross Validation and Data Sensitivity}


\subsection*{Propagation of Uncertainty in Variational Bayes}




\bibliography{references}
\bibliographystyle{plainnat}

\end{document}
