


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Recall the EM algorithm}
%
\vspace{-1em}
%
\begin{align*}
    \textrm{Observations: }y ={}& (y_1, \ldots, y_N)\\
    \textrm{Unknown latent variables: }z ={}& (z_1, \ldots, z_N)\\
    \textrm{Unknown global parameter: }\theta \in{}& \mathbb{R}^D.
    \quad \textrm{We want: } \thetahat ={}
    \argmax{\theta} \log p(y | \theta) .
\end{align*}

\pause
\hrulefill

The EM algorithm alternates between two steps.  Starting at an iterate
$\hat\theta_{(i)}$, repeat until convergence:

\textbf{The E-step:}  Compute $Q_{(i)}(\theta) := \expect{p\left(z | y, \hat\theta_{(i)} \right)}{\log p(y | \theta, z) + \log p(z | \theta)}$

\textbf{The M-step:}  Compute the next iterate $\thetahat_{(i + 1)} := \argmax{\theta} Q_{(i)}(\theta)$

\pause
\hrulefill

The EM algorithm works / is useful when:

\begin{itemize}
    %
    \item The joint log probability $\log p(y | \theta, z) + \log p(z | \theta)$ is easy to write down
    \item The posterior $p(z | y, \theta)$ is easy to compute
    \item The marginalizing integral $p(y | \theta) = \int p(y | \theta, z) p(z | \theta) dz$ is hard
    %
\end{itemize}

\pause

\hrulefill
\begin{center}
\textbf{Is the EM algorithm VI?} \hspace{5em}\textbf{Can you spot the lie?}
\end{center}

\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{The EM algorithm as VI}

Let $\qdom_z$ denote a family of distributions on $\z$, parameterized by a
finite-dimensional parameter $\eta$, such that $p(\z | \theta, \y) \in \qdom_z$
for the observed $\y$ and all $\theta$.

\textbf{Exercise:} When does $\qdom_z$ exist?  (Indexed by a
finite-dimensional parameter $\eta$.)

\pause

Let $\q(z | \etaopt(\theta)) := p(\z | \theta, \y)$.

In an abuse of notation,
write $\eta \in \qdom_z$ for $\eta \in \{\eta: \q(\z | \eta) \in \qdom_z \}$.

\pause

Then:
%
\begin{align*}
%
\log p(\y | \theta) ={}&
\log p(\y | \theta) + \kl{\q(\z | \etaopt(\theta))}{p(\z | \theta, \y)}
\\={}&
\log p(\y | \theta) + \argmax{\eta \in \qdom_z}\left( -
    \kl{\q(\z | \eta)}{p(\z | \theta, \y)} \right) & \bigstar
\\={}&
\argmax{\eta \in \qdom_z} \left(
    \log p(\y | \theta) -
    \kl{\q(\z | \eta)}{p(\z | \theta, \y)}\right)
\\={}&
\argmax{\eta \in \qdom_z} \left(
\log p(\y | \theta) +
\expect{\q(\z | \eta)}{
    \log p(\z | \theta, \y) - \log \q(\z | \eta)
}\right)
\\={}&
\argmax{\eta \in \qdom_z} \left(
\expect{\q(\z | \eta)}{
    \log p(\y | \theta) +
    \log p(\z | \theta, \y) -  \log \q(\z | \eta)
}\right)
\\={}&
\argmax{\eta \in \qdom_z} \left(
\expect{\q(\z | \eta)}{
    \log p(\y, \z | \theta) }
- \expect{\q(\z | \eta)}{\log \q(\z | \eta)
}\right) & \bigstar\bigstar
%
\end{align*}
%
\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{The EM algorithm as VI}
%
From the previous slide, the marginal MLE is given by
%
\begin{align*}
%
\MoveEqLeft
\thetahat := \argmax{\theta} \log p(\y | \theta) \\
={}&
\argmax{\theta} \argmax{\eta \in \qdom_z} \left(
\log p(\y | \theta) -
    \kl{\q(\z | \eta)}{p(\z | \theta, \y)}  \right) & \bigstar
\\={}&
\argmax{\theta} \argmax{\eta \in \qdom_z} \left(
\expect{\q(\z | \eta)}{
    \log p(\y, \z | \theta) }
+ \expect{\q(\z | \eta)}{\log \q(\z | \eta)
}\right) & \bigstar\bigstar
%
\end{align*}
%

\pause
\hrulefill

\textbf{The EM algorithm revisited.}  Starting at an iterate $\hat\theta_{(i)}$:

\textbf{The E-step:}
\begin{enumerate}
%
    \item For a fixed $\hat\theta_{(i)}$, optimize
$\bigstar$ for $\eta$.  Since only the KL divergence depends on
$\eta$, the optimum is $\etaopt(\hat\theta_{(i)})$,
and $\q(\z | \etaopt(\hat\theta_{(i)})) = p(\z | \hat\theta_{(i)}, \y)$.
%
    \item Then use $\etaopt(\hat\theta_{(i)})$ to compute the expectation in
$\bigstar\bigstar$ as a function of $\theta$.
%
\end{enumerate}

\textbf{The M-step:}  Keeping $\eta$ fixed at $\etaopt(\hat\theta_{(i)}))$,
optimize $\bigstar\bigstar$ as a function of $\theta$ to give
$\thetahat_{i + 1}$.  The entropy
$\expect{\q(\z | \eta)}{\log \q(\z | \eta)}$ does not depend on $\theta$
and can be ignored.

\pause

\hrulefill

$\Rightarrow$ The EM algorithm is coordinate ascent on the objective
%
\begin{align*}
%
f(\theta, \eta) =
\log p(\y | \theta) - \kl{\q(\z | \eta)}{p(\z | \theta, \y)}.
%
\end{align*}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{The EM algorithm as VI}

\begin{align*}
%
\thetahat, \etaopt :={}& \argmax{\theta, \eta \in \qdom_z} \left(
\log p(\y | \theta) -
    \kl{\q(\z | \eta)}{p(\z | \theta, \y)}  \right).
%
\end{align*}
%

The EM algorithm is coordinate ascent on the preceding objective.\\
\citep{neal1998view}

\textbf{Corollaries}:

\begin{itemize}
%
\item<2-> The EM algorithm converges to a local optimum of $\log p(y | \theta)$.
%
\item<3-> The EM algorithm is VI, and you don't need to optimize with coordinate
ascent.
%
\item<4-> If both $p(\z | \theta, \y)$ and $p(\z, \y | \theta )$ are easy,
then so is $p(\y | \theta)$.  (This was the lie.)
\begin{itemize}
    \item<5-> \textbf{Exercise: } Prove this a different way using exponential
    families (in real-life problems, $p(\z | \y, \theta)$ is only
    really tractable in exponential families).
\end{itemize}
%
\item<6-> If $p(\z | \theta, \y)$ is intractable, we can now consider different
approximating families which may not contain $p(\z | \theta, \y)$.
%
\end{itemize}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Different approximating families: Point masses on $\z$.}
%
Suppose instead of $\qdom_z$ we used $\qdom_{\z}^{pm}$, a family of
constant-entropy near-point mass distributions on $\z$, located at
some free parameter $\eta$.

\pause

Then
%
\begin{align*}
%
\MoveEqLeft
\argmax{\theta, \eta \in \qdom_{\z}^{pm}}
\left(
\log p(\y | \theta) -
    \kl{\q(\z | \eta)}{p(\z | \theta, \y)}  \right)
\\={}&
\argmax{\theta, \eta \in \qdom_{\z}^{pm}}
\left(
\expect{\q(\z | \eta)}{
    \log p(\y, \z | \theta) }
+ \expect{\q(\z | \eta)}{\log \q(\z | \eta)
}\right)
\\={}&
\argmax{\theta, \eta \in \qdom_{\z}^{pm}}
\left(
\expect{\q(\z | \eta)}{
    \log p(\y, \z | \theta) }
\right)
\\={}&
\argmax{\theta, \z}
    \log p(\y, \z | \theta).
%
\end{align*}
%
\pause

\textbf{
$\Rightarrow$ The Neyman-Scott paradox occurs because point masses are
poor approximations for the distribution $p(\z | \theta, \y)$.
}

\pause

\textbf{Exercise: }  Recall that the Neyman-Scott paradox disappears when,
instead of pairs, we have many observations, all from the same $\z_n$.
Can you use the VI perspective on the marginal EM algorithm to explain
this phenomenon?


\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Different approximating families: Point masses on $\theta$.}
%
Let $\qdom_{\theta}^{pm}$ denote a family of constant-entropy near-point mass
distributions on $\theta$, located at some free parameter $\vartheta$. Assume a
uniform prior on $\theta$.

\pause

Then:
%
\begin{align*}
%
\MoveEqLeft
\argmax{\theta, \eta \in \qdom_\z}
\left(
\log p(\y | \theta) -
    \kl{\q(\z | \eta)}{p(\z | \theta, \y)}  \right)
\\={}&
%
\argmax{\vartheta \in \qdom_{\theta}^{pm}, \eta \in \qdom_\z}
\left(
\expect{\q(\theta \vert \vartheta)}{\log p(\y | \theta)} -
\expect{\q(\theta \vert \vartheta)}{\log \q(\theta | \vartheta)} -
    \kl{\q(\z | \eta)}{p(\z | \theta, \y)}
\right)
\\={}&
\argmax{\vartheta \in \qdom_{\theta}^{pm}, \eta \in \qdom_\z}
\kl{\q(\theta \vert \vartheta) \q(\z \vert \eta)}{\log p(\z, \theta \vert \y)}.
%
\end{align*}
%

\pause

\textbf{
$\Rightarrow$ The marginal MLE is a point-mass approximation to the
posterior with a uniform prior.}
%
It will be a good approximation when $p(\theta \vert \y)$ is approximately a
point mass.

\pause

\textbf{Exercise: } Under what circumstances is $p(\theta \vert \y)$ is
approximately a point mass?

\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Different approximating families.}

Suppose we can't compute $p(\z | \theta, \y)$ and / or we think that
$p(\theta \vert \y)$ may not be well-approximated by a point mass.

Choose some tractable approximating family $\q(\theta, \z \vert \gamma) \in
\qdom_{\theta\z}$.  Then find
%
\begin{align*}
%
\hat\gamma := \argmin{\gamma \in \qdom_{\theta\z}}
\kl{\q(\theta, \z \vert \gamma)}{p(\theta, \z \vert \y)}.
%
\end{align*}
%
\pause

\textbf{Now we're doing ``Variational Bayes'' (VB).}

\pause

The EM algorithm --- and, indeed, the MLE --- can be understood as Variational
Bayes with a uniform prior and particular choices of approximating
distributions.

\end{frame}


\begin{frame}{Different approximating families.}

Some common approximating families:
%
\begin{itemize}
%
\item Factorizing families, e.g.
$\q(\theta, \z \vert \gamma) = \q(\theta \vert
\gamma) \q(\z \vert \gamma)$.  These families model some components
of the posterior as independent.
\begin{itemize}
\item For historical reasons, this is known as a
\textbf{mean-field approximation} \citep{wainwright2008graphical}.
\end{itemize}
%
\item Factorizing families + an exponential family assumption.
%
\item Normal approximations (possibly after an invertible
unconstraining transformation): $\q(\theta, \z \vert \gamma) =
\mathcal{N}(\theta, \z | \gamma)$.
%
\item Independent normal approximations.  This is used by a lot of
``black-box VI'' methods \citep{ranganath2014black, kucukelbir2017automatic}.
%
\end{itemize}
%
\pause

\hrulefill

What do you need from an approximating family?  Expressivity, plus:
%
\begin{align*}
%
\kl{\q}{\p} :=
\underbrace{
    \expect{\q(\xi | \eta)}{\log \q(\xi | \eta)}
}_{\textrm{Tractable entropy}}
\quad     - \quad
\underbrace{
    \expect{\q(\xi | \eta)}{\log \p(\xi)}
}_{\textrm{Tractable expectations}}
%
\end{align*}
%

\pause

\begin{itemize}
\item Monte Carlo is often used for the expectations.
\begin{itemize}
    \item See, e.g., \citet{ranganath2014black}.
\end{itemize}

\item The entropy is harder.  In general, there is a tradeoff between expressivity
and tractable entropy.
\begin{itemize}
\item ``Normalizing flows'' are an example of a highly expressive approximating
family (neural nets!) designed to maintain a tractable entropy.
\citep{rezende2015variational}
\end{itemize}
\end{itemize}



\end{frame}
