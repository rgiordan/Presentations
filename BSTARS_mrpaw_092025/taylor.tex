
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[t]{Generalized covariate balance for MrP}

% To apply GCB to MrP, we need two things:
% %
% \begin{itemize}
%     \item To define a $\ytil$ with $\expect{}{\ytil \vert \x} = \expect{}{\y \vert \x} + \delta f(\x)$
%     \item To evaluate $\muhat_\mrp(\ytil) - \muhat_\mrp(\y)$.
% \end{itemize}
% %
\textbf{Step one:} Construct
$\ytil$ such that $\expect{}{\ytil \vert \x} = \expect{}{\y \vert \x} + \delta f(\x)$.
\pause

\textbf{Problem:} Our $\y$ is binary!  (We're motivated by hierarchical linear regression.)

\pause
Two possibilities:
%
\begin{itemize}
    \item Allow $\ytil$ to take values other than $\{0,1\}$ and set
        $\ytil = \y + \delta \f(\x)$, or
    \item Use an estimate of $\expect{}{\y \vert \x}$ to draw new binary $\ytil$.
\end{itemize}
%
We define theory and methods for the first, and provide tools for generating data
using the second method for potentially problematic regressors.


\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[t]{Generalized covariate balance for MrP}

\textbf{Step one:} Take $\ytil = \y + \delta f(\x)$.\\
\textbf{Step two:} Evaluate $\muhat_\mrp(\ytil) - \muhat(\y)$.

\pause

\textbf{Problem:} $\muhat_\mrp(\cdot)$ is computed with MCMC.
%
\begin{itemize}
\item Takes hours to re-run, and
\item Output is noisy, and $\muhat_\mrp(\ytil) - \muhat(\y)$ may be small.
\end{itemize}
%

\pause
\begin{block}{Taylor series}
Form the approximation
$$
\muhat_{\mrp}(\ytil) =
    \sumsur \w_s^\mrp (\ytil_s  - \y_s) + \mathrm{Residual}
\quad\textrm{where}\quad
    \wmrp_s := \frac{d}{d\y_s} \muhat_{\mrp}(\y). %= \meantar \frac{d}{d\y_s} \yhat_p.
$$
\end{block}

\only<3>{
If MrP were linear (e.g.~if you use OLS instead of hierarchical
logistic regression), then
%
\begin{itemize}
\item The residual is zero,
\item $\muhat_{\mrp}(\y) = \sumsur \w_s^\mrp \y_s$, and so
\item $\muhat_{\mrp}(\ytil)$ is a calibration weighting
estimator, and $\wmrp_s$ are its weights.  (Cite Gelman)
\end{itemize}
%
In general, MrP is truly nonlinear. The residual is only small when $\ytil \approx \y$
(i.e., when $\delta \ll 1$).
}

\only<4->{
It happens that the needed derivatives are given
by simple a posterior covariances involving only the inverse
link function $m(\x; \theta)$ and
log likelihood \citep{giordano:2018:covariances}.
\\[1em]
These can be computed using standard MCMC software (e.g.~\citet{brms}).
}



\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[t]{Covariate balance}

\begin{block}{Theorem}
    %
    \begin{itemize}
        \item Let $\ytil = \y + \delta \f(\x)$,
        \item $\muhat_{\mrp}$ be a hierachical logistic regression posterior expectation, and
        % \item $\mathcal{F}$ a class of functions such that
        %     $\sup_{\f \in \mathcal{F}} \expect{}{\norm{\x \f(\x)}_2^2} < \infty$
        \item $\mathcal{F}$ be a Donsker class of uniformly bounded functions on $\x$.
    \end{itemize}
    %
    Then, with probability approaching one, as $N \rightarrow \infty$,
    $$
    \begin{aligned}
        \sup_{\f \in \mathcal{F}} \left(
            \muhat_\mrp(\ytil) -
            \left(\muhat_\mrp(\y) + \sumsur \w_s^{\mrp} \delta \f(\x_s) \right)
            \right)
        = O(\delta^2)
        \quad\textrm{as }\delta \rightarrow 0
    \end{aligned}
    $$
\end{block}

The supremum over $\mathcal{F}$ is the primary technical contribution!
It means we are justified in searching over regressors
to find imbalance.

Draws on our prior work on uniform and finite--sample error bounds for Bernstein--von Mises
theorem--like results \citep{giordano:2024:bayesij,kasprzak:2025:laplace}.

% In practice, compute
% $$
% \textrm{Imbalance}(\f) := \sumsur \w_s^{\mrp} \f(\x_s)  - \meantar \f(\x_p)
% $$
% for any $\f(\cdot)$ you think might capture variability in $\y$.

\end{frame}




