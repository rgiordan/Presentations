
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[t]{Generalized covariate balance for MrP}

\textbf{Step one:} Construct
$\ytil$ such that $\expect{}{\ytil \vert \x} = \expect{}{\y \vert \x} + \delta f(\x)$.
\pause

\textbf{Problem:} Our $\y$ is binary!  (We're motivated by hierarchical linear regression.)

\pause
Two possibilities:
%
\begin{itemize}
    \item Allow $\ytil$ to take values other than $\{0,1\}$ and set
        $\ytil = \y + \delta \f(\x)$, or
    \item Use an estimate of $\expect{}{\y \vert \x}$ to draw new binary $\ytil$.
\end{itemize}
%
Our approach:
%
\begin{itemize}
    \item Use $\ytil = \y + \delta \f(\x)$ to identify problematic ``imbalanced'' $\f(\x)$
    \item Sanity check by generating binary $\ytil$ using $\f(\x)$ (which is fast and easy)
\end{itemize}


\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[t]{Generalized covariate balance for MrP}

\textbf{Step one:} Construct
$\ytil$ such that $\expect{}{\ytil \vert \x} = \expect{}{\y \vert \x} + \delta f(\x)$.\\
\pause
\textbf{Step two:} Evaluate \surcol{$\muhat_\mrp(\ytil) - \muhat(\y)$}.
\pause

\textbf{Problem:} \surcol{$\muhat_\mrp(\cdot)$} is computed with MCMC.
%
\begin{itemize}
\item Each MCMC run typically takes hours, and
\item Output is noisy, and \surcol{$\muhat_\mrp(\ytil) - \muhat(\y)$} may be small.
\end{itemize}
%

\pause
\begin{block}{MrP Local Equivalent Weights (MrPlew)}
Form the approximation
\surcol{
$$
\muhat_{\mrp}(\ytil) =
    \sumsur \w_i^\mrp (\ytil_i  - \y_i) + \mathrm{Residual}
\quad\textrm{where}\quad
    \wmrp_i := \frac{d}{d\y_i} \muhat_{\mrp}(\y). %= \meantar \frac{d}{d\y_s} \yhat_p.
$$
}
\end{block}

\only<5>{
    \textbf{Computation: }
The weights are given by weighted averages of posterior covariances  \footcite{giordano:2018:covariances}.
\\[1em]
They can be easily computed with standard software\footnote{We use \texttt{brms} \parencite{brms}.}
\textbf{without re--running MCMC}.\\
}


\only<6>{
    %
    \textbf{Theory: }
We state conditions under which, as $\delta \rightarrow 0$, and $N \rightarrow \infty$,
\begin{itemize}
\item The residual is of lower order than the MrPlew term,
\item \textit{Uniformly} over a very wide class of $\f(\cdot)$.
\end{itemize}
%
\textbf{Uniformity} is the hard part, but this justifies
using MrPlew to \textit{identify} problematic $\f(\cdot)$.

Builds on earlier work on uniform error bounds
for Bernstein--von Mises theorem(--ish) results
\footcite{giordano:2024:bayesij,kasprzak:2025:laplace}.
}


\only<7>{
If MrP were linear (e.g.~if you use OLS instead of hierarchical
logistic regression), then
%
\begin{itemize}
\item The residual is zero,
\item $\muhat_{\mrp}(\y) = \sumsur \w_i^\mrp \y_i$, and so
\item $\muhat_{\mrp}(\ytil)$ is a calibration weighting
estimator, and $\wmrp_i$ are its weights.  (Cite Gelman)
\end{itemize}
%
In general, MrP is truly nonlinear. The residual is only small when $\ytil \approx \y$
(i.e., when $\delta \ll 1$).
}




\end{frame}

