


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[t]{Data re-weighting.}

    \question{How can we use the approximation?}
    
    % Assume the \blue{slope} is computable and \red{error} is small.
    % %
    % \expansion{}
    
    
    \pause
    \textbf{Cross validation.} 
    Let $\w_{(-n)}$ leave out point $n$, and loss
    $f(\theta) = -\ell(\x_n \vert \theta)$.
    %
    %
    \begin{align*}
    \textrm{LOO CV loss at point }n =
        \expect{p(\theta \vert \x, w_{(-n)})}{f(\theta)}
    \red{\approx} 
        \expect{p(\theta \vert \x)}{f(\theta)} - \blue{\infl_{n}}
    \end{align*}
    
    
    
    \pause
    \textbf{Example: Approximate bootstrap.}
    
    Draw bootstrap
    weights $\w \sim \p(\w) = \textrm{Multinomial}(N, N^{-1})$.
    %
    \begin{align*}
    %
    \textrm{Bootstrap variance} ={}&
    \var{\p(\w)}{\expect{p(\theta \vert \x, w)}{f(\theta)}} 
    \\\red{\approx}{}&
      \var{\p(\w)}{\expect{p(\theta \vert \x)}{f(\theta)} + \blue{\infl_n}(\w_n - 1)} 
    \\={}& \sumn \left(\blue{\infl_n} - \overline{\blue{\infl}}\right)^2.
    %
    \end{align*}
    %
    \pause
    \textbf{Influential subsets:
    Approximate maximum influence perturbation (AMIP).}
    
    Let $W_{(-K)}$ denote weights leaving out $K$ points.
    %
    \begin{align*}
    %
    \max_{\w \in W_{(-K)}} \left(
    \expect{p(\theta \vert \x, w)}{f(\theta)} -
    \expect{p(\theta \vert \x)}{f(\theta)}
    \right) \red{\approx} - \sum_{n=1}^K \blue{\infl_{(n)}}.
    %
    \end{align*}
    %
    
    % \pause
    
    % \question{How to compute the slopes \blue{$\infl_n$}?\\
    % How large is the error \red{$\err(\w)$}?}
    
\end{frame}
    

    



\begin{frame}[t]{Example: A negative binomial model}


Consider $\p(\xvec \vert \gamma) = \prod_{n=1}^N \textrm{NegativeBinomial}(\x_n \vert \gamma)$. Here, $\theta = \gamma$ is a scalar.  

\pause
As $N \rightarrow \infty$, $\p(\gamma \vert \xvec)$ concentrates at rate $1 / \sqrt{N}$ (Bernstein--von Mises).

$\Rightarrow 
N \left( \expect{\p(\gamma \vert \xvec, \w_n)}{\gamma} -
\expect{\p(\gamma \vert \xvec)}{\gamma} \right) = \blue{\infl_n} (\w_n - 1) + \red{O_p(N^{-1})}.
$


\pause
\vspace{1.5em}
\LowDimAccuracyGraph{}

\pause
\question{\textbf{Problem:} Most computationally hard Bayesian problems don't concentrate.}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Experiments}

Example: \textbf{Poisson model with random effects (REs)
$\lambda$ and fixed effect $\gamma$.}

\HighDimAccuracyGraph{}

\end{frame}


\begin{frame}{A contradiction?}
    %
    \vspace{-1em}
    \begin{align*}
    %
    &\text{\textbf{Negative binomial observations.}}
    &
    &\text{\textbf{Poisson observations with random effects.}}
    \\
    &\text{\textbf{Asymptotically linear in $\w$.}}
    &
    &\text{\textbf{Asymptotically non-linear in $\w$.}}
    %
    \onslide<3->{
    \\
    &\log \p(\xvec \vert \gamma, \w^{m}) =
        \sumn \w^{m}_n \log \p(\x_n \vert \gamma)
    &
    &\log \p(\xvec \vert \gamma, \lambda, \w^{c}) =
        \sumn \w^{c}_n \log \p(\x_n \vert \lambda, \gamma)
    }
    %
    \end{align*}
    
    \onslide<2->{
    \begin{center}
        % \large
    With a constant regressor, Gamma REs, and one RE per observation,\\
    these are the same model, with the same $\p(\gamma \vert \xvec)$.
    
    % \spskip
    \textbf{Is $\expect{\p(\gamma \vert \xvec, \w)}{\gamma}$
    linear in the \only<1-2>{data weights}\only<3->{\red{data weights}}
    or not?}
    }
    
    \onslide<3->{
    \pause
    \spskip
    \textbf{\red{Trick question!}}  We weight a log likelihood
    contribution, not a datapoint.
    
    % % \spskip
    \textbf{The two weightings are not equivalent in general.}
    }
    
    \end{center}
    %
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Experimental results}
    Our results were actually computed on \textbf{identical datasets}
    with $G = N$ and $g_n=n$.

    \begin{center}
    \begin{minipage}{0.34\textwidth}
        Uses $\log \p(\x_n \vert \gamma)$:\\
        $\infl_n = \expect{\p(\gamma \vert \xvec)}{\gammabar \ellbar_n(\gamma)}$

        \onslide<2->{
        \spskip
        Not computable from\\
        $\gamma, \lambda \sim \p(\gamma, \lambda \vert \xvec)$\\
        in general.
        }
    \end{minipage}
    \begin{minipage}{0.65\textwidth}
        \LowDimAccuracyGraph{}
        % \HighDimAccuracyGraph{}
    \end{minipage}
    %

    \begin{minipage}{0.34\textwidth}
        Uses $\log \p(\x_n \vert \gamma, \lambda)$:\\
        $\infl_n = \expect{\p(\gamma,\lambda \vert \xvec)}{\gammabar \ellbar_n(\gamma, \lambda)}$

        \onslide<2->{
        \spskip
        Computable from\\
        $\gamma, \lambda \sim \p(\gamma, \lambda \vert \xvec)$.
        }

        \onslide<3->{
        \spskip
        May still be useful when $\p(\lambda \vert \xvec)$
        is {\em somewhat} concentrated.
        }

    \end{minipage}
    \begin{minipage}{0.65\textwidth}
        %\LowDimAccuracyGraph{}
        \HighDimAccuracyGraph{}
    \end{minipage}
    %
\end{center}
    %
\end{frame}

