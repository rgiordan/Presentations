This work is about detecting when you can change a conclusion by dropping a small proportion of your data.  So let's begin by talking about when you might care.  There are circumstances when you won't!  But we argue that you might care in the sort of situation that often occurs in economics, as well as applications of statistics and machine learning.

I think the easiest way to think about this is by first considering collecting data in absence of random noise.  Let’s give two examples, with no randomness, in the same setting, where you would not care for one use case and care for another.  Suppose you are interested in whether or not a particular farm yielded more than 170 bushels per acre.  At harvest time, you actually go out and collect all the soybeans and measure exactly 200 bushels per acre.

Consider scenario one.  Maybe you care because that would give you a certain amount of money at a price you've locked in.  In comes a fancy statistician who has read our paper, and points out that, because some acres produced more than others, you could change the average by dropping 1% of your acres.  Do you care?  Of course not!  You have measured precisely the quantity you care about and drawn a conclusion.

Now let’s consider a different scenario.  Suppose, in contrast, that you are interested not in your particular farm, but whether some method you have used increases yield above 170 bushels per acre, perhaps because you want to recommend your method to your friends on the other
side of the valley.  Now your neighbor might care that your conclusion can be reversed by removing only a few acres.  Fine, they might say, you have measured 200 bushels per acre on average, but 1% of your acres are next to a wetland and have much higher yield than any of my acres; if you take them out, your conclusion changes.

Neither of these examples contain any random sampling.  We care about whether our conclusions depend on a small proportion of the data because, if it does, it affects our willingness to generalize to populations that may differ from ours, possibly in difficult-to-formalize ways.  Of course, we do careful experimental design, including randomization, to try to make our samples as representative as possible.  Nevertheless, I'd argue that we often read studies in statistics and machine learning with an interest in generalizing beyond the particular population.


What sorts of specific circumstances can motivate concern about sensitivity to data dropping?

- The policy population is different from the measured population.  Most development economists don't read a study of microcredit in Mexico because they're interested in Mexico, but because they're interested in microcredit.  Random sampling from Mexico doesn't fully capture the variability we might expect between countries.

- Small fractions of the data may be missing not-at-random.  It is difficult, even in carefully designed RCTs, to guarantee full compliance.  If we show that dropping small fractions of the existing dataset can cause big changes, we might be concerned that we have already lost some conclusion-changing data.

- Sometimes we report a convenient proxy of what we actually care about.  For example, the analytical output of a classical RCT is an average counterfactual effect, but the average might not be an adequate summary of everything we care about.  In the context of development economics, it would be great if microcredit increases everyone’s profit.  But the same average increase could be undesirable if it were produced by the poor getting poorer, and the rich getting so much richer that they offset the losses to the poor.  By ensuring robustness against dropping small data subsets, we can ensure robustness against heterogeneity of effects without modeling it formally.

- Models are necessarily misspecified.  This is a bit of a catch-all, but the point is that frequentist variability and model robustness checks presume that you're not missing important features of the data distribution.

In our paper, we focus on examples from economics partly because of great data sharing and reproducibility.  However, these concerns are quite general, and potentially apply to many applications of machine learning and statistics.

------------------------------------------

Why do we need an approximation?  Say (as in the Mexico dataset) that you have about 16k observations.  We wish to ask: can I find a group of no more than 16 points which can reverse my conclusion?  This is precisely a combinatorial optimization problem, where the search space is all sets of 16 or fewer points, and the objective function is some aspect of your original estimation procedure. The search space is vast --- there are about 10^53 sets of size exactly 16. Each evaluation of your objective function requires re-running your original estimation problem.  Even if your original estimation problem could be computed in one second (much faster than a neural net could be trained!), an exhaustive search would take 10^46 years.  It's ridiculous.  Either you need to find special structure (which may be hard, even in simple cases like OLS), or you need an approximation.  So we provide an approximation.


Our approximation works in two steps:
- Represent dropping data via _data reweighting_
- Form a Taylor series approximation to the dependence of your estimator on the continuous data weights

Going back to the combinatorial optimization perspective, we perform a single gradient step in a continuous relaxation of the combinatorial space.

We provide finite-sample, non-stochastic error bounds on the Taylor series expansion. However, one can easily check the accuracy of our approximation in a particular application with no need to rely on our theory.  Our method returns a set of data which we predict will produce a particular change when dropped.  If you can actually re-run your estimation procedure leaving out our selected points, then any change you observe is definitive.  Contrast this single re-run with needing to re-run for all 10^53 subsets.

Our Taylor series approximation works for Bayesian estimators like variational Bayes or MAP estimation, as well as traditionally frequentist estimators like OLS or the generalized method of moments.  In fact, our approximation works for all minimizers of smooth empirical loss.



One might hope that Bayesian procedures would be more robust than frequentist procedures.  Our theory and experiments suggest that this is not necessarily the case.  In particular, we investigated a Bayesian hierarchical model published by one of our co-authors, Rachael Meager.  The model was designed to perform meta-analysis on seven RCTs studying microcredit, including the Mexico study, incorporating realistic priors, hierarchical shrinkage and a more realistic error distribution.

In order to apply our data-dropping approximation, we fit the model using a mean-field VB approximation, and verified that our results matched the output of MCMC.  Mean field VB is known to badly under-estimate posterior variances, and this application was no exception, so we used linear response covariances from our earlier work, Giordano 2018, to quantify posterior uncertainty.

Once we had reliable VB posterior approximations, we applied our methods to investigate the sensitivity of the posterior mean microcredit effectiveness. We found that we could change the sign of the posterior expected average efficacy of microcredit by dropping less than 0.1% of the data — a degree of sensitivity comparable to OLS.




In our paper we also studied a number of other applications.  First, we studied the famous Oregon Medicaid experiment, in which individuals were randomly assigned to receive access to Medicaid or not.  The Oregon Medicaid experiment found significant effects on all health outcomes, but this statistical significance was quite non-robust.  For example, one health outcome’s significance level could be changed by dropping only 11 out 21k data points, which is only 0.05% of the data.

Not all applications were non-robust.  For example, when investigating Angelucci et al. (2009), we found robust effects of cash transfers on poor households, although the effect of cash transfers on wealthy households was non-robust.




So what is driving this variety of results?  In contrast to what you might expect, we show that, unlike standard errors, sensitivity to dropping small data subsets is not mitigated by large sample sizes.  Further, unlike classical gross-error robustness, it’s not primarily driven by outliers nor misspecification.  Rather, we show that what makes an estimator sensitive is a small "signal to noise ratio" --- in other words, the ratio of the size of the effect you're trying to measure to the sampling variability of each data point.  Our analysis applies equally to Bayesian and non-Bayesian procedures.  For all this and more, we encourage you to look at the paper for yourself.
