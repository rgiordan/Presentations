\documentclass[8pt]{beamer}\usepackage[]{graphicx}\usepackage[]{color}


\usetheme{metropolis}           % Use metropolis theme
\usepackage{amsmath}
\usepackage{mathrsfs}
\usepackage{tabularx}

\usepackage{mathtools}

\usepackage{cleveref}

% For custom oversets
\usepackage{accents}

% Algorithm notation
\usepackage{algorithm}
\usepackage{algpseudocode}
% algorithmicx package

\input{_math_macros}
\input{_knitr_header}

\title{Deterministic Automatic Differentiation Variational Inference}
\author{Ryan Giordano}
\date{Sep 23rd, 2022}
\institute{Massachusetts Institute of Technology}

\begin{document}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Problem statement}

We all want to do accurate Bayesian inference quickly:
%
\begin{itemize}
    \item In terms of compute (wall time, model evaluations, parallelism)
    \item In terms of analyst effort (tuning, algorithmic complexity)
\end{itemize}
%
\textbf{Markov Chain Monte Carlo (MCMC)} can be
straightforward and accurate but slow.

\vspace{-0.5em}
\hrulefill

\textbf{Black Box Variational Inference (BBVI)} can be faster alternative to MCMC.
%
\textbf{But...}

%
\begin{itemize}
    \item BBVI is cast as an optimization problem with an intractable objective $\Rightarrow$
    \item Most BBVI methods use \textbf{stochastic gradient} (SG) optimization $\Rightarrow$
    %
    \begin{itemize}
        \item SG algorithms can be hard to tune
        \item Assessing convergence and stochastic error can be difficult
        \item SG optimization can perform worse than second-order methods on tractable objectives
    \end{itemize}
    %
    \item Many BBVI methods employ a \textbf{mean-field (MF) approximation} $\Rightarrow$
    %
    \begin{itemize}
        \item Posterior variances are poorly estimated
    \end{itemize}
    %
\end{itemize}
%
\vspace{-0.5em}
\hrulefill

\textbf{Our proposal:}
replace the intractable BBVI objective with a fixed approximation.
%
\begin{itemize}
    \item Better optimization methods can be used (e.g. true second-order methods)
    \item Convergence and approximation error can be assessed directly
    \item Can correct posterior covariances with linear response covariances
    \item This technique is well-studied (but there's still work to do in the context of BBVI)
\end{itemize}
%
$\Rightarrow$
\textbf{Simpler, faster, and better BBVI posterior approximations ... in some cases.}
%
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Outline}
%
\begin{itemize}
    \item BBVI Background and our proposal
    \begin{itemize}
        \item Automatic differentiation variational inference (ADVI) (a BBVI method)
        \item Our approximation: ``Deterministic ADVI'' (DADVI)
        \item Linear response (LR) covariances
        \item Estimating approximation error
    \end{itemize}
    \item Experimental results: DADVI vs ADVI
    \begin{itemize}
        \item DADVI converges faster than ADVI, and requires no tuning
        \item DADVI's posterior mean estimates' accuracy are comparable to ADVI
        \item DADVI+LR provides more accurate posterior variance estimates than ADVI
        \item DADVI provides accurate estimates of its own approximation error
        \item ADVI often results in better objective function values (eventually)
    \end{itemize}
    \item Why don't we do DADVI all the time?
    \begin{itemize}
        \item DADVI fails for expressive BBVI approximations (e.g. full-rank ADVI)
        \item Pessimistic dimension dependence results from optimization theory
        \item ...which may not apply in certain BBVI settings.
    \end{itemize}
\end{itemize}
%
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[t]{Notation}
%
%
\begin{align*}
    \text{Parameter:}& \quad \theta \in \mathbb{R}^{\thetadim}\\
    \text{Data:}&\quad \y \\
    \text{Prior:}&\quad \p(\theta) 
        \quad \text{(density w.r.t. Lebesgue $\mathbb{R}^\thetadim$, 
            nonzero everywhere)}\\
    \text{Likelihood:}&\quad\p(\y \vert \theta)
    \quad \text{(nonzero for all $\theta$)}\\
\end{align*}


We will be interested in means and covariances of the (intractable) posterior
%
\begin{align*}
%
\post ={}& \frac{\p(\theta, \y)}{\int \p(\theta', \y) d\theta'}.
%
\end{align*}
%


Denote gradients with $\nabla$, e.g.,
%
\begin{align*}
%
\logjointgrad := \fracat{\partial \logjoint}{\partial \theta}{\theta}
\quad\text{and}\quad
\logjointhess := \fracat{\partial^2 \logjoint}
    {\partial \theta \partial \theta^\trans}{\theta}
%
\end{align*}
%

Assume we have a twice auto-differentiable software implementation of
%
\begin{align*}
    \theta \mapsto \logjoint = \log \p(\y \vert \theta) + \log \p(\theta).
\end{align*}


\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{frame}[t]{Notation}
%
% \begin{align*}
%     \text{Parameter: }& \theta \in \mathbb{R}^{\thetadim} &
%     \text{Data: }& \y &
%     \text{Log joint: }& \logjoint
% \end{align*}
% \hrulefill


\textbf{Automatic differentiation variational inference (ADVI)}
is a particular BBVI method.

ADVI specifies a family $\qdom$ of $\thetadim$-dimensional
Gaussian distributions.

The family $\qdom$ is parameterized by 
$\eta \in \etadom$, 
encoding the means and covariances.

The covariances of the family $\qdom$ can either be
%
\begin{itemize}
\item Diagonal: ``Mean-field'' (MF) approximation, $\etadim = 2 \thetadim$
\item Any PD matrix: 
    ``Full-rank'' (FR) approximation, 
        $\etadim = \thetadim + \thetadim  (\thetadim - 1) / 2$
\end{itemize}
%
% ADVI tries to find
%
\begin{align*}
%
\argmin_{\q \in \qdom} 
\mathrm{KL}\left(\q(\theta | \eta) || \post \right) ={}&
\argmin_{\eta \in \etadom} \klfullobj{\eta} 
\\ \textrm{where }
\klfullobj{\eta} :={}&
\expect{\q(\theta \vert \eta)}{\log \q(\theta \vert \eta)}
-\expect{\q(\theta \vert \eta)}{\logjoint}
\\={}&
\expect{\normz}{\log \q(\theta(\z, \eta) \vert \eta)} -
\undernote{\expect{\normz}{\log \p(\theta(\z, \eta), \y)}}
{\text{Typically intractable}}.
%
\end{align*}
%
The final line uses the ``reparameterization trick''
with standard Gaussian $\z \sim \normz$. 

\hrulefill

ADVI is an instance of the general problem of finding
%
\begin{align*}
%
\argmin_{\eta} F(\eta)
\text{ where } F(\eta) := \expect{\normz}{f(\eta, \z)}.
%
\end{align*}


\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{frame}{Two approaches}
%
%
\vspace{-1em}
\begin{align*}
%
\text{Consider }\quad
\argmin_{\eta} F(\eta)
\quad\text{ where } \quad F(\eta) := \expect{\normz}{f(\eta, \z)}.
%
\end{align*}
%
Let $\Z = \{\z_1, \ldots, \z_\znum \} \iid \normz$, and let
%
%\begin{align*}
%
$
\hat{F}(\eta | \Z) := \meann f(\eta, \z_n).
$
%
%\end{align*}
%
%
\begin{minipage}{1.0\linewidth}
    \begin{minipage}[t]{0.49\linewidth}
        %
        \begin{algorithm}[H]
        \caption{\\Stochastic gradient (SG)\\ADVI (and most BBVI)}\label{alg:sadvi}
        \begin{algorithmic}
        % \Procedure{SG}{}
        \State Fix $\znum$ (typically $\znum = 1$)
        \State $t \gets 0$
            \While{Not converged}
                \State $t \gets t + 1$
                \red{\State Draw $\Z$}
                \State $\Delta_S \gets \grad{\eta}{\hat{F}(\eta_{t-1} | \Z)}$
                \blue{\State $\alpha_t \gets \textrm{SetStepSize(Past state)}$}
                \State $\eta_t \gets \eta_{t-1} - \alpha_t \Delta_S$
                \green{\State $\textrm{AssessConvergence(Past state)}$}
            \EndWhile
            % \State $\etaopts \gets \eta_t$ or
            %     $\etaopts \gets \frac{1}{M} \sum_{t'=t- M}^t \eta_{t'}$
            \State \Return 
            $\eta_t$ or
                $\frac{1}{M} \sum_{t'=t- M}^t \eta_{t'}$
        % \EndProcedure
        \end{algorithmic}
        \end{algorithm}
        %
    \end{minipage}
    \begin{minipage}[t]{0.49\linewidth}
        %
        \begin{algorithm}[H]
        \caption{\\Sample average approximation (SAA)\\Deterministic ADVI (DADVI) (proposal)}
        \label{alg:dadvi}
        \begin{algorithmic}
        % \Procedure{DADVI}{}
            \State Fix $\znum$ (our experiments use $\znum = 30$)
            \State \red{Draw $\Z$}
            \State $t \gets 0$
            \While{Not converged}
                \State $t \gets t + 1$
                \State
    \blue{$\Delta_D \gets \textrm{GetStep}(\hat{F}(\cdot | \Z), \eta_{t-1})$}
                \State $\eta_t \gets \eta_{t-1} + \Delta_D$
                \State \green{$\textrm{AssessConvergence}(\hat{F}(\cdot | \Z), \eta_{t})$}
            \EndWhile
            % \State $\etaoptd \gets \eta_t$
            \State \Return $\eta_t$
        \end{algorithmic}
        \end{algorithm}
    \end{minipage}
\end{minipage}

% \begin{minipage}{1.0\linewidth}
%     \begin{minipage}[t]{0.49\linewidth}
%         {\small ADVI approximately minimizes an exact objective.}
%     \end{minipage}    
%     \begin{minipage}[t]{0.49\linewidth}
%         SAA exactly minimizes an approximate objective.
%     \end{minipage}    
% \end{minipage}    

\textbf{Our proposal: } Apply \cref{alg:dadvi} with the ADVI objective.  \\
Take \blue{better steps}, easily \green{assess convergence}, with less tuning.
    
\end{frame}



\begin{frame}{Linear response covariances}
%
Posterior variances are often badly estimated by mean-field (MF) approximations.

Take a variational approximation 
$\etastar := \argmin_{\eta \in \etadom} \klfullobj{\eta}$.  Often,

\begin{align}\label{eq:mfvb_conceit}
    %
    \expect{\q(\theta \vert \etastar)}{\theta} \approx
    \expect{\post}{\theta} \quad\textrm{but}\quad
    \var{\q(\theta \vert \etastar)}{\theta} \ne
    \var{\post}{\theta}.
\end{align}    
%
\textbf{Example: }Correlated Gaussian $\post$ with ADVI.

\hrulefill

\textbf{Linear response covariances} use the fact that, if
$\p(\theta \vert \y, t) \propto \p(\theta \vert \y) \exp(t \theta)$, then
%
\begin{align}\label{eq:deriv_is_cov}
    %
    \fracat{d \expect{\p(\theta \vert \y, t)}{\theta}}
           {dt}{t=0} = \cov{\p(\theta \vert \y)}{\theta}.
    %
\end{align}

Let $\etastar(t)$ be the variational approximation to $\p(\theta \vert \y, t)$, and
take
%
\begin{align*}
    \lrcovfull{\q(\theta \vert \etastar)}{\theta}
    =
    \fracat{d \expect{\q(\theta \vert \etastar(t))}{\theta}}
       {dt}{t=0}
    =
    % \fracat{\partial \expect{\q(\theta \vert \eta)}{\theta}}
    %       {\partial\eta^\trans}{\eta=\etastar}
    \left( \grad{\eta}{\expect{\q(\theta \vert \etastar)}{\theta}}\right)
    \left(\hess{\eta}{\klfullobj{\etastar}} \right)^{-1}
    \left( \grad{\eta}{\expect{\q(\theta \vert \etastar)}{\theta}}\right)
    % \fracat{\partial \expect{\q(\theta \vert \eta)}{\theta}}
    %      {\partial\eta}{\eta=\etastar}.    
\end{align*}
%
% Often, $\lrcovfull{\q(\theta \vert \etastar)}{\theta}$ is a much better 
% approximation to $\var{\post}{\theta}$ than $\var{\q(\theta \vert \etastar)}{\theta}$ is.

\textbf{Example: } For ADVI with a correlated Gaussian $\post$,
$\lrcovfull{\q(\theta \vert \etastar)}{\theta}  = \cov{\q(\theta \vert \etastar)}{\theta}$.


\end{frame}



\begin{frame}
\end{frame}


\end{document}
