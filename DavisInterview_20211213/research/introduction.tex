\begin{frame}{Dropping data: Motivation}

Suppose you're a data analyst, and you've

\begin{itemize}
    \item Gathered some exchangeable data,
    \item Cleaned up / removed outliers,
    \item Checked for correct specification, and
    \item Drawn a conclusion from your statistical analysis \\(e.g., based
    the sign / significance of some estimated parameter).
\end{itemize}

\pause
\vspace{1em}
Would you be concerned if you could \textbf{reverse your conclusion} by removing
a \textbf{small proportion} (say, $0.1\%$) of your data?

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Dropping data: Mexico Microcredit}

Consider \citet{angelucci2015microcredit}, a randomized controlled trial study
of the efficacy of microcredit in Mexico based on 16,560 data points.

The variable ``Beta" estimates the effect of microcredit in US dollars.

%\MicrocreditMexicoRerunTable{}

\begin{table}[ht]
\centering
\begin{tabular}{lll} \hline
  & Left out points & Beta (SE) \\\hline
Original & 0 & -4.55 (5.88) \\ \hline
\onslide<2-> {Change sign & 1 & 0.4 (3.19) \\\hline }
\onslide<3-> {Change significance & 14 & -10.96 (5.57) \\\hline }
\onslide<4-> {Change both & 15 & 7.03 (2.55) \\\hline }
\end{tabular}
\end{table}

\vspace{-1em}
\onslide<5-> { By removing very few data points ($15 / 16560 \approx 0.1\% $),
we can reverse the qualitative conclusions of the original study! }

% \onslide<6->{
% Do you care?  \textbf{Maybe not!}  But, often in economics:
% %
% \begin{itemize}
% \item Policy population is different from analyzed population,
% \item We report a convenient summary (e.g. mean) of a complex effect,
% \item Models are stylized proxies of reality.
% \end{itemize}
% }

\onslide<6-> {
\vspace{1em}
\textbf{Question:} Is the reported interval $-4.55 \pm (5.88)$ a reasonable
description of the uncertainty in the estimated efficacy of microcredit?
}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[t]{Can Dropping a Little Data Make a Big Difference?}

Do you care whether you can \textbf{reverse your conclusion} by removing
a \textbf{small proportion} of your data?

\vspace{1em}
\textbf{Not always!}

\vspace{1em}
\textbf{...but sometimes, surely yes.}

\vspace{1em}
For example, it often occurs that:
%
\begin{itemize}
\item Policy population is different from analyzed population,
\item Small fractions of data are missing not-at-random,
\item We report a convenient summary (e.g. mean) of a complex effect,
\item Models are stylized proxies of reality.
\end{itemize}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[t]{Can Dropping a Little Data Make a Big Difference?}

\textbf{How do we find influential datapoints?}

\only<1>{

\vspace{1em}
The number of subsets ${N \choose \lfloor \alpha N \rfloor}$ can be very large
even when $\alpha$ is small.

\vspace{1em}
In the MX microcredit study, ${16560 \choose 15} \approx 1.4 \cdot 10^{51}$
for $\alpha = 0.0009$.

\vspace{1em}
We provide a fast, automatic approximation based on the
\textbf{empirical influence function}.

\vspace{1em}
Though we provide finite-sample, non-stochastic accuracy guarantees,
there is no need to rely on our theory.  A single re-fit provides an exact
lower bound on sensitivity.
}

\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[t]{Can Dropping a Little Data Make a Big Difference?}

\textbf{What causes sensitivity to dropping small fractions of the data?}

\vspace{1em}
We consider a number of studies, including seven studies of microcreidt, the
Medicaid experiment, a study of cash transfers, and a number of models,
including OLS, IV, and a hierachical Bayesian model. Some analyses were robust,
and others were not.

\vspace{1em}
\textbf{What drives the variety of results?}  We show that sensitivity
to dropping small subsets is:

\only<1>{
\begin{itemize}
    \item Not (necessarily) caused by misspecification.
    \item Not (necessarily) caused by outliers.
    \item Not captured by standard errors.
    \item Not mitigated by large $N$.
    \item Primarily determined by the \textbf{signal to noise} ratio
    \begin{itemize}
        \item[] ... that is, the ratio of the measured effect size to data
        variability.
    \end{itemize}
\end{itemize}
}


\end{frame}
