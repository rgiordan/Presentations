Would you care if dropping a small fraction of your data could overturn
your analyses substantive conclusions?  Example: microcredit.

There are certainly cases when you wouldn't care.  But when you intend
to generalize from one population to another, you probably do care.

This is like a combinatorial optimization problem, where the search
space is the number of sets, and the objective requires re-fitting your
objective.  We provide an approximation based on the influence function.

The approximation comes with guarantees, but you don't need to rely on them,
you can re-fit once.  For example, here are the refit results on Mexico
microcredit.  This was all done with our R package, zaminfluence.


There are many other data perturbations that are conceivable.
The relationship between our estimator and the influence function plays
a key role in our analysis, and would play a similar role in other analyses.

We analyze a number of studies, including Microcredit (OLS and VB), Medicaid,
and cash transfers.  Some results are robust, some not.  How to explain?
We demonstrate that sensitivity is driven by low signal to noise ratio.
