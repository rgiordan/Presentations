


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[t]{Data re-weighting.}

    \question{How can we use the approximation?}
    
    % Assume the \blue{slope} is computable and \red{error} is small.
    % %
    % \expansion{}
    
    
    \pause
    \textbf{Cross validation.} 
    Let $\w_{(-n)}$ leave out point $n$, and loss
    $f(\theta) = -\ell(\x_n \vert \theta)$.
    %
    %
    \begin{align*}
    \textrm{LOO CV loss at point }n =
        \expect{p(\theta \vert \x, w_{(-n)})}{f(\theta)}
    \red{\approx} 
        \expect{p(\theta \vert \x)}{f(\theta)} - \blue{\infl_{n}}
    \end{align*}
    
    
    
    \pause
    \textbf{Example: Approximate bootstrap.}
    
    Draw bootstrap
    weights $\w \sim \p(\w) = \textrm{Multinomial}(N, N^{-1})$.
    %
    \begin{align*}
    %
    \textrm{Bootstrap variance} ={}&
    \var{\p(\w)}{\expect{p(\theta \vert \x, w)}{f(\theta)}} 
    \\\red{\approx}{}&
      \var{\p(\w)}{\expect{p(\theta \vert \x)}{f(\theta)} + \blue{\infl_n}(\w_n - 1)} 
    \\={}& \sumn \left(\blue{\infl_n} - \overline{\blue{\infl}}\right)^2.
    %
    \end{align*}
    %
    \pause
    \textbf{Influential subsets:
    Approximate maximum influence perturbation (AMIP).}
    
    Let $W_{(-K)}$ denote weights leaving out $K$ points.
    %
    \begin{align*}
    %
    \max_{\w \in W_{(-K)}} \left(
    \expect{p(\theta \vert \x, w)}{f(\theta)} -
    \expect{p(\theta \vert \x)}{f(\theta)}
    \right) \red{\approx} - \sum_{n=1}^K \blue{\infl_{(n)}}.
    %
    \end{align*}
    %
    
    % \pause
    
    % \question{How to compute the slopes \blue{$\infl_n$}?\\
    % How large is the error \red{$\err(\w)$}?}
    
\end{frame}
    

    



\begin{frame}[t]{Example: A negative binomial model}


Consider $\p(\xvec \vert \gamma) = \prod_{n=1}^N \textrm{NegativeBinomial}(\x_n \vert \gamma)$. Here, $\theta = \gamma$ is a scalar.  

\pause
As $N \rightarrow \infty$, $\p(\gamma \vert \xvec)$ concentrates at rate $1 / \sqrt{N}$ (Bernstein--von Mises).

$\Rightarrow 
N \left( \expect{\p(\gamma \vert \xvec, \w_n)}{\gamma} -
\expect{\p(\gamma \vert \xvec)}{\gamma} \right) = \blue{\infl_n} (\w_n - 1) + \red{O_p(N^{-1})}.
$


\pause
\vspace{1.5em}
\LowDimAccuracyGraph{}

\pause
\question{\textbf{Problem:} Most computationally hard Bayesian problems don't concentrate.}

\end{frame}
