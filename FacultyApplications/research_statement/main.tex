\input{_headers.tex}
\usepackage{enumitem}
\setlist{nolistsep}

\usepackage{geometry}
%\geometry{margin=1.2in}
\geometry{top=0.8in}
\geometry{left=1.2in}
\geometry{right=1.2in}

\title{Ryan Giordano Research Statement}

\author{
  Ryan Giordano \\ \texttt{rgiordan@mit.edu }
}

\begin{document}

\begin{minipage}[t]{0.5\textwidth}
\hspace{-2em} % Easier than doing it right!
{\bf \LARGE Research Statement}\\
\end{minipage}
\begin{minipage}[t]{0.5\textwidth}
%    \begin{flushright}
        \hspace{8em} % Easier than doing it right!
        {\LARGE Ryan Giordano}
%    \end{flushright}
\end{minipage}

Many researchers would be concerned if they learned that some core conclusion of
their statistical analysis---such as the sign or statistical significance of some
key effect---could be overturned by removing a small fraction, say 0.1\%, of
their data.  Such non-robustness would be particularly concerning if the data
were not actually drawn randomly from precisely the population of interest, or if
the model may have been misspecified---circumstances that often obtain in  the
social sciences, for example.  Nevertheless, analysts do not routinely check
whether ablation of such a small set could overturn their results, in part
because the number of possible subsets containing 0.1\% of the data points is
combinatorially large.

In \citet{giordano:2020:amip} (the author order is alphabetical; I am a lead
author), I identify problematic subsets of the data using {\em sensitivity
analysis}---that is, by forming a linear approximation to how a wide class of
statistical estimators depend on their datasets.  The key idea is that, although
there are a very large number of subsets containing 0.1\% of the data points,
none of them are very different from the original dataset, and so we expect the
linear approximation to work well.  I confirm this intuition with finite-sample
accuracy bounds in terms of intuitive and verifiable assumptions.  I provide an
\texttt{R} package \citep{zaminfluence} to compute the approximation quickly and
automatically using automatic differentiation \citep{baydin:2015:automatic,
autograd}.  And, with my co-authors, I show that the approximation is capable of
detecting meaningful non-robustness in several published econometrics analyses.
For example, in a study of microcredit in Mexico
\citep{angelucci:2015:microcredit}, we find that, by removing just 15 households
out of 16,561 studied (a change of less than 0.1\%), the estimated effect of
microcredit changes from negative and statistically insignificant to positive
and statistically significant.

In fact, my research shows that many standard, computationally demanding data
analysis tasks are also amenable to fast, automatic approximation using
sensitivity analysis. For example:
%
\begin{itemize}
    %
\item Cross validation (CV) requires repeatedly leaving out subsets of the
observed data and re-evaluating a statistical estimator. By forming a Taylor
series approximation on the dependence of the estimator on the left-out set, I
provide fast approximations to CV with finite-sample accuracy guarantees
\citep{giordano:2019:ij}.
%
\item Prior specification encodes key assumptions in Bayesian statistics.  But
Bayesian inference can be sensitive to prior specification, and evaluating the
sensitivity of Bayesian posterior expectations to prior specification by
re-fitting is typically computationally prohibitive due both to the large space
of possible priors (often infinite dimensional), as well as the high
computational cost of evaluating even a single posterior approximation.  By
forming a Taylor series approximation to the dependence of the posterior mean on
the prior, I can explore the consequences of alternative prior functional forms
at a small fraction of the cost of exact re-fitting
\citep{giordano:2020:rstansensitivity, giordano:2021:bnpsensitivity}.
%
\item When analyzing randomly sampled data using possibly misspecified Bayesian
posteriors, frequentist variability in excess of posterior variability is
symptomatic of \emph{data non-robustness}. For example, one might worry that a
new random sample of poll respondents in the presidential forecast model of
\citet{economist:2020:election} would lead to a different prediction.  This
frequentist variability can be evaluated by the bootstrap, but at the
considerable cost of re-running Markov Chain Monte Carlo (MCMC) hundreds of
times.  By approximating the dependence of the posterior on the data with
sensitivity analysis, I compute accurate estimates of the frequentist variance
using only a single MCMC chain---orders of magnitude faster than the bootstrap
\citep{giordano:2020:stanconbayesij}.
%
\item Mean field variational Bayes (MFVB) is a popular posterior approximation
method for Bayesian problems which are too large to be tractable by Markov Chain
Monte Carlo \citep{blei:2017:variational, regier:2019:cataloging}.  However,
MFVB approximations provide notoriously poor estimates of posterior uncertainty
\citep{turner:2011:two}.  In \citet{giordano:2018:covariances}, I show that
accurate posterior covariances can be recovered from MFVB approximations with
sensitivity analysis by exploiting a duality between Bayesian covariances and
sensitivity.
%
\end{itemize}

For the remainder of this essay, I will discuss each of these applications,
emphasizing the ways in which I update classical results with intuitive,
relevant theory and easy-to-use computational tools.

\newpage



\subsection*{Robustness to data ablation.}

In \citet{zaminfluence} and \citet{giordano:2020:amip}, I propose a method to
assess the sensitivity of statistical analyses to the removal of a small
fraction of the sample. Analyzing all possible data subsets of a certain size is
computationally prohibitive, so I provide a finite-sample metric to
approximately compute the number (or fraction) of observations that has the
greatest influence on a given result when dropped.  At minimal computational
cost, our method provides an exact finite-sample lower bound on sensitivity for
any estimator, so any non-robustness one finds is conclusive. I demonstrate that
non-robustness to data ablation is driven by a low signal-to-noise ratio in the
inference problem, is not reflected in standard errors, does not disappear
asymptotically, and is not inherently a product of outliers or misspecification.

The approximation works for Z-estimators based on smooth estimating equations, a
class which includes ordinary least squares, instrumental variables, generalized
method of moments, variational Bayes, and maximum likelihood estimators. Using
my \texttt{R} package \citep{zaminfluence}, the approximation is automatically
computable from the specification of the estimating equation alone.  By
analyzing several published econometric analyses \citep{angelucci:2009:indirect,
finkelstein:2012:oregon, meager:2019:microcredit}, I show that even 2-parameter
linear regression analyses of randomized trials can be highly sensitive.  While
I find some applications are robust, in others the sign of a treatment effect
can be changed by dropping less than 1\% of the sample even when standard errors
are small.


\subsection*{Approximate cross validation.}

The error or variability of machine learning algorithms is often assessed by
repeatedly re-fitting a model with different weighted versions of the observed
data; cross-validation (CV) can be thought of as a particularly popular example
of this technique.
%
In \citet{giordano:2019:ij}, I use a linear approximation to the dependence of
the fitting procedure on the weights, producing results that can be faster by an
order of magnitude than repeated re-fitting. I provide explicit finite-sample
error bounds for the approximation in terms of a small number of simple,
verifiable assumptions.  My results apply whether the weights and data are
stochastic or deterministic, and so can be used as a tool for proving the
accuracy of the infinitesimal jackknife on a wide variety of problems. As a
corollary, I state mild regularity conditions under which the approximation
consistently estimates true leave-$k$-out cross-validation for any fixed $k$. I
demonstrate the accuracy of the approximation on a range of simulated and real
datasets, including an unsupervised clustering problem from genomics
\citep{Luan:2003:clustering, shoemaker:2015:ultrasensitive}.


\subsection*{Approximately bootstrapping Bayesian posterior means.}

The frequentist (i.e., sampling) variance of Bayesian posterior expectations
differs in general from the posterior variance even for large datasets,
particularly when the model is misspecified or contains many latent variables
\citep{kleijn:2006:misspecification}. Unlike the posterior variance, the
frequentist variance is meaningful even in the presence of misspecification,
particularly when the data is known to arise from random sampling
\citep{waddell:2002:bayesphyloboot}.  However, the principal existing approach
for computing the frequentist variability of MCMC procedures is the bootstrap,
which can be extremely computationally intensive due to the need to run hundreds
of extra MCMC procedures \citep{huggins:2019:bayesbag}.

In \citet{giordano:2021:bayesij, giordano:2020:stanconbayesij}, I propose an
efficient alternative to bootstrapping an MCMC procedure.  My approach is based
on the Bayesian analogue of the influence function from the classical
frequentist robustness literature.  Using results from
\citep{giordano:2018:covariances, giordano:2019:ij}, I show that the influence
function for posterior expectations can be easily computed from the posterior
samples of a single MCMC procedure and consistently estimates the bootstrap
variance. I demonstrate the accuracy and computational benefits of the influence
function variance estimates on array of experiments including an election
forecasting model \citep{economist:2020:election}, the Cormack-Jolly-Seber model
from ecology \citep{kery:2011:bayesian}, and a large collection of models and
datasets from the social sciences \citep{gelman:2006:arm}.

\clearpage
\subsection*{Bayesian sensitivity analysis.}

\paragraph{Prior sensitivity for Markov Chain Monte Carlo.}

MCMC is arguably the most commonly used computational tool to estimate Bayesian
posteriors, which is made still easier by modern black-box MCMC tools such as
\texttt{Stan} \citep{carpenter:2017:stan, rstan}.  However, a single run of MCMC
typically remains time-consuming, and systematically exploring alternative prior
parameterizations by re-running MCMC would be computationally prohibitive for
all but the simplest models.

My software package, \texttt{rstansensitivity},
\citep{giordano:2020:rstansensitivity, giordano:2018:mcmchyper}, takes advantage
of the automatic differentiation capacities of \texttt{Stan}
\citep{carpenter:2015:stanmath} together with a classical result from  Bayesian
robustness \citep{gustafson:1996:localposterior, basu:1996:local,
giordano:2018:covariances} to provide automatic hyperparameter sensitivity for
generic \texttt{Stan} models from only a single MCMC run.  I demonstrate the
speed and utility of the package in detecting excess prior sensitivity,
particularly in a social sciences model taken from \citet[Chapter
13.5]{gelman:2006:arm}.


\paragraph{Prior sensitivity for discrete Bayesian nonparametrics.}

% From BNP_sensitivity/writing/NIPS_2018_BNP_workshop
A central question in many probabilistic clustering problems is how many
distinct clusters are present in a particular dataset and which observations
cluster together. Discrete Bayesian nonparametric (BNP) mixture models address
this question by placing a generative process on cluster assignment, making the
number of distinct clusters present amenable to Bayesian inference.  However,
like all Bayesian approaches, BNP requires the specification of a prior, and
this prior may favor a greater or lesser number of distinct clusters.

In \citet{giordano:2021:bnpsensitivity}, I derive and analyze prior sensitivity
measures for variational Bayes approximations in general, with a focus on
discrete BNP models. Unlike much previous work on local Bayesian sensitivity for
BNP (e.g. \citet{Basu:2000:BNP_robustness}), I pay special attention to the
ability of the sensitivity measures to \emph{extrapolate} to different priors,
rather than treating the sensitivity as a measure of robustness \textit{per se}.
Further, though there are many valid choices for measuring the ``size'' of a
prior perturbation for exact Bayesian posteriors
\citep{gustafson:1996:localposterior}, I prove that, for variational Bayes
approximations, local approximations are valid only when based on the largest
point-wise change in the log density.  My co-authors and I apply our sensitivity
measures to a number of real-world problems, including an unsupervised
clustering problem from genomics using fastSTRUCTURE
\citep{raj:2014:faststructure}, demonstrating that the approximation is
accurate, orders of magnitude faster than re-fitting, and capable of detecting
meaningful prior sensitivity.

\subsection*{Uncertainty propagation in mean-field variational Bayes.}

Mean-field Variational Bayes (MFVB) is an approximate Bayesian posterior
inference technique that is increasingly popular due to its fast runtimes on
large-scale scientific data sets (e.g., \citet{raj:2014:faststructure,
kucukelbir:2017:advi, regier:2019:cataloging}). However, even when MFVB provides
accurate posterior means for certain parameters, it often mis-estimates
variances and covariances \citep{wang:2005:inadequacy, turner:2011:two} due to
its inability to propagate Bayesian uncertainty between statistical parameters.

In \citet{giordano:2015:linear, giordano:2018:covariances}, I derive a simple
formula for the effect of infinitesimal perturbations on MFVB posterior means,
thus providing improved covariance estimates and greatly expanding the practical
usefulness of MFVB posterior approximations. The estimates for MFVB posterior
covariances rely on a result from the classical Bayesian robustness literature
that relates derivatives of posterior expectations to posterior covariances and
includes the Laplace approximation as a special case.
% The key condition is that the MFVB
% approximation provides good estimates of a select subset of posterior means---an
% assumption that has been shown to hold in many practical settings.
In the experiments, I demonstrate that my methods are simple, general, and
fast, providing accurate posterior uncertainty estimates and robustness measures
with runtimes that can be an order of magnitude faster than MCMC, including
models from ecology \citep{kery:2011:bayesian}, the social sciences
\citep{gelman:2006:arm}, and on a massive internet advertising dataset
\citep{criteo:2014:dataset}.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\newpage

\clearpage
\subsection*{Selected Future work}

My research is ideally driven by the needs of my scientific and industry
collaborators, and so I expect my future work will be determined to a large part
by my colleagues.  However, I will now discuss a few directions that I find
promising and interesting, and which I believe could be applicable to a diverse
set of problems.

\paragraph{The higher-order infinitesimal jackknife for the bootstrap.}

In the preprint \citet{giordano:2019:hoij}, I extend \citet{giordano:2019:ij} to
higher-order Taylor series approximations, providing a family of estimators
which I collectively call the higher-order infinitesimal jackknife (HOIJ).  In
addition to providing higher-quality approximations to CV and extending the
results to k-fold CV, the higher-order approach promises to provide a scalable
alternative to the bootstrap, a procedure that estimates frequentist variability
by repeatedly re-evaluating a model at datasets drawn with replacement from the
observed data. The bootstrap is known to enjoy higher-order accuracy in certain
circumstances \citep{hall:2013:bootstrap}, and the HOIJ can approach the
bootstrap at a rate faster than the bootstrap approaches the truth.  The HOIJ
thus promises to make bootstrap inference available to models which are
differentiable but too expensive to re-evaluate (e.g. simulation-based models
\citep[Section 2.6]{baker:2019:workshop}), but also to allow efficient
bootstrap-after-bootstrap procedures that are currently out of reach for
all but the simplest statistics \citep{efron:1994:bootstrap}.


\paragraph{Scaling sensitivity measures to high dimensions.}

Sensitivity analysis typically avoids the expense of re-fitting a model, but
incurs the expense of solving a large linear system, the size of which is
determined by the dimensionality of the inferred parameter.  Thus, extending the
benefits of the sensitivity analysis to increasingly large scientific problems
requires developing methods to efficiently solve correspondingly large linear
systems. Stochastic second-order methods are currently an active research topic
in optimization \citep{agarwal:2017:secondorder, berahas:2020:newtonsketch}, and
methods developed therein should apply directly to sensitivity analysis.

\paragraph{Partitioned Bayesian inference.}

The ideas of \citep{giordano:2018:covariances} can be naturally extended to
approximately propagate uncertainty among separately estimated components of an
inference problem.  For example, astronomical catalogs are customarily produced
with MFVB-like algorithms \citep{lang:2016:tractor, regier:2019:cataloging},
which take inputs such as the sky background and optical point spread function
as fixed inputs, though these quantities are themselves inferred with
uncertainty.  Viewing all the separate inference procedures as a sequential
quasi-MFVB objective, one could directly apply the techniques of
\citep{giordano:2018:covariances} to propagate the uncertainty from the modeling
inputs to the astronomical catalog's uncertainty.



\newpage

\bibliography{references}
\bibliographystyle{plainnat}

\end{document}
