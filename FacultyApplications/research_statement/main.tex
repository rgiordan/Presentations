\input{_headers.tex}
\usepackage{enumitem}
\setlist{nolistsep}

\usepackage{geometry}
%\geometry{margin=1.2in}
\geometry{top=0.8in}
\geometry{left=1.1in}
\geometry{right=1.1in}

\title{Ryan Giordano Research Statement}

\author{
  Ryan Giordano \\ \texttt{rgiordan@mit.edu }
}

\begin{document}

\begin{minipage}[t]{0.5\textwidth}
\hspace{-2em} % Easier than doing it right!
{\bf \LARGE Research Statement}\\
\end{minipage}
\begin{minipage}[t]{0.5\textwidth}
%    \begin{flushright}
        \hspace{8em} % Easier than doing it right!
        {\LARGE Ryan Giordano}
%    \end{flushright}
\end{minipage}

Many researchers would be concerned if they learned that some core conclusion of
their statistical analysis---such as the sign or statistical significance of some
key effect---could be overturned by removing a small fraction, say 0.1\%, of
their data.  Such non-robustness would be particularly concerning if the data
were not actually drawn randomly from precisely the population of interest, or if
the model may have been misspecified---circumstances that often obtain in  the
social sciences, for example.  Nevertheless, analysts do not routinely check
whether ablation of such a small set could overturn their results, in part
because the number of possible subsets containing 0.1\% of the data points is
combinatorially large.

In recent work, I identify problematic subsets of the data using {\em
sensitivity analysis}---that is, by forming a linear approximation to the
dependence of statistical estimators on their datasets.  The key idea is that,
although there are a very large number of subsets containing 0.1\% of the data
points, none of them are very different from the original dataset, and so we
expect the linear approximation to work well.  I confirm this intuition with
finite-sample accuracy bounds in terms of intuitive and verifiable assumptions.
I provide an \texttt{R} package \citep{zaminfluence} to compute the
approximation quickly and automatically using automatic differentiation
\citep{baydin:2015:automatic, autograd}.  And, with my co-authors, I show that
the approximation is capable of detecting meaningful non-robustness in several
published econometrics analyses. For example, in a study of microcredit in
Mexico \citep{angelucci:2015:microcredit}, we find that, by removing just 15
households out of 16,561 studied (a change of less than 0.1\%), the estimated
effect of microcredit changes from negative and statistically insignificant to
positive and statistically significant.

In fact, my research shows that many standard, computationally demanding data
analysis tasks are also amenable to fast, automatic approximation using
sensitivity analysis. For example:
%
\begin{itemize}
    %
\item Cross validation (CV) requires repeatedly leaving out subsets of the
observed data and re-evaluating a statistical estimator. By forming a Taylor
series approximation on the dependence of the estimator on the left-out set, I
provide fast approximations to CV with finite-sample accuracy guarantees
\citep{giordano:2019:ij}.
%
\item Prior specification encodes key assumptions in Bayesian statistics.  But
Bayesian inference can be sensitive to prior specification, and evaluating the
sensitivity of Bayesian posterior expectations to prior specification by
re-fitting is typically computationally prohibitive due both to the large space
of possible priors (often infinite dimensional), as well as the high
computational cost of evaluating even a single posterior approximation.  By
forming a Taylor series approximation to the dependence of the posterior mean on
the prior, I can explore the consequences of alternative prior functional forms
at a small fraction of the cost of exact re-fitting
\citep{giordano:2020:rstansensitivity, giordano:2021:bnpsensitivity}.
%
\item When analyzing randomly sampled data using possibly misspecified Bayesian
posteriors, frequentist variability in excess of posterior variability is
symptomatic of \emph{data non-robustness}. For example, one might worry that a
new random sample of poll respondents in the presidential forecast model of
\citet{economist:2020:election} would lead to a different prediction.  This
frequentist variability can be evaluated by the bootstrap, but at the
considerable cost of re-running Markov Chain Monte Carlo (MCMC) hundreds of
times.  By approximating the dependence of the posterior on the data with
sensitivity analysis, I compute accurate estimates of the frequentist variance
using only a single MCMC chain---orders of magnitude faster than the bootstrap
\citep{giordano:2020:stanconbayesij}.
%
\item Mean field variational Bayes (MFVB) is a popular posterior approximation
method for Bayesian problems which are too large to be tractable by Markov Chain
Monte Carlo \citep{blei:2017:variational, regier:2019:cataloging}.  However,
MFVB approximations provide notoriously poor estimates of posterior uncertainty
\citep{turner:2011:two}.  In \citet{giordano:2018:covariances}, I show that
accurate posterior covariances can be recovered from MFVB approximations with
sensitivity analysis by exploiting a duality between Bayesian covariances and
sensitivity.
%
\end{itemize}

For the remainder of this statement, I will elaborate each of these themes,
emphasizing the ways in which I update classical results with intuitive,
relevant theory and easy-to-use computational tools.

\newpage



\subsection*{Robustness to data ablation}

In many applied settings, particularly in econometrics, an statistical analysis
might be considered non-robust if it could be overturned or even reversed by
removing only a small proportion of the dataset. Analyzing all possible data
subsets of a certain size is computationally prohibitive, so I provide a
finite-sample metric to approximately compute the number (or fraction) of
observations that has the greatest influence on a given result when dropped
\citep{giordano:2020:amip}\footnote{Following conventions in econometrics, the
authors are listed alphabetically.  Rachael Meager and I are equal contribution
primary authors.}.  At minimal computational cost, our method provides an exact
finite-sample lower bound on sensitivity for any estimator, so any
non-robustness one finds is conclusive. I demonstrate that non-robustness to
data ablation is driven by a low signal-to-noise ratio in the inference problem,
is not reflected in standard errors, does not disappear asymptotically, and is
not inherently a product of outliers or misspecification.

The approximation works for M-estimators based on smooth estimating equations, a
class which includes ordinary least squares, instrumental variables, generalized
method of moments, variational Bayes, and maximum likelihood estimators. Using
my \texttt{R} package \citep{zaminfluence}, the approximation is automatically
computable from the specification of the estimating equation alone.  By
analyzing several published econometric analyses \citep{angelucci:2009:indirect,
finkelstein:2012:oregon, meager:2019:microcredit}, I show that even two-parameter
linear regression analyses of randomized trials can be highly sensitive.  While
I find some applications are robust, in others the sign of a treatment effect
can be changed by dropping less than 1\% of the sample even when standard errors
are small.


\subsection*{Approximate cross validation}

The error or variability of machine learning algorithms is often assessed by
repeatedly re-fitting a model with different weighted versions of the observed
data; cross-validation (CV) can be thought of as a particularly popular example
of this technique.
%
In \citet{giordano:2019:ij}, I use a linear approximation to the dependence of
the fitting procedure on the weights, producing results that can be faster by an
order of magnitude than repeated re-fitting. I provide explicit finite-sample
error bounds for the approximation in terms of a small number of simple,
verifiable assumptions.  My results apply whether the weights and data are
stochastic or deterministic, and so can be used as a tool for proving the
accuracy of the approximation on a wide variety of problems. As a
corollary, I state mild regularity conditions under which the approximation
consistently estimates true leave-$k$-out cross-validation for any fixed $k$. I
demonstrate the accuracy of the approximation on a range of simulated and real
datasets, including an unsupervised clustering problem from genomics
\citep{Luan:2003:clustering, shoemaker:2015:ultrasensitive}.


\subsection*{Approximately bootstrapping Bayesian posterior means}

The frequentist (i.e., sampling) variance of Bayesian posterior expectations
differs in general from the posterior variance even for large datasets,
particularly when the model is misspecified or contains many latent variables
\citep{kleijn:2006:misspecification}. Unlike the posterior variance, the
frequentist variance is meaningful even in the presence of misspecification,
particularly when the data is known to arise from random sampling
\citep{waddell:2002:bayesphyloboot}.  However, the principal existing approach
for computing the frequentist variability of MCMC procedures is the bootstrap,
which can be extremely computationally intensive due to the need to run hundreds
of extra MCMC procedures \citep{huggins:2019:bayesbag}.

In \citet{giordano:2020:stanconbayesij, giordano:2021:bayesij}, I propose an
efficient alternative to bootstrapping an MCMC procedure.  My approach is based
on the Bayesian analogue of the influence function from the classical
frequentist robustness literature.  Using results from
\citet{giordano:2018:covariances, giordano:2019:ij}, I show that the influence
function for posterior expectations can be easily computed from the posterior
samples of a single MCMC procedure and consistently estimates the bootstrap
variance. I demonstrate the accuracy and computational benefits of the influence
function variance estimates on an array of experiments including an election
forecasting model \citep{economist:2020:election}, the Cormack-Jolly-Seber model
from ecology \citep{kery:2011:bayesian}, and a large collection of models and
datasets from the social sciences \citep{gelman:2006:arm}.

\clearpage
\subsection*{Bayesian sensitivity analysis}

\paragraph{Prior sensitivity for Markov Chain Monte Carlo.}
%
MCMC is arguably the most commonly used computational tool to estimate Bayesian
posteriors, which is made still easier by modern black-box MCMC tools such as
\texttt{Stan} \citep{carpenter:2017:stan, rstan}.  However, a single run of MCMC
typically remains time-consuming, and systematically exploring alternative prior
parameterizations by re-running MCMC would be computationally prohibitive for
all but the simplest models.

My software package, \texttt{rstansensitivity},
\citep{giordano:2020:rstansensitivity, giordano:2018:mcmchyper}, takes advantage
of the automatic differentiation capacities of \texttt{Stan}
\citep{carpenter:2015:stanmath} together with a classical result from  Bayesian
robustness \citep{gustafson:1996:localposterior, basu:1996:local,
giordano:2018:covariances} to provide automatic hyperparameter sensitivity for
generic \texttt{Stan} models from only a single MCMC run.  I demonstrate the
speed and utility of the package in detecting excess prior sensitivity in a
social sciences model taken from \citet[Chapter 13.5]{gelman:2006:arm}.


\paragraph{Prior sensitivity for discrete Bayesian nonparametrics.}
%
% From BNP_sensitivity/writing/NIPS_2018_BNP_workshop
A central question in many probabilistic clustering problems is how many
distinct clusters are present in a particular dataset and which observations
cluster together. Discrete Bayesian nonparametric (BNP) mixture models address
this question by placing a generative process on cluster assignment, making the
number of distinct clusters present amenable to Bayesian inference.  However,
like all Bayesian approaches, BNP requires the specification of a prior, and
this prior may favor a greater or lesser number of distinct clusters.

In \citet{giordano:2021:bnpsensitivity}, I derive and analyze prior sensitivity
measures for variational Bayes (VB) approximations in general, with a practical
focus on discrete BNP models. Unlike much previous work on local Bayesian
sensitivity for BNP (e.g. \citet{Basu:2000:BNP_robustness}), I pay special
attention to the ability of the sensitivity measures to \emph{extrapolate} to
different priors, rather than treating the sensitivity as a measure of
robustness \textit{per se}. Under mild regularity conditions, I prove that VB
approximations are Fr{\'e}chet differentiable functions of the prior density,
though only in one extreme of the standard family of embeddings considered for
exact Bayesian posteriors \citep{gustafson:1996:localposterior}.
% Further, though there are many valid choices for measuring the ``size'' of a
% prior perturbation for exact Bayesian posteriors
% \citep{gustafson:1996:localposterior}, I prove that, for variational Bayes
% approximations, local approximations are valid only when based on the largest
% point-wise change in the log density.
My co-authors and I apply the sensitivity measures to a number of real-world
problems, including an unsupervised clustering problem from genomics using
fastSTRUCTURE \citep{raj:2014:faststructure}, demonstrating that the
approximation is accurate, orders of magnitude faster than re-fitting, and
capable of detecting meaningful prior sensitivity.

\subsection*{Uncertainty propagation in mean-field variational Bayes}

Mean-field Variational Bayes (MFVB) is an approximate Bayesian posterior
inference technique that is increasingly popular due to its fast runtimes on
large-scale scientific data sets (e.g., \citet{raj:2014:faststructure,
kucukelbir:2017:advi, regier:2019:cataloging}). However, even when MFVB provides
accurate posterior means for certain parameters, it often mis-estimates
variances and covariances \citep{wang:2005:inadequacy, turner:2011:two} due to
its inability to propagate Bayesian uncertainty between statistical parameters.

In \citet{giordano:2015:linear, giordano:2018:covariances}, I derive a simple
formula for the effect of infinitesimal perturbations on MFVB posterior means,
thus providing improved covariance estimates and greatly expanding the practical
usefulness of MFVB posterior approximations. My method for computing posterior
covariances from an MFVB approximation exploits a result from the classical
Bayesian robustness literature relating derivatives of posterior expectations to
posterior covariances, and can be seen as generalizing the Laplace approximation
for maximum \emph{a-posteriori} estimates to more general MFVB procedures.
% The key condition is that the MFVB
% approximation provides good estimates of a select subset of posterior means---an
% assumption that has been shown to hold in many practical settings.
In experiments on simulated and real-life datasets,  including models from
ecology \citep{kery:2011:bayesian}, the social sciences \citep{gelman:2006:arm},
and on a massive internet advertising dataset \citep{criteo:2014:dataset}, I
demonstrate that my method is simple, general, and fast, providing accurate
posterior uncertainty estimates and robustness measures with runtimes that can
be an order of magnitude faster than MCMC.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\newpage

\clearpage
\subsection*{Selected Future work}

My research is ideally driven by the needs of my scientific and industry
collaborators, and so I expect my future work will be determined to a large part
by my colleagues, and so inherently difficult to predict in advance.
Nevertheless, there are a few thematic directions that I find promising,
and which I look forward to pursuing as faculty.

\paragraph{The empirical influence function (EIF).}

Much of my work (particularly \citet{giordano:2019:ij, giordano:2020:amip,
giordano:2021:bayesij}) has strong connections to the classical theory of von
Mises expansions and the closely related concept of the influence function,
which measures the effect of individual datapoints on an estimator
\citep{mises:1947:asymptotic, reeds:1976:thesis, hampel:1986:robustbook,
serfling:2009:approximation}.  But my focus on the influence function
evaluated at the observed data---i.e., the ``empirical influence function''
(EIF)---stands in contrast with much of the classical literature, which studies
the asymptotic behavior of estimators via their (unobserved) limiting influence
function.
%
In our present age of automatic differentiation, large datasets, and complex
models, I believe that the EIF will continue to provide practical benefits and
is relatively under-studied.
% Since the EIF is observed, one can in principal
% use it to compute exact finite-sample error bounds for its
% approximations.~\footnote{Though I provide finite-sample error bounds in
% \citet{giordano:2019:ij, giordano:2019:hoij}, the bounds need to be tightened
% and simplified to be practically useful.  There is good reason to believe they
% can be, especially in simple cases such as least-squares estimators.} Studying
% the EIF alone avoids many of the technical difficulties requried for classical
% von Mises calculus, since the latter must study distribution functions in spaces
% that embed both continuous and discrete measures \citep{gill:1989:mises}.

In \citet{giordano:2019:hoij}, I show that higher-order EIFs can be
easily and automatically evaluated and analyzed for M-estimators at a
computational cost comparable to the first-order EIF---that of forming and
factorizing a Hessian matrix of second-order derivatives.
%
Thus, the EIF ``amortizes'' the cost of evaluating an M-estimator large number
of alternative datasets: by paying a large fixed price up front (approximately
computing and factorizing a Hessian matrix), one can cheaply approximate
M-estimators at a very large number of alternative datasets.  Natural
applications of the idea include approximating the bootstrap-after-bootstrap
\citep{hall:2013:bootstrap}, evaluating the sampling properties of
cross-validation \citep{bayle:2020:cv}, and computing higher-order jackknife
bias correction \citep{shao:2012:jackknife}.
% , burnham:1978:jackknifebias

\paragraph{Sensitivity analysis in difficult situations.}

It is not always as easy to apply sensitivity analysis in practice as it is in
theory.  I have found that a few key problems tend to recur, and I will discuss
them in turn, as well as potential solutions which draw connections to the
optimization literature.

First, sensitivity analysis should, ideally, deal gracefully with incomplete
optimization.  For example, a collaborator from biostatistics and I have found
that the \texttt{R} package \texttt{DESeq2} package \citep{deseq2} can fail in
practice to fully optimize the log likelihood, and so fail to satisfy the
assumptions that make sensitivity analysis possible.  Instead of forcing users
to optimize further, I propose that second-order EIFs could simulate the effect
of simultaneously taking a Newton step and perturbing the data, permitting
sensitivity analysis on incompletely optimized objectives with little
computation beyond that required for well-optimized objectives.
%
%
% is currently working with me to
% apply my work in \citet{giordano:2020:amip} to single-cell sequencing data in
% hopes of flagging small groups of highly influential cells.  Careful analysis of
% the output of the \texttt{R} package \texttt{DESeq2} \citep{deseq2} shows that
% the package's iteratively reweighted least squares algorithm may not be finding
% a high-quality optimum of the objective function.  The ``sensitivity'' of
% incomplete optimization is not well-defined, but we ideally do not want to
% require end-users to to re-implement their optimization algorithm to use
% sensitivity analysis.  One potential solution using higher-order expansions
% could be to represent incomplete optimization as a constrained optimization
% problem in which the gradient is set not to zero, but to the value numerically
% observed in the incomplete optimization.  Representing the constraint with a
% Lagrange multiplier, one can then use a second-order expansion to estimate the
% effect of simultaneously removing the constraint (setting the multiplier to
% zero) and perturbing the objective (e.g., removing some data).

Second, the key computational bottleneck in sensitivity analysis in
high-dimensional problems is typically the solution of linear systems involving
the inverse Hessian of the objective function. Off-the-shelf iterative
algorithms like the conjugate-gradient algorithm \citep{nocedal:2006:numerical}
suffice in many cases, but there is reason to believe that the present active
research into stochastic second-order methods (e.g.
\citet{agarwal:2017:secondorder, berahas:2020:newtonsketch}) could significantly
speed up sensitivity analysis in large problems.

Finally, practitioners are often interested in non-smooth objectives. For these,
one promising idea is to use local approximations to speed up computationally
intensive but smooth components in non-smooth problems. For example,
\citet{wilson:2020:approximatecv} speeds up cross-validation of linear
regression with a non-smooth lasso penalty by forming a fast approximation to
the effect on the optimal squared error of leaving out a single datapoint, and
retaining non-smoothness in the lasso penalty. We take a similar approach in
\citet{giordano:2021:bnpsensitivity}, retaining easy-to-compute non-linearities
in posterior summary statistics.


\paragraph{``Crossing'' my existing work.}

Finally, many of my existing papers can be easily ``crossed'' with one another, producing
relatively straightforward projects suitable for collaborations with more junior
researchers.  For example, I am presently working with a PhD student to combine
the Bayesian influence function of \citet{giordano:2021:bayesij} with the
adversarial data ablation metrics of \citet{giordano:2020:amip}, to
automatically find influential data subsets from MCMC output.


\newpage

\bibliography{references}
\bibliographystyle{plainnat}

\end{document}
