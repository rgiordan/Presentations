\input{_headers.tex}
\usepackage{enumitem}
\setlist{nolistsep}

\usepackage{geometry}
%\geometry{margin=1.2in}
\geometry{top=1.0in}
\geometry{left=1.1in}
\geometry{right=1.1in}

\title{Ryan Giordano Research Statement}

\author{
  Ryan Giordano \\ \texttt{rgiordan@mit.edu }
}

\begin{document}

\begin{minipage}[t]{0.5\textwidth}
\hspace{-2em} % Easier than doing it right!
{\bf \LARGE Research Statement}\\
\end{minipage}
\begin{minipage}[t]{0.5\textwidth}
%    \begin{flushright}
        \hspace{8em} % Easier than doing it right!
        {\LARGE Ryan Giordano}
%    \end{flushright}
\end{minipage}

Many researchers would be concerned if they learned that some core conclusion of
their statistical analysis---such as the sign or statistical significance of
some key effect---could be overturned by removing a small fraction of their
data. Such non-robustness would be particularly concerning if the data were not
actually drawn randomly from precisely the population of interest, or if the
model may have been misspecified---circumstances that often occur in the social
sciences. For example, a study of microcredit in Mexico on 16,561 households
measured a negative but statistically insignificant effect of microcredit
\citep{angelucci:2015:microcredit}.  However, in recent work, my co-authors and
I show that one can make the estimated effect of microcredit positive and
statistically significant by removing just 15 households from the analysis,
reversing the paper's qualitative conclusions by removing only 0.1\% of the
data.  Since there are a combinatorially large number of ways to leave 15
datapoints out of 16,561 (over $10^{51}$), finding such influential subsets by
brute force would be impossible.  I circumvent this difficulty by forming a
\emph{linear approximation} to the dependence of the estimator on the dataset.
My technique works for a wide class of commonly used estimators, providing a
fast, automatic tool for identifying small but influential subsets with
finite-sample guarantees in many applied problems.

In fact, my research shows that many desirable but computationally demanding
data analysis tasks are similarly amenable to automatic approximation, providing
fast, easy-to-use computational tools for assessing whether small changes to
modeling assumptions or datasets can meaningfully alter an analyst's substantive
conclusions.  As I detail below, my work encompasses Bayesian approaches (such
as measuring robustness to prior specification), frequentist approaches (such as
fast approximations to leave-one-out cross validation), and combinations of the
two (such as measuring the robustness of Markov chain Monte Carlo procedures to
data resampling). For the remainder of this statement, I will describe some of
my specific past and future projects, emphasizing the ways in which I update
classical results for a modern computational environment using intuitive,
relevant theory, and with motivation drawn from practical applications.

\section{Selected prior and ongoing work}

\subsection{Data sensitivity}

\paragraph{Robustness to data ablation.}
%
In many applied settings, particularly in econometrics, a statistical analysis
might be considered non-robust if it could be overturned or even reversed by
removing only a small proportion of the dataset. Analyzing all possible data
subsets of a certain size is typically computationally prohibitive, so I provide
a method to approximately compute the number (or fraction) of observations that
has the greatest influence on a given result when dropped
\citep{giordano:2020:amip}.\footnote{Following conventions in econometrics, the
authors are listed alphabetically.  Rachael Meager and I are equal contribution
primary authors.}
%
At the minimal computational cost of a single re-fit, my method provides an
exact finite-sample lower bound on sensitivity to data ablation since, at worst,
we have identified a sub-optimal subset to drop.
%
I demonstrate that non-robustness to data ablation is driven by a
low signal-to-noise ratio in the inference problem, is not reflected in standard
errors, does not disappear asymptotically, and is not inherently a product of
outliers or misspecification.

The approximation works for all M-estimators based on smooth estimating
equations, a class which includes most standard estimators, including ordinary
least squares, instrumental variables, generalized method of moments,
variational Bayes, and maximum likelihood estimators. Using my \texttt{R}
package \citep{zaminfluence}, the approximation is automatically computable from
the specification of the estimating equation alone.  By analyzing several
published econometric analyses,
% \citep{angelucci:2009:indirect, finkelstein:2012:oregon,
% meager:2019:microcredit},
I show that even two-parameter linear regression analyses of randomized trials
can be highly sensitive.  While I find some applications are robust, I show that
the sign of a treatment effect can be changed by dropping less than 1\% of the
sample in  several high-profile econometrics studies, even when standard errors
are small.


\paragraph{Approximate cross validation.}
%
The error or variability of machine learning algorithms is often assessed by
repeatedly re-fitting a model with different weighted versions of the observed
data; cross-validation (CV) can be thought of as a particularly popular example
of this technique.
%
In \citet{giordano:2019:ij}, I use a linear approximation to the dependence of
the fitting procedure on the weights, producing results that can be faster by an
order of magnitude than repeated re-fitting. I provide explicit finite-sample
error bounds for the approximation in terms of a small number of simple,
verifiable assumptions.  My results apply whether the weights and data are
stochastic or deterministic, and so can be used as a tool for proving the
accuracy of the approximation on a wide variety of problems. As a
corollary, I state mild regularity conditions under which the approximation
consistently estimates true leave-$k$-out cross-validation for any fixed $k$. I
demonstrate the accuracy of the approximation on a range of simulated and real
datasets, including an unsupervised clustering problem from genomics.
%\citep{Luan:2003:clustering, shoemaker:2015:ultrasensitive}.


\paragraph{Approximately bootstrapping Bayesian posterior means.}
%
When analyzing randomly sampled data using Bayesian posteriors, it can make
sense to consider the \emph{sampling variability} of the posterior mean.  For
example, one might ask whether a new random sample of poll respondents in the
presidential forecast model of \citet{economist:2020:election} would lead to a
different prediction for the election outcome.  When Bayesian models are
misspecified, or when they contain latent parameters on which one wishes to
condition when sampling, the sampling and posterior variances can differ.  In
such cases, sampling variability in excess of posterior variability is
symptomatic of \emph{data non-robustness}---new data sets which are \textit{a
priori} plausible would lead to different substantive conclusions.

The sampling variability of a posterior mean can be evaluated by the bootstrap,
but at the considerable cost of re-running Markov chain Monte Carlo (MCMC)
hundreds of times. In \citet{giordano:2020:stanconbayesij,
giordano:2021:bayesij}, I propose an efficient alternative to bootstrapping an
MCMC procedure.  My approach is based on the Bayesian analogue of the influence
function from the classical frequentist robustness literature.  Using results
from \citet{giordano:2018:covariances, giordano:2019:ij}, I show that the
influence function for posterior expectations can be easily computed from the
posterior samples of a single MCMC procedure and state conditions under which it
consistently estimates the bootstrap variance. I demonstrate the accuracy and
computational benefits of my approach on an array of experiments including an
election forecasting model, the Cormack--Jolly--Seber model from ecology, and a
large, curated collection of models and datasets from the social sciences.
% \citep{economist:2020:election}
% \citep{kery:2011:bayesian}
% \citep{gelman:2006:arm}

%\clearpage
% \subsection*{Bayesian sensitivity analysis}

% \citep{carpenter:2017:stan, rstan}
% \citep{carpenter:2015:stanmath}

\subsection{Prior sensitivity}

\paragraph{Prior sensitivity for Markov chain Monte Carlo.}
%
Prior specification encodes key assumptions in Bayesian statistics.  Often, a
range of prior choices seems reasonable, and Bayesian inference can be sensitive
which particular prior is used.  Unfortunately, evaluating the sensitivity of
Bayesian posterior expectations to prior specification by re-estimating the
posterior for a large number of alternative priors is typically computationally
prohibitive. In particular, Markov chain Monte Carlo (MCMC) is arguably the most
commonly used computational tool to estimate Bayesian posteriors, which is made
still easier by modern black-box MCMC tools such as \texttt{Stan}. However, a
single run of MCMC typically remains time-consuming, and systematically
exploring alternative prior parameterizations by re-running MCMC would be
computationally prohibitive for all but the simplest models.

My software package \texttt{rstansensitivity}
\citep{giordano:2020:rstansensitivity, giordano:2018:mcmchyper}, takes advantage
of the automatic differentiation capacities of \texttt{Stan} together with a
classical result from  Bayesian robustness \citep{gustafson:1996:localposterior,
giordano:2018:covariances} to provide automatic hyperparameter sensitivity for
generic \texttt{Stan} models from only a single MCMC run.  I demonstrate the
speed and utility of the package in detecting excess prior sensitivity in
several examples taken from the \texttt{Stan} example datasets,
including real-life datasets from the social sciences.
%social sciences model taken from \citet[Chapter 13.5]{gelman:2006:arm}.



\paragraph{Prior sensitivity for discrete Bayesian nonparametrics.}
%
% From BNP_sensitivity/writing/NIPS_2018_BNP_workshop
A central question in many probabilistic clustering problems is how many
distinct clusters are present in a particular dataset and which observations
cluster together. Discrete Bayesian nonparametric (BNP) mixture models address
this question by placing a generative process on cluster assignment, making the
number and composition of distinct clusters amenable to Bayesian inference.
However, like all Bayesian approaches, BNP requires the specification of a
prior, and this prior may favor different numbers and types of posterior
clusters.

In \citet{giordano:2021:bnpsensitivity}, I derive and analyze prior sensitivity
measures for variational Bayes (VB) approximations in general, with a practical
focus on discrete BNP models. Unlike much previous work on local Bayesian
sensitivity for BNP (e.g. \citet{Basu:2000:BNP_robustness}), I pay special
attention to the ability of the sensitivity measures to \emph{extrapolate} to
different priors, rather than treating the sensitivity as a measure of
robustness \textit{per se}.  I state conditions under which VB approximations
are Fr{\'e}chet differentiable functions of prior densities in a particular
vector space, while also proving that VB approximations are in fact
\emph{non-differentiable} in another wide class of vector space embeddings
popular in the classical Bayesian robustness literature.
My co-authors and I apply the sensitivity measures to a number of real-world
problems, including an unsupervised clustering problem from genomics using
fastSTRUCTURE.  We demonstrate that the approximation is accurate, orders of
magnitude faster than re-fitting, and capable of detecting meaningful prior
sensitivity in quantities of practical interest.

% \citep{raj:2014:faststructure}

\subsection{Improved variational inference}

\paragraph{Uncertainty propagation in mean-field variational Bayes.}
%
Mean-field Variational Bayes (MFVB) is an approximate Bayesian posterior
inference technique that is increasingly popular due to its fast runtimes on
large-scale scientific data sets. For example, in
\citet{regier:2019:cataloging}, my co-authors and I use MFVB to construct an
approximate posterior for the identity of every astronomical object in 55TB
of image data from the Sloan Digital Sky Survey.  However, even when MFVB provides
accurate posterior means for certain parameters, it often mis-estimates
variances and covariances due to its inability to propagate Bayesian uncertainty
between statistical parameters.

In \citet{giordano:2015:linear, giordano:2018:covariances}, I derive a simple
formula for the effect of infinitesimal perturbations on MFVB posterior means,
thus providing improved covariance estimates and greatly expanding the practical
usefulness of MFVB posterior approximations. My approach builds on a result from
the classical Bayesian robustness literature relating posterior covariances to
the derivatives of posterior expectations, and includes the classical Laplace
approximation as a special case. In experiments on simulated and real-life
datasets,  including models from ecology, the social sciences, and on a massive
internet advertising dataset, I demonstrate that my method is simple, general,
and fast, providing accurate posterior uncertainty estimates and robustness
measures with runtimes that can be an order of magnitude faster than MCMC.


\paragraph{Simplified and improved black-box variational inference.}
%
Black box variational inference (BBVI) is an easy-to-use version of variational
inference requiring little more from the user than the software implementation
of a differentiable log joint probability distribution. Unfortunately, standard
BBVI implementations (such as the \texttt{vb} method implemented in the R
software \texttt{Stan}) depend on stochastic gradient optimization, which can be
more difficult to implement and assess than standard deterministic optimization
methods.  Further, since the exact BBVI objective function cannot be directly
evaluated, one cannot compute the linear response covariances and prior
sensitivity measures described above.

In \citet{giordano:2018:covariances, giordano:2021:bbvi} I overcome these
difficulties with a simple idea: rather than use stochastic gradient to optimize
the exact variational objective, one can fix a finite number of draws in advance
and optimize an approximate but deterministic objective.  In
\citet{giordano:2021:bbvi}, my co-authors and I show on a a wide range of
real-life problems that the BBVI with a deterministic objective and linear
response covariances produces much more accurate posterior approximations than
stochastic BBVI.  Further, using off-the-shelf optimization methods, we produce
approximations no more slowly than, and often much more quickly than, custom
implementations of stochastic BBVI.  Finally, we show that the error induced by
using an approximate objective is not only small but quantifiable using standard
frequentist measures of uncertainty.


% \citep{kery:2011:bayesian}
% \citep{gelman:2006:arm}
% \citep{criteo:2014:dataset}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\newpage

%\clearpage
\section{Selected future work}

Though there are many potential directions for my future research, I articulate
below two broad areas that I find particularly exciting.

\paragraph{The empirical influence function.}
%
Much of my work (particularly \citet{giordano:2019:ij, giordano:2020:amip,
giordano:2021:bayesij}) has strong connections to the classical theory of von
Mises expansions and the closely related concept of the influence function,
which measures the effect of individual datapoints on an estimator
\citep{mises:1947:asymptotic, reeds:1976:thesis, hampel:1986:robustbook,
serfling:2009:approximation}.  But my focus on the influence function
evaluated at the observed data---i.e., the ``empirical influence function''
(EIF)---stands in contrast with much of the classical literature, which studies
the asymptotic behavior of estimators via their (unobserved) limiting influence
function.
%
In our present age of automatic differentiation, large datasets, and complex
models, I believe that the EIF will continue to provide practical benefits and
is relatively under-studied.

In \citet{giordano:2019:hoij}, I show that higher-order EIFs can be
easily and automatically evaluated and analyzed for M-estimators at a
computational cost comparable to the first-order EIF---that of forming and
factorizing a Hessian matrix of second-order derivatives.
%
Thus, the EIF ``amortizes'' the cost of evaluating an M-estimator large number
of alternative datasets: by paying a large fixed price up front (approximately
computing and factorizing a Hessian matrix), one can cheaply approximate
M-estimators at a very large number of alternative datasets.  Natural
applications of the idea include approximating the bootstrap-after-bootstrap,
evaluating the sampling properties of cross-validation, and computing
higher-order jackknife bias correction.
% , burnham:1978:jackknifebias
% \citep{bayle:2020:cv}
% \citep{shao:2012:jackknife}
% \citep{hall:2013:bootstrap}

\paragraph{Sensitivity analysis in difficult situations.}
%
It is not always as easy to apply sensitivity analysis in practice as it is in
theory.  I have found that a few key problems tend to recur, and I will discuss
them in turn, as well as potential solutions which draw connections to the
optimization literature.

First, sensitivity analysis should, ideally, deal gracefully with incomplete
optimization.  For example, a collaborator from biostatistics and I have found
that the popular \texttt{R} package \texttt{DESeq2} can fail in practice to
fully optimize the log likelihood, and so fail to satisfy the assumptions that
make sensitivity analysis possible.  Instead of forcing users to optimize
further, I propose that second-order empirical influence functions could
simulate the effect of simultaneously taking a Newton step and perturbing the
data, permitting sensitivity analysis on incompletely optimized objectives with
little computation beyond that required for well-optimized objectives.

Second, the key computational bottleneck in sensitivity analysis in
high-dimensional problems is typically the solution of linear systems involving
the inverse Hessian of the objective function. Off-the-shelf iterative
algorithms like the conjugate gradient algorithm %\citep{nocedal:2006:numerical}
suffice in many cases, but there is reason to believe that the present active
research into stochastic second-order methods
% (e.g.
% \citet{agarwal:2017:secondorder, berahas:2020:newtonsketch})
could significantly speed up sensitivity analysis in large problems.

Finally, practitioners are often interested in non-smooth objectives. For these,
one promising idea is to use local approximations to speed up computationally
intensive but smooth components in non-smooth problems. For example,
\citet{wilson:2020:approximatecv} speeds up cross-validation of linear
regression with a non-smooth lasso penalty by forming a fast approximation to
the effect on the optimal squared error of leaving out a single datapoint, and
retaining non-smoothness in the lasso penalty. We take a similar approach in
\citet{giordano:2021:bnpsensitivity}, retaining easy-to-compute non-linearities
in posterior summary statistics.


\paragraph{Projects for junior collaborators.}
%
Finally, a professor must also be able to provide research topics suitable for
more junior researchers, such as those who are early in their PhD.  To this end,
many of my existing papers can be easily ``crossed'' with one another, producing
impactful but relatively straightforward projects for more junior collaborators.
For example, I am presently working with a PhD student to combine the Bayesian
influence function of \citet{giordano:2021:bayesij} with the adversarial data
ablation metrics of \citet{giordano:2020:amip}, to automatically find
influential data subsets from Markov chain Monte Carlo output.


\newpage

\bibliography{references}
\bibliographystyle{plainnat}

\end{document}
