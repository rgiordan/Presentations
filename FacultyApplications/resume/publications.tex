\section{\sc Preprints / \\in preparation}

% \me \& \tamara (2021).
% The Bayesian Infinitesimal Jackknife for Variance.
% (In preparation)

\mestar, M.~Ingram$^\star$ \& \tamara  (2021).
Faster and More Accurate Black Box Variational Inference Using a Deterministic Objective.\\
$\star$ = equal contribution first authors.
In preparation.

\me \& \tamara  (2021).
The Bayesian Infinitesimal Jackknife for Variance.\\
In preparation.

\me, \mike, \& \tamara (2019).
A Higher-Order Swiss Army Infinitesimal Jackknife.
\emph{arXiv:1907.12116 [stat.ME]}.
\paperref{https://arxiv.org/abs/1907.12116}

\section{\sc Under review}

\tamara, \mestar, R.~Meager$^\star$ \&  (2021).
An Automatic Finite-Sample Robustness Metric: When Can Dropping a Little Data
Make a Big Difference?\\
$\star$ = equal contribution first authors (author order alphabetical).
\emph{arXiv:2011.14999 [stat.ME]}.
\paperref{https://arxiv.org/abs/2011.14999}
Submitted to Econometrica.


\mestar, \runjing{}$^\star$, \mike, \& \tamara (2021).
Evaluating Sensitivity to the Stick-Breaking Prior in Bayesian Nonparametrics.\\
$\star$ = equal contribution first authors.
\emph{arXiv:2107.03584 [stat.ME]}.
\paperref{https://arxiv.org/abs/2107.03584}.\\
Submitted to Bayesian Analysis.


\section{\sc Publications}

\me, W.~Stephenson, \runjing, \mike, \& \tamara (2019).  A Swiss Army Infinitesimal
Jackknife.  \emph{The 22nd International Conference on Artificial Intelligence
and Statistics.}
\textbf{Notable paper award.}
\paperref{https://arxiv.org/abs/1806.00550}

\me, \tamara, \& \mike (2018).  Covariances, Robustness, and Variational Bayes.
In \emph{Journal of Machine Learning Research.}
\paperref{https://arxiv.org/abs/1709.02536}

J.~Regier, K.~Fischer, K~.Pamnany, A.~Noack, J.~Revels, M.~Lam, S.~Howard,
\me, D.~Schlegel, J.~McAuliffe, \& R.~Thomas (2019). Cataloging the Visible
Universe Through Bayesian Inference in Julia at Petascale. In \emph{Journal of
Parallel and Distributed Computing.}
\paperref{https://doi.org/10.1016/j.jpdc.2018.12.008}

J.~Regier, K.~Pamnany, K.~Fischer, A.~Noack, M.~Lam, J.~Revels, S.~Howard, \me,
D.~Schlegel, J.~McAuliffe, R.~Thomas, \& Prabhat (2018).  Cataloging the Visible
Universe Through Bayesian Inference at Petascale.  In \emph{IEEE International
Parallel and Distributed Processing Symposium (IPDPS). IEEE, 2018.}
\paperref{https://arxiv.org/abs/1801.10277}

\me, \tamara, \& \mike (2015). Linear Response Methods for Accurate Covariance
Estimates from Mean Field Variational Bayes. In \emph{Advances in Neural
Information Processing Systems.}
\textbf{Spotlight presentation.}
\paperref{https://arxiv.org/abs/1506.04088}

R.~Winther, \me, M.~D.~Edge, \& R.~Nielsen (2015).  The Mind, the Lab, and the
Field: Three Kinds of Populations in Scientific Practice.  In \emph{Studies in
History and Philosophy of Science Part C: Studies in History and Philosophy of
Biological and Biomedical Sciences.}
\paperref{https://doi.org/10.1016/j.shpsc.2015.01.009}


\section{\sc Workshop \\ Papers}

\mestar, \runjing{}$^\star$, \mike, \& \tamara (2018).
Evaluating Sensitivity to the Stick Breaking Prior in Bayesian Nonparametrics.
In \emph{NeurIPS 2018 Bayesian Nonparametrics Workshop.}\\
$\star$ = equal contribution first authors.
\paperref{https://arxiv.org/abs/1810.06587}

\mestar, \runjing{}$^\star$, N.~Varoquaux$^\star$, \mike, \& \tamara (2017).
Measuring Cluster Stability for Bayesian Nonparametrics Using the Linear Bootstrap.
In \emph{NeurIPS 2017 Advances in Approximate Bayesian Inference Workshop.}\\
$\star$ = equal contribution first authors.
\paperref{https://arxiv.org/abs/1712.01435}

\me, \tamara, R.~Meager, J.~Huggins, \& \mike (2016). Fast Robustness
Quantification with Variational Bayes. In \emph{2016 ICML Workshop on
\#Data4Good: Machine Learning in Social Good Applications.}
\paperref{https://arxiv.org/abs/1606.07153}
