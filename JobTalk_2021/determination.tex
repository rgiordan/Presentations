




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{What makes an analysis sensitive?  Preliminaries}
%
We are \textbf{robust to data dropping} if, for the $\Delta$
that changes conclusions and $\w^*$ dropping the $\alphan$
most influential points,
%
\begin{align*}
% $
%
\Delta \ge \thetafunlin(\w^*) - \thetafun(\thetahat(\onevec))
    % = {\color{red}- \sum_{n=1}^{\lfloor \alpha N \rfloor} \infl_{(n)} }
    =:   {\color{black} \noise } { \color{black} \shape}
\quad\Leftrightarrow\quad
%
{\color{red}\frac{\Delta}{\noise} \ge \shape}.
%
% $
\end{align*}
%
% \hspace{1em} where \vspace{1em}

\begin{itemize}
\item The ``signal'' $\Delta$ is the smallest change that reverses your conclusion
\item The ``noise'' $\noise^2 \rightarrow \mathrm{Var}(\sqrt{N}\phi)$
    (``sandwich'' variance estimator)
\item The ``shape''
    $\shape$
    $\rightarrow$ a nonzero constant and is
    $\le \sqrt{\alpha (1 - \alpha)}$
\end{itemize}

\hrulefill

\textbf{Contrast with sampling variability.}

A 95\% CI is given by
$\thetafun(\thetahat(\onevec)) \pm \frac{1.96}{\sqrt{N}} \noise.$
%
We reject $\thetafun(\thetahat(\onevec)) + \Delta$ when
%
\begin{align*}
%
\thetafun(\thetahat(\onevec)) + \Delta \ge
\thetafun(\thetahat(\onevec)) + \frac{1.96}{\sqrt{N}} \noise
\quad\quad
\Leftrightarrow
\quad\quad
{\color{red}
\frac{\Delta}{\noise} \ge \frac{1.96}{\sqrt{N}.}
}
%
\end{align*}
%


\hrulefill

%\vspace{1em}
The \textbf{signal to noise ratio} $\frac{\Delta}{\noise}$
determines robustness to both data dropping
and sampling variability, but with \textbf{different thresholds}.

% \begin{center}
% \begin{minipage}{0.8\textwidth}
% \begin{tikzpicture}
%     \node[anchor=south west,inner sep=0] (image) at (0,0) {
%     \SimInflHistogram{}
%     };
%     \begin{scope}[x={(image.south east)},y={(image.north west)}]
%         \draw [stealth-stealth][thick][white](0.42, 0.35) -- (0.6, 0.35);
%         \draw (0.51, 0.35) node[below][text width=3cm][align=center][white]
%             {\normalsize $\noise$};
%
%         \draw [stealth-][thick][red](0.3, 0.25) -- (0.3, 0.5);
%         \draw (0.25, 0.5) node[above][text width=3cm][align=center][red]
%             {\normalsize $-\sum_{n=1}^{\alphan} \infl_{(n)}$};
%     \end{scope}
% \end{tikzpicture}
% \end{minipage}
% \end{center}

\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[t]{What makes an analysis sensitive?}
%
\begin{minipage}{0.45\textwidth}
\begin{center}
    Robust to data dropping:\\
    (``dropping robustness'')\\
    \vspace{1em}
    $\textrm{SNR} := \frac{\Delta}{\noise} \ge \shape$
\end{center}
\end{minipage}
%
\begin{minipage}{0.45\textwidth}
\begin{center}
    Robust to sampling variation:\\
    (``sampling robustness'')\\
    \vspace{1em}
    $\textrm{SNR} := \frac{\Delta}{\noise} \ge
        \frac{1.96}{\sqrt{N}}$
\end{center}
\end{minipage}

\vspace{1em}

\hrulefill

\vspace{0.5em} $\bullet\quad$
\textbf{Dropping robustness $\ne$ sampling robustness in general.\\}
\textit{Proof: }
$\shape \ne \frac{1.96}{\sqrt{N}}$.

\vspace{0.5em} $\bullet\quad$
\textbf{When the SNR is small, sufficiently large $N$
produces sampling robustness, but not necessarily
dropping robustness.\\}
\textit{Proof: }
$\frac{1.96}{\sqrt{N}} \rightarrow 0$, but $\shape \rightarrow$ a nonzero
constant.

\vspace{0.5em} $\bullet\quad$
\textbf{Statistical insignificance is dropping non-robust for large $N$.\\}
\textit{Proof: }
%
Insignificance means
$|\thetafun(\thetahat(\onevec))| \le \frac{1.96}{\sqrt{N}} \noise$.

$\Rightarrow$ A result can be made significant by a change of no more than
$\frac{1.96}{\sqrt{N}} \noise$.

$\Rightarrow$ The SNR for a conclusion
of ``insignificance'' is $\frac{\Delta}{\noise} \le \frac{1.96}{\sqrt{N}}
\rightarrow 0 \le \shape$.

\vspace{0.5em} $\bullet\quad$
\textbf{P-hacking is dropping non-robust for large $N$.\\}
\textit{Proof: }P-hacked effect sizes are of the order
$\frac{1.96}{\sqrt{N}} \noise$.

\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[t]{What makes an analysis sensitive?}
%
\begin{minipage}[t]{0.45\textwidth}
\begin{center}
    Robust to data dropping:\\
    (``dropping robustness'')\\
    \vspace{1em}
    $\textrm{SNR} := \frac{\Delta}{\noise} \ge \shape$
\end{center}
\end{minipage}
%
\begin{minipage}[t]{0.45\textwidth}
\begin{center}
    Robust to gross errors:\\
    (``gross error robustness'')\\
    \vspace{1em}
    Gross outliers cannot produce
    arbitrarily large changes to $\thetafun$.
\end{center}
\end{minipage}

\vspace{1em}
\hrulefill

\vspace{1em} $\bullet\quad$
\textbf{Dropping non-robustness is not driven by misspecification.\\}
\textit{Proof: }
Small $\Delta$ are dropping non-robust irrespective of specification.

\vspace{1em} $\bullet\quad$
\textbf{Gross outliers primarily affect dropping robustness through $\noise$.\\}
\textit{Proof: }
For a fixed $\noise$, outliers decrease $\shape$.
(Details in paper.)

\end{frame}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[t]{How to make an analysis less sensitive?}

\begin{center}
    Robust to data dropping:\\
    (``dropping robustness'')\\
    \vspace{1em}
    $\textrm{SNR} := \frac{\Delta}{\noise} \ge \shape$
\end{center}

\vspace{1em}
\hrulefill

\vspace{1em}
\textbf{To achieve dropping robustness,
reduce $\noise$ and / or increase $\Delta$.\\}
\textit{Proof: }
Across typical distributions, $\shape$ varies little.
(Details in paper.)

% \vspace{1em}
% \hrulefill

\vspace{1em}
In the Mexico microcredit example,
%
\begin{align*}
%
\noise = \MxNoise
\quad\quad\quad
\thetafun(\thetahat(\onevec)) = \MxBetahat
\quad\quad\quad
N = \MxNobs
%
\end{align*}
%
The study overcame a very low signal to noise ratio with a very large $N$.

\vspace{1em} This (canonical) response to low signal to noise ratio --- to
gather more data --- produces small SEs, but cannot produce dropping
robustness.

\end{frame}
