




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{What makes an analysis sensitive?}
%
We are ``sensitive to data dropping'' if, for some $\Delta$ large enough to
change conclusions, $\exists \w^*$ dropping $\alphan$ points such that
%
\begin{align*}
% $
%
\textrm{``Signal''} :=
\Delta < \thetafunlin(\w^*) - \thetafun(\thetahat(\onevec))  =
    {\color{red}- \sum_{n=1}^{\lfloor \alpha N \rfloor} \infl_{(n)} }
    =:  {\color{red} \noise } { \color{red} \shape}
%
% $
\end{align*}
%
% \hspace{1em} where \vspace{1em}

\begin{itemize}
\item The ``noise'' $\noise^2 \rightarrow \mathrm{Var}(\sqrt{N}\phi)$
    (``sandwich'' variance estimator)
\item The ``shape''
    $\shape := \frac{-\sum_{n=1}^{\lfloor \alpha N \rfloor} \infl_{(n)}}{\noise}$
    $\rightarrow$ nonzero constant
    $\le \sqrt{\alpha (1 - \alpha)}$
\end{itemize}

\begin{center}
\begin{minipage}{0.8\textwidth}
\begin{tikzpicture}
    \node[anchor=south west,inner sep=0] (image) at (0,0) {
    \SimInflHistogram{}
    };
    \begin{scope}[x={(image.south east)},y={(image.north west)}]
        \draw [stealth-stealth][thick][white](0.42, 0.35) -- (0.6, 0.35);
        \draw (0.51, 0.35) node[below][text width=3cm][align=center][white]
            {\normalsize $\noise$};

        \draw [stealth-][thick][red](0.3, 0.25) -- (0.3, 0.5);
        \draw (0.25, 0.5) node[above][text width=3cm][align=center][red]
            {\normalsize $-\sum_{n=1}^{\alphan} \infl_{(n)}$};
    \end{scope}
\end{tikzpicture}
\end{minipage}
\end{center}

\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[t]{Example.}
%
\vspace{-2em}
\begin{align*}
%
\alpha :={}& \textrm{Proportion of points to drop}\\
\Delta :={}& \textrm{Signal (difference large enough to change conclusions)}\\
\noise :={}& \textrm{Noise (consistent estimator of }
    \var{}{\sqrt{N} \thetafun}\textrm{)}\\
\shape :={}& \textrm{Shape (bounded by }
    \sqrt{\alpha(1-\alpha)}\textrm{ and given by }
    N\infl_n\textrm{ tail shape)}
%
\end{align*}
%
\hrulefill

Sensitive to data dropping if:
%
\begin{align*}
%
\thetafunlin(\w^*) - \thetafun(\thetahat(\onevec))  =
    \noise \shape \ge \Delta
\quad\quad
\Leftrightarrow
\quad\quad
\frac{\Delta}{\noise} \le \shape.
%
\end{align*}
%
The \textbf{signal to noise ratio} $\frac{\Delta}{\noise}$
determines sensitivity to data dropping.

\hrulefill

\textbf{Contrast with standard errors.}  A 95\% CI is given by
%
%\begin{align*}
%
$
\thetafun(\thetahat(\onevec)) \pm \frac{1.96}{\sqrt{N}} \noise.
$
%
%\end{align*}
%
We fail to reject the value
$\thetafun(\thetahat(\onevec)) + \Delta$ when
%
\begin{align*}
%
\thetafun(\thetahat(\onevec)) + \Delta \le
\thetafun(\thetahat(\onevec)) + \frac{1.96}{\sqrt{N}} \noise
\quad\quad
\Leftrightarrow
\quad\quad
\frac{\Delta}{\noise} \le \frac{1.96}{\sqrt{N}}.
%
\end{align*}
%
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[t]{Corollaries.}
%
\begin{minipage}{0.45\textwidth}
\begin{center}
    Robust to data dropping:\\
    (``dropping robustness'')\\
    \vspace{1em}
    $\textrm{SNR} = \frac{\Delta}{\noise} > \shape$
\end{center}
\end{minipage}
%
\begin{minipage}{0.45\textwidth}
\begin{center}
    Robust to sampling variation:\\
    (``sampling robustness'')\\
    \vspace{1em}
    $\textrm{SNR} = \frac{\Delta}{\noise} >
        \frac{1.96}{\sqrt{N}} \noise$
\end{center}
\end{minipage}

\vspace{1em}

\hrulefill

\vspace{1em} $\bullet\quad$
\textbf{Dropping robustness $\ne$ sampling robustness in general.\\}
\textit{Proof: }
$\shape \ne \frac{1.96}{\sqrt{N}} \noise$.

\vspace{1em} $\bullet\quad$
\textbf{When the SNR is small, sufficiently large $N$
produces sampling robustness, but not necessarily
dropping robustness.\\}
\textit{Proof: }
$\frac{1.96}{\sqrt{N}} \noise \rightarrow 0$, but $\shape \rightarrow$ a nonzero
constant.

\vspace{1em} $\bullet\quad$
\textbf{Statistical insignificance is dropping non-robust for large $N$.\\}
\textit{Proof: }
%
Insignificance means
$|\thetafun(\thetahat(\onevec))| \le \frac{1.96}{\sqrt{N}} \noise$.

$\Rightarrow$ A result can be made significant by a change of no more than
$\frac{1.96}{\sqrt{N}} \noise$.

$\Rightarrow$ The SNR for a conclusion
of ``insignificance'' is $\frac{\Delta}{\noise} \le \frac{1.96}{\sqrt{N}}
\rightarrow 0 \le \shape$.

\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[t]{Corollaries.}
%
\begin{minipage}[t]{0.45\textwidth}
\begin{center}
    Robust to data dropping:\\
    (``dropping robustness'')\\
    \vspace{1em}
    $\textrm{SNR} = \frac{\Delta}{\noise} > \shape$
\end{center}
\end{minipage}
%
\begin{minipage}[t]{0.45\textwidth}
\begin{center}
    Robust to gross errors:\\
    (``gross error robustness'')\\
    \vspace{1em}
    Gross outliers cannot produce
    arbitrarily large changes to $\thetafun$.
\end{center}
\end{minipage}

\vspace{1em}
\hrulefill

\vspace{1em} $\bullet\quad$
\textbf{Dropping non-robustness is not driven by misspecification.\\}
\textit{Proof: }
Small $\Delta$ are dropping non-robust irrespective of specification.

\vspace{1em} $\bullet\quad$
\textbf{Gross outliers primarily affect dropping robustness through $\noise$.\\}
\textit{Proof: }
For a fixed $\noise$, outliers decrease $\shape$.
(See paper for details.)

\vspace{1em} $\bullet\quad$
\textbf{To achieve dropping robustness,
reduce $\noise$ and / or increase $\Delta$.\\}

\end{frame}
