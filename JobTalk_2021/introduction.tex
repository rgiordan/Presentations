\begin{frame}{Dropping data: Motivation}

All the time in data science, we:

\begin{itemize}
    \item Gather + clean exchangeable data,
    \item Specify and fit a model, and
    \item Drawn a qualitative conclusion from your fit
    \\(e.g., based on the sign / significance of
        some estimated parameter).
\end{itemize}

\vspace{1em}
Decisions are important: We want \textbf{trustworthy} conclusions.

Data / models not always perfect: We want \textbf{robust} conclusions.

\vspace{1em}
\textbf{Running example:} \citet{angelucci2015microcredit}, a
randomized controlled trial study of the efficacy of microcredit in Mexico based
on 16,560 data points.

\vspace{1em}
Would you be concerned if you could \textbf{reverse your conclusion} by removing
a \textbf{small proportion} (say, $0.1\%$) of your data?

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Dropping data: Mexico Microcredit}

Consider \citet{angelucci2015microcredit}, a randomized controlled trial study
of the efficacy of microcredit in Mexico based on 16,560 data points.

The variable ``Beta" estimates the effect of microcredit in US dollars.

%\MicrocreditMexicoRerunTable{}

\begin{table}[ht]
\centering
\begin{tabular}{lll} \hline
  & \onslide<2->{Left out points} & Beta (SE) \\\hline
\hspace{0.05em} Original result & \onslide<2->{0} & -4.55 (5.88) \\ \hline
\onslide<2-> {\hspace{0.05em} Change sign & 1 & 0.4 (3.19) \\\hline }
\onslide<3-> {Change significance & 14 & -10.96 (5.57) \\\hline }
\onslide<4-> {Change both & 15 & 7.03 (2.55) \\\hline }
\end{tabular}
\end{table}

\vspace{-1em}
\onslide<5-> { By removing very few data points ($15 / 16560 \approx 0.1\% $),
we can reverse the qualitative conclusions of the original study! }

\end{frame}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Dropping data: Motivation}

Would you be concerned if you could \textbf{reverse your conclusion} by removing
a \textbf{small proportion} (say, $0.1\%$) of your data?

\pause
Not always!  But sometimes, surely yes.

Thinking without random noise can be helpful.

Suppose you have a farm, and want to know whether
your average yield is greater than 170 bushels per acre.
At harvest, you measure 200 bushels per acre.

\begin{itemize}
    \item Scenario one: If your yield is greater than 170 bushels
        per acre, you make a profit.
        \begin{itemize}
            \item Donâ€™t care about sensitivity to small subsets
        \end{itemize}
    \item Scenario two: You want to recommend your farming
    methods to a friend across the valley.
    \begin{itemize}
        \item Might care about sensitivity to small subsets
    \end{itemize}
\end{itemize}

For example, often in economics:
%
\begin{itemize}
\item Small fractions of data are missing not-at-random,
\item Policy population is different from analyzed population,
\item We report a convenient summary (e.g. mean) of a complex effect,
\item Models are stylized proxies of reality.
\end{itemize}


\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[t]{Objective}

Estimate the effect of leaving out $\lfloor \alpha N \rfloor$ datapoints,
where $\alpha$ is small.
\pause

\vspace{1em}
\textbf{Question 1: How do we find influential datapoints?}

\only<2>{
\small

The number of subsets ${N \choose \lfloor \alpha N \rfloor}$ can be very large
even when $\alpha$ is very small.

In the MX microcredit study, ${16560 \choose 15} \approx 1.4 \cdot 10^{51}$
sets to check for $\alpha = 0.0009$.

We provide a fast, automatic approximation based on the \textbf{influence function}.
}

\onslide<3->{
\pause
\vspace{1em}
\textbf{Question 2: What makes an estimator non-robust?}
}

\only<4>{
Non-robustness to removal of $\lfloor \alpha N \rfloor$ points is:
\vspace{-0.5em}
\begin{itemize}
    \item Not (necessarily) caused by misspecification.
    \item Not (necessarily) caused by outliers.
    \item Not captured by standard errors.
    \item Not mitigated by large $N$.
    \item Primarily determined by the \textbf{signal to noise} ratio
    \begin{itemize}
        \item[] ... in a sense which we will define.
    \end{itemize}
\end{itemize}
}

\onslide<5->{
\vspace{1em}
\textbf{Question 3: When is our approximation accurate?}
}

\only<6>{
\begin{itemize}
    \item We provide deterministic error bounds for small $\alpha$.
    \item We show the accuracy in simple experiments.
    \item We show the accuracy in a number of real-world experiments.
\end{itemize}
}

\onslide<7->{
\vspace{1em}
\textbf{Conclusion: Related work and future directions}
}

\end{frame}
