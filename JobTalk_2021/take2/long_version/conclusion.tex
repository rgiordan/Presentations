\begin{frame}{Conclusion}

\begin{itemize}
\item You may be concerned if you could reverse your conclusion by removing
a $\lfloor \alpha N \rfloor$ datapoints, for some small $\alpha$.

\pause
\item Using a linear approximation, we can quickly and automatically find an
approximate influential set which is accurate for small $\alpha$.

\pause
\item Robustness to removing a $\lfloor \alpha N \rfloor$ datapoints is
principally determined by the signal to noise ratio, does not disappear
asymptotically, and is distinct from (and typically larger than) standard
errors.

\pause
\item Robustness to removing a $\lfloor \alpha N \rfloor$ datapoints is
easy to check, and non-robustness should at least be reported.

\end{itemize}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Links and references}

\footnotesize

% {\bf A workshop paper: }\newline
Tamara Broderick, Ryan Giordano, Rachael Meager (alphabetical authors) \newline
``An Automatic Finite-Sample Robustness Metric: Can Dropping a Little Data Change Conclusions?''
\newline {\color{blue}\url{https://arxiv.org/abs/2011.14999}}

See the paper for applications to:
\vspace{-1em}
\begin{itemize}
    \item[--] Hierarchical meta-analysis of microcredit \citep{meager2020aggregating} \vspace{-0.5em}
    \item[--] Cash transfers randomized controlled trial \citep{angelucci2009indirect} \vspace{-0.5em}
    \item[--] Oregon Medicaid experiment \citep{finkelstein2012oregon} \vspace{-0.5em}
    \item[--] Expository simulations
\end{itemize}

% \vspace{0.5em}

% {\bf Code: }\newline
% Paragami: parameter folding and flattening for optimization problems \newline
% {\color{blue}\url{https://github.com/rgiordan/paragami}}
%
% Vittles: library for sensitivity analysis in optimization problems \newline
% {\color{blue}\url{https://pypi.org/project/vittles/}}
%
\texttt{zaminfluence}: \texttt{R} package with leave-$\alpha$-out
robustness for OLS and IV estimators \newline
{\color{blue}\url{https://github.com/rgiordan/zaminfluence}}
\vspace{-0.5em}

\par\noindent\rule{\textwidth}{0.4pt}

\vspace{-0.5em}

\bibliographystyle{plainnat}
% Hide the references header
% https://tex.stackexchange.com/questions/22645/hiding-the-title-of-the-bibliography/370784
\begingroup
\renewcommand{\section}[2]{}%
{
\tiny
%\scriptsize
\bibliography{bibliography}
}
\endgroup

\end{frame}
