\input{_headers.tex}
\usepackage{enumitem}
\setlist{nolistsep}

\usepackage{geometry}
%\geometry{margin=1.2in}
\geometry{top=1.in}
\geometry{left=1.5in}
\geometry{right=1.5in}

\title{Ryan Giordano Research Statement}

\author{
  Ryan Giordano \\ \texttt{rgiordan@mit.edu }
}

\begin{document}

\begin{minipage}[t]{0.5\textwidth}
\hspace{-2em} % Easier than doing it right!
{\bf \LARGE Research Statement}\\
\end{minipage}
\begin{minipage}[t]{0.5\textwidth}
%    \begin{flushright}
        \hspace{7.5em} % Easier than doing it right!
        {\LARGE Ryan Giordano}
%    \end{flushright}
\end{minipage}

Many researchers would be concerned if they learned that some core conclusion of
their statistical analysis---such as the sign or statistical significance of some
key effect---could be overturned by removing a small fraction, say 0.1\%, of
their data.  Such non-robustness would be particularly concerning if the data
was not actually drawn randomly from precisely the population of interest, or if
the model may have been misspecified---circumstances that often obtain in  the
social sciences, for example.  Nevertheless, analysts do not routinely check
whether ablation of such a small set could overturn their results, in part
because the number of possible subsets containing 0.1\% of the data points is
combinatorially large.  To find the worst-case subset of a given size that, when
removed, induces the largest change in a statistical estimate is, in general, a
difficult discrete optimization problem.

In \citet{giordano:2020:amip}, I identify problematic subsets of the data by
forming a {\em linear approximation} to how statistical estimators depend on
their datasets. In particular, I form an approximation for estimators that can be
expressed as the root of a smooth estimating equation---a broad class that
includes maximum likelihood (MLE), ordinary least squares (OLS), instrumental
variable (IV), generalized method of moments (GMM), and variational Bayes (VB)
estimators, among others.  The approximation can be computed quickly and
automatically using automatic differentiation \citep{baydin:2015:automatic,
autograd}, and I provide an \texttt{R} package to do so (\texttt{zaminfluence}
\citep{zaminfluence}).
% In my \texttt{R} package, \texttt{zaminfluence}
% \citep{zaminfluence}, I use automatic differentiation
% \citep{baydin:2015:automatic, autograd} to compute the approximation quickly and
% automatically for any user-supplied estimating equation; the approximations for
% OLS and IV are provided by default.
By ablating the subset identified by the approximation re-computing the
estimator only once, one can form a lower bound on the sensitivity since, at
worst, the approximation identified a sub-optimal subset.

An approximation is only as useful as its accuracy.  Using techniques from
\citet{giordano:2019:hoij}, I provide finite-sample accuracy bounds for the
approximation  in terms of the set complexity of the estimating equation and its
derivatives; for OLS and IV, our error bounds are exactly computable using only
the regression output.  I show that the approximation's relative error is small
when the proportion of data removed, even if the total number of data points
removed is very large.

By studying properties of the approximation, I show that our metric captures a
type of non-robustness that is qualitatively different from classical
robustness.  Non-robustness to the ablation of small datasets is driven by low
signal-to-noise ratio in the inference problem, is not reflected in standard
errors, does not disappear asymptotically, and is not a product of
misspecification.

To show that we can detect meaningful non-robustness in real datasets, my
co-authors and I applied our methods to a number of published studies in
econometrics.  Though some results were robust to the ablation of small subsets
of the data, others were not. For example, in a study of microcredit in Mexico
\citep{angelucci:2015:microcredit}, we find that, by removing just 15 households
out of 16,561 studied (a change of less than 0.1\%), the estimated effect of
microcredit changes from negative and statistically insignificant to positive
and statistically significant.

The core idea underlying the above work underlies all of my current research,
and it is this: many tasks in data science are computationally difficult because
they require re-computation of a statistical estimator on inputs that are, in
some sense, ``near'' the original inputs, and Taylor series approximations can
provide fast and accurate approximations to expensive re-computation.  In my
work, I show that essentially the same ideas can provide fast approximations to
cross validation, the bootstrap, Bayesian prior sensitivity, and even posterior
covariance estimation for mean field variational Bayes.  For the remainder
of this essay, I will discuss each of these applications in turn, emphasizing
the ways in which I update classical results with intuitive, relevant theory
and easy-to-use computational tools.


\newpage

\subsection*{Approximate cross validation.}

The error or variability of machine learning algorithms is often assessed by
repeatedly re-fitting a model with different weighted versions of the observed
data; cross-validation (CV) can be thought of as a particularly popular example
of this technique.
%
In \citet{giordano:2019:ij}, I use a linear approximation to the
dependence of the fitting procedure on the weights, producing results that can
be faster than repeated re-fitting by an order of magnitude. I provide explicit
finite-sample error bounds for the approximation in terms of a small number of
simple, verifiable assumptions.  My results apply whether the weights and data
are stochastic or deterministic, and so can be used as a tool for proving the
accuracy of the infinitesimal jackknife on a wide variety of problems. As a
corollary, I state mild regularity conditions under which the approximation
consistently estimates true leave-$k$-out cross-validation for any fixed $k$. I
demonstrate the accuracy of the approximation on a range of simulated and real
datasets, including an unsupervised clustering problem from genomics
\citep{Luan:2003:clustering, shoemaker:2015:ultrasensitive}.




\subsection*{Approximately bootstrapping Bayesian posterior means.}

The frequentist (i.e., sampling) variance of Bayesian posterior expectations
differs in general from the posterior variance even for large datasets,
particularly when the model is misspecified or contains many latent variables
\citep{kleijn:2006:misspecification}.
Knowing the frequentist variance of a posterior expectation can be useful even
to a committed Bayesian, particularly when the data is known to arise from
random sampling and there is a possibility of model misspecification
\citep{waddell:2002:bayesphyloboot}.  However, the
principal existing approach for computing the frequentist variability from MCMC
procedures is the bootstrap, which can be extremely computationally intensive
due to the need to run hundreds of extra MCMC procedures
\citep{huggins:2019:bayesbag}.

In \citep{giordano:2020:bayesij, giordano:2020:stanconbayesij}, I propose an
efficient alternative to bootstrapping an MCMC procedure which is based on the
influence function from sensitivity analysis.  Using results from
\citep{giordano:2018:covariances, giordano:2019:ij}, I show that the influence
function for posterior expectations can be easily computed from the posterior
samples of a single MCMC procedure and consistently estimates the bootstrap
variance. I demonstrate the accuracy and computational benefits of the influence
function variance estimates on array of experiments including an election
forecasting model \citep{economist:2020:election}, the Cormack-Jolly-Seber model
from ecology \citep{kery:2011:bayesian}, and a large collection of models and
datasets from the social sciences \citep{gelman:2006:arm}.

\subsection*{Bayesian prior sensitivity.}

\paragraph{Prior sensitivity for discrete Bayesian nonparametrics.}

% From BNP_sensitivity/writing/NIPS_2018_BNP_workshop
A central question in many probabilistic clustering problems is how many
distinct clusters are present in a particular dataset. A Bayesian nonparametric
(BNP) model addresses this question by placing a generative process on cluster
assignment, making the number of distinct clusters present amenable to Bayesian
inference.  However, like all Bayesian approaches, BNP requires the
specification of a prior, and this prior may favor a greater or lesser number of
distinct clusters.
% In practice, it is important to quantitatively establish that
% the prior is not too informative, particularly when---as is often the case in
% BNP---the particular form of the prior is chosen for mathematical convenience
% rather than because of a considered subjective belief.

In \citep{giordano:2018:bnpsensitivity}, I derive prior sensitivity measures for
a truncated variational Bayes approximation using ideas from
\citep{gustafson:1996:localposterior, giordano:2018:covariances}. Unlike
previous work on local Bayesian sensitivity for BNP
\citep{Basu:2000:BNP_robustness}, I pay special attention to the ability of the
sensitivity measures to \emph{extrapolate} to different priors, rather than
treating the sensitivity as a measure of robustness \textit{per se}. In work
currently in progress \citep{liu:2020:bnpjisba}, my co-author and I apply the
approximation from \citep{giordano:2018:bnpsensitivity} to an unsupervised
clustering problem on a human genome dataset \citep{huang:2011:haplotype,
raj:2014:faststructure}, demonstrating that the approximate is accurate, orders
of magnitude faster than re-fitting, and capable of detecting meaningful prior
sensitivity.



\paragraph{Prior sensitivity for Markov Chain Monte Carlo.}

MCMC is arguably the most commonly used computational tool to estimate Bayesian
posteriors, which is made still easier by modern black-box MCMC tools such as
\texttt{Stan} \citep{carpenter:2017:stan, rstan}.  However, a single run of MCMC
typically remains time-consuming, and systematically exploring alternative prior
parameterizations by re-running MCMC would be computationally prohibitive for
all but the simplest models.

My software package, \texttt{rstansensitivity},
\citep{giordano:2020:rstansensitivity, giordano:2018:mcmchyper}, takes advantage
of the automatic differentiation capacities of \texttt{Stan}
\citep{carpenter:2015:stanmath} together with a classical result from  Bayesian
robustness \citep{gustafson:1996:localposterior, basu:1996:local,
giordano:2018:covariances} to provide automatic hyperparameter sensitivity for
generic \texttt{Stan} models from only a single MCMC run.  I demonstrate the
speed and utility of the package in detecting excess prior sensitivity,
particularly in a social sciences model taken from \citet[Chapter
13.5]{gelman:2006:arm}.


\subsection*{Uncertainty propagation in mean-field variational Bayes.}

Mean-field Variational Bayes (MFVB) is an approximate Bayesian posterior
inference technique that is increasingly popular due to its fast runtimes on
large-scale scientific data sets (e.g., \citet{raj:2014:faststructure,
kucukelbir:2017:advi, regier:2019:cataloging}). However, even when MFVB provides
accurate posterior means for certain parameters, it often mis-estimates
variances and covariances \citep{wang:2005:inadequacy, turner:2011:two} due to
its inability to propagate Bayesian uncertainty between statistical parameters.

In \citet{giordano:2015:linear, giordano:2018:covariances}, I derive a simple
formula for the effect of infinitesimal model perturbations on MFVB posterior
means, thus providing improved covariance estimates and greatly expanding the
practical usefulness of MFVB posterior approximations. The estimates for MFVB
posterior covariances rely on a result from the classical Bayesian robustness
literature that relates derivatives of posterior expectations to posterior
covariances and includes the Laplace approximation as a special case.
% The key condition is that the MFVB
% approximation provides good estimates of a select subset of posterior means---an
% assumption that has been shown to hold in many practical settings.
In the experiments, I demonstrate that my methods are simple, general, and
fast, providing accurate posterior uncertainty estimates and robustness measures
with runtimes that can be an order of magnitude faster than MCMC, including
models from ecology \citep{kery:2011:bayesian}, the social sciences
\citep{gelman:2006:arm}, and on a massive internet advertising dataset
\citep{criteo:2014:dataset}.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\newpage

\subsection*{Selected Future work}

My research is ideally driven by the needs of my scientific and industry
collaborators, and so I expect my future work will be determined to a large part
by my colleagues.  However, I will now discuss a few directions that I find
promising and interesting, and which I believe could be applicable to a diverse
set of problems.

\paragraph{The higher-order infinitesimal jackknife for the bootstrap.}

In the preprint \citet{giordano:2019:hoij}, I extend \citet{giordano:2019:ij} to
higher-order Taylor series approximations, providing a family of estimators
which I collectively call the higher-order infinitesimal jackknife (HOIJ).  In
addition to providing higher-quality approximations to CV and extending the
results to k-fold CV, the higher-order approach promises to provide a scalable
alternative to the bootstrap, a procedure that estimates frequentist variability
by repeatedly re-evaluating a model at datasets drawn with replacement from the
observed data. The bootstrap is known to enjoy higher-order accuracy in certain
circumstances \citet{hall:2013:bootstrap}, and the HOIJ can approach the
bootstrap at a rate faster than the bootstrap approaches the truth.  The HOIJ
thus promises to make bootstrap inference available to models which are
differentiable but too expensive to re-evaluate (e.g. simulation-based models
\citep[Section 2.6]{baker:2019:workshop}), but also to allow efficient
bootstrap-after-bootstrap procedures which that are currently out of reach for
all but the simplest statistics \citep{efron:1994:bootstrap}.


% \paragraph{Sensitivity for non-differentiable preprocessing.}
%
% Analyses in genomics often begin with a pre-processing step in observation units
% are clustered together according to ad-hoc measures of similarity across a large
% number of feature vectors \citep{xu:2015:identification,
% stuart:2019:comprehensive}.  Quickly assessing the sensitivity of such
% procedures to the inclusion or exclusion of individual features would allow the
% researcher to identify high-leverage observations and avoid imposing structure
% via arbitrary modeling assumptions.  However, ordinary sensitivity analysis
% cannot be applied directly to the clustering step, which is typically
% non-differentiable.  With a colleague from biology, I am currently investigating
% an importance sampling technique that would allow us to apply sensitivity
% analysis using a continuous relaxation only of the distance measure, while
% retaining the non-differentiable clustering step. Ideally, this sensitivity
% analysis would allow for quick exploration of the high-dimensional space of
% feature inclusion, similar to our work in \citet{giordano:2020:amip}.

% With a colleague from biology, I am currently investigating a technique that
% would use importance sampling to compute the sensitivity of such a
% non-differentiable pre-processing step.  We first form a probabilistic
% relaxation only of the similarity measures, and then run the non-differentiable
% clustering for an ensemble of Monte Carlo samples of the similarities.  Removing
% a feature would change the probabilities of the similarity measure draws.  By
% differentiating the importance sampling estimate of the effect of the changing
% probabilities, we can form black-box sensitivity measures with little extra
% computation other than clustering the original similarity ensemble.  Ideally,
% this sensitivity analysis would allow for quick exploration of the
% high-dimensional space of feature inclusion, similar to our work in
% \citet{giordano:2020:amip}.

\paragraph{Scaling sensitivity measures.}

Sensitivity analysis typically avoids the expense of re-fitting a model, but
incurs the expense of solving a large linear system.  Thus, extending
the benefits of the sensitivity analysis to increasingly large scientific
problems requires developing methods to efficiently solve correspondingly large
linear systems.  Stochastic second-order methods are currently an active
research topic in optimization \citep{agarwal:2017:secondorder,
berahas:2020:newtonsketch}, and methods developed therein should apply
directly to sensitivity analysis.

\paragraph{Partitioned Bayesian inference.}

The ideas of \citep{giordano:2018:covariances} can be naturally extended to
approximately propagate uncertainty among separately estimated components of an
inference problem.  For example, astronomical catalogs are customarily produced
with MFVB-like algorithms \citep{lang:2016:tractor, regier:2019:cataloging},
which take inputs such as the sky background and optical point spread function
as fixed inputs, though these quantities are themselves inferred with
uncertainty.  Viewing all the separate inference procedures as a sequential
quasi-MFVB objective, one could directly apply the techniques of
\citep{giordano:2018:covariances} to propagate the uncertainty from the modeling
inputs to the astronomical catalog's uncertainty.



\newpage

\bibliography{references}
\bibliographystyle{plainnat}

\end{document}
