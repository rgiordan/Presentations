\input{_headers.tex}
\usepackage{enumitem}
\setlist{nolistsep}

\usepackage{geometry}
%\geometry{margin=1.2in}
\geometry{top=0.9in}
\geometry{left=1.3in}
\geometry{right=1.3in}

\title{Ryan Giordano Research Statement}

\author{
  Ryan Giordano \\ \texttt{rgiordan@mit.edu }
}

\begin{document}

\begin{minipage}[t]{0.5\textwidth}
\hspace{-2em} % Easier than doing it right!
{\bf \LARGE Research Statement}\\
\end{minipage}
\begin{minipage}[t]{0.5\textwidth}
%    \begin{flushright}
        \hspace{7.5em} % Easier than doing it right!
        {\LARGE Ryan Giordano}
%    \end{flushright}
\end{minipage}

Many researchers would be concerned if they learned that some core conclusion of
their statistical analysis---such as the sign or statistical significance of some
key effect---could be overturned by removing a small fraction, say 0.1\%, of
their data.  Such non-robustness would be particularly concerning if the data
were not actually drawn randomly from precisely the population of interest, or if
the model may have been misspecified---circumstances that often obtain in  the
social sciences, for example.  Nevertheless, analysts do not routinely check
whether ablation of such a small set could overturn their results, in part
because the number of possible subsets containing 0.1\% of the data points is
combinatorially large.
% To find the worst-case subset of a given size that, when
% removed, induces the largest change in a statistical estimate is, in general, a
% difficult discrete optimization problem.

In \citet{zaminfluence} and \citet{giordano:2020:amip}, I identify problematic
subsets of the data using {\em sensitivtiy analysis}---that is, by forming a
linear approximation to how a wide class of statistical estimators depend on
their datasets.  The key idea is that, although there are a very large number of
subsets containing 0.1\% of the data points, none of them are very different
from the original dataset, and so we expect the linear approximation to work
well.  I confirm this intuition with finite-sample accuracy bounds in terms of
intuitive and verifiable assumptions.  I provide an \texttt{R} package
\citep{zaminfluence} to compute the approximation quickly and automatically
using automatic differentiation \citep{baydin:2015:automatic, autograd}.  And,
with my co-authors, I show that the approximation is capable of detecting
meaningful non-robustness in several published econometrics analyses. For
example, in a study of microcredit in Mexico \citep{angelucci:2015:microcredit},
we find that, by removing just 15 households out of 16,561 studied (a change of
less than 0.1\%), the estimated effect of microcredit changes from negative and
statistically insignificant to positive and statistically significant.

In fact, my research shows that many standard, computationally demanding data
analysis tasks are also amenable to fast, automatic approximation using
sensitivity analysis. For example:
%
\begin{itemize}
    %
\item Cross validation (CV) requires repeatedly leaving out subsets of the
observed data and re-evaluating a statistical estimator. By forming a Taylor
series approximation on the dependence of the estimator on the left-out set, I
provide fast approximations to CV with finite-sample accuracy guarantees
\citep{giordano:2019:ij}.
%
\item Prior specification encodes key assumptions in Bayesian statistics.  But
Bayesian inference can be sensitive to prior specification, and evaluating the
sensitivty of Bayesian posterior expectations to prior specification by
re-fitting is typically computationally prohibitive due both to the large space
of possible priors (often infinite dimensional), as well as the high
computational cost of evaluating even a single posterior approximation.  By
forming a Taylor series approximation to the dependence of the posterior mean on
the prior, I can explore the consequences of alternative prior functional forms
at a small fraction of the cost of exact re-fitting
\citep{giordano:2018:bnpsensitivity, giordano:2020:rstansensitivity}.
%
\item Evaluating the frequentist variability of Bayesian estimators formed using
Markov Chain Monte Carlo (MCMC) can be particularly important when there is
concern about model misspecification.  This frequentist variability can be
evaluated by the bootstrap, but at the considerable cost of re-running the MCMC
chain hundreds of times.  By approximating the dependence of the posterior on
the data with sensitivity analysis, I compute consistent estimates of the
frequentist variance using only a single MCMC chain---orders of magnitude faster
than the bootstrap \citep{giordano:2020:stanconbayesij}.
%
\item Mean field variational Bayes (MFVB) is a popular posterior approximation
method for Bayesian problems which are too large to be tractable by Markov Chain
Monte Carlo \citep{blei:2017:variational, regier:2019:cataloging}.  However,
MFVB approximations provide notoriously poor estimates of posterior uncertainty
\citep{turner:2011:two}.  In \citet{giordano:2018:covariances}, I show that
accurate posterior covariances can be recovered from MFVB approximations with
sensitivity analysis by exploiting a duality between Bayesian covariances and
senstivity.
%
\end{itemize}

For the remainder of this essay, I will discuss each of these applications,
emphasizing the ways in which I update classical results with intuitive,
relevant theory and easy-to-use computational tools.

\newpage



\subsection*{Robustness to data ablation.}

In \citet{zaminfluence} and \citet{giordano:2020:amip}, I propose a method to
assess the sensitivity of statistical analyses to the removal of a small
fraction of the sample. Analyzing all possible data subsets of a certain size is
computationally prohibitive, so I provide a finite-sample metric to
approximately compute the number (or fraction) of observations that has the
greatest influence on a given result when dropped. I provide explicit
finite-sample error bounds on my approximation for linear and instrumental
variables regressions. At minimal computational cost, the metric provides an
exact finite-sample lower bound on sensitivity for any estimator, so any
non-robustness my metric finds is conclusive. I demonstrate that non-robustness
to data ablation is driven by a low signal-to-noise ratio in the inference
problem, is not reflected in standard errors, does not disappear asymptotically,
and is not inherently a product of outliers or misspecification.

The approximation works for Z-estimators based on smooth estimating equations, a
class which includes ordinary least squares, instrumental variables, generalized
method of moments, variational Bayes, and maximum likelihood estimators. Using
my \texttt{R} package \citep{zaminfluence}, the approximation is automatically
computable from the specification of the estimating equation alone.  By
analyzing several published econometric analyses \citep{angelucci:2009:indirect,
finkelstein:2012:oregon, meager:2019:microcredit}, I show that even 2-parameter
linear regression analyses of randomized trials can be highly sensitive.  While
I find some applications are robust, in others the sign of a treatment effect
can be changed by dropping less than 1\% of the sample even when standard errors
are small.


\subsection*{Approximate cross validation.}

The error or variability of machine learning algorithms is often assessed by
repeatedly re-fitting a model with different weighted versions of the observed
data; cross-validation (CV) can be thought of as a particularly popular example
of this technique.
%
In \citet{giordano:2019:ij}, I use a linear approximation to the dependence of
the fitting procedure on the weights, producing results that can be faster by an
order of magnitude than repeated re-fitting. I provide explicit finite-sample
error bounds for the approximation in terms of a small number of simple,
verifiable assumptions.  My results apply whether the weights and data are
stochastic or deterministic, and so can be used as a tool for proving the
accuracy of the infinitesimal jackknife on a wide variety of problems. As a
corollary, I state mild regularity conditions under which the approximation
consistently estimates true leave-$k$-out cross-validation for any fixed $k$. I
demonstrate the accuracy of the approximation on a range of simulated and real
datasets, including an unsupervised clustering problem from genomics
\citep{Luan:2003:clustering, shoemaker:2015:ultrasensitive}.




\subsection*{Approximately bootstrapping Bayesian posterior means.}

The frequentist (i.e., sampling) variance of Bayesian posterior expectations
differs in general from the posterior variance even for large datasets,
particularly when the model is misspecified or contains many latent variables
\citep{kleijn:2006:misspecification}. Unlike the posterior variance, the
frequentist variance is meaningful even in the presence of misspecification,
particularly when the data is known to arise from random sampling
\citep{waddell:2002:bayesphyloboot}.  However, the principal existing approach
for computing the frequentist variability of MCMC procedures is the bootstrap,
which can be extremely computationally intensive due to the need to run hundreds
of extra MCMC procedures \citep{huggins:2019:bayesbag}.

In \citet{giordano:2020:bayesij, giordano:2020:stanconbayesij}, I propose an
efficient alternative to bootstrapping an MCMC procedure.  My approach is based
on the Bayesian analogue of the influence function from the classical
frequentist robustness literature.  Using results from
\citep{giordano:2018:covariances, giordano:2019:ij}, I show that the influence
function for posterior expectations can be easily computed from the posterior
samples of a single MCMC procedure and consistently estimates the bootstrap
variance. I demonstrate the accuracy and computational benefits of the influence
function variance estimates on array of experiments including an election
forecasting model \citep{economist:2020:election}, the Cormack-Jolly-Seber model
from ecology \citep{kery:2011:bayesian}, and a large collection of models and
datasets from the social sciences \citep{gelman:2006:arm}.

\subsection*{Bayesian prior sensitivity.}



\paragraph{Prior sensitivity for Markov Chain Monte Carlo.}

MCMC is arguably the most commonly used computational tool to estimate Bayesian
posteriors, which is made still easier by modern black-box MCMC tools such as
\texttt{Stan} \citep{carpenter:2017:stan, rstan}.  However, a single run of MCMC
typically remains time-consuming, and systematically exploring alternative prior
parameterizations by re-running MCMC would be computationally prohibitive for
all but the simplest models.

My software package, \texttt{rstansensitivity},
\citep{giordano:2020:rstansensitivity, giordano:2018:mcmchyper}, takes advantage
of the automatic differentiation capacities of \texttt{Stan}
\citep{carpenter:2015:stanmath} together with a classical result from  Bayesian
robustness \citep{gustafson:1996:localposterior, basu:1996:local,
giordano:2018:covariances} to provide automatic hyperparameter sensitivity for
generic \texttt{Stan} models from only a single MCMC run.  I demonstrate the
speed and utility of the package in detecting excess prior sensitivity,
particularly in a social sciences model taken from \citet[Chapter
13.5]{gelman:2006:arm}.


\paragraph{Prior sensitivity for discrete Bayesian nonparametrics.}

% From BNP_sensitivity/writing/NIPS_2018_BNP_workshop
A central question in many probabilistic clustering problems is how many
distinct clusters are present in a particular dataset. Discrete Bayesian
nonparametric (BNP) mixture models address this question by placing a generative
process on cluster assignment, making the number of distinct clusters present
amenable to Bayesian inference.  However, like all Bayesian approaches, BNP
requires the specification of a prior, and this prior may favor a greater or
lesser number of distinct clusters.
% In practice, it is important to quantitatively establish that
% the prior is not too informative, particularly when---as is often the case in
% BNP---the particular form of the prior is chosen for mathematical convenience
% rather than because of a considered subjective belief.

In \citet{giordano:2018:bnpsensitivity}, I derive prior sensitivity measures for
a truncated variational Bayes approximation using ideas from
\citep{gustafson:1996:localposterior, giordano:2018:covariances}. Unlike
previous work on local Bayesian sensitivity for BNP
\citep{Basu:2000:BNP_robustness}, I pay special attention to the ability of the
sensitivity measures to \emph{extrapolate} to different priors, rather than
treating the sensitivity as a measure of robustness \textit{per se}. In work
currently in progress \citep{liu:2020:bnpjisba}, my co-authors and I apply the
approximation from \citep{giordano:2018:bnpsensitivity} to an unsupervised
clustering problem on a human genome dataset \citep{huang:2011:haplotype,
raj:2014:faststructure}, demonstrating that the approximate is accurate, orders
of magnitude faster than re-fitting, and capable of detecting meaningful prior
sensitivity.


\subsection*{Uncertainty propagation in mean-field variational Bayes.}

Mean-field Variational Bayes (MFVB) is an approximate Bayesian posterior
inference technique that is increasingly popular due to its fast runtimes on
large-scale scientific data sets (e.g., \citet{raj:2014:faststructure,
kucukelbir:2017:advi, regier:2019:cataloging}). However, even when MFVB provides
accurate posterior means for certain parameters, it often mis-estimates
variances and covariances \citep{wang:2005:inadequacy, turner:2011:two} due to
its inability to propagate Bayesian uncertainty between statistical parameters.

In \citet{giordano:2015:linear, giordano:2018:covariances}, I derive a simple
formula for the effect of infinitesimal model perturbations on MFVB posterior
means, thus providing improved covariance estimates and greatly expanding the
practical usefulness of MFVB posterior approximations. The estimates for MFVB
posterior covariances rely on a result from the classical Bayesian robustness
literature that relates derivatives of posterior expectations to posterior
covariances and includes the Laplace approximation as a special case.
% The key condition is that the MFVB
% approximation provides good estimates of a select subset of posterior means---an
% assumption that has been shown to hold in many practical settings.
In the experiments, I demonstrate that my methods are simple, general, and
fast, providing accurate posterior uncertainty estimates and robustness measures
with runtimes that can be an order of magnitude faster than MCMC, including
models from ecology \citep{kery:2011:bayesian}, the social sciences
\citep{gelman:2006:arm}, and on a massive internet advertising dataset
\citep{criteo:2014:dataset}.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\newpage

\subsection*{Selected Future work}

My research is ideally driven by the needs of my scientific and industry
collaborators, and so I expect my future work will be determined to a large part
by my colleagues.  However, I will now discuss a few directions that I find
promising and interesting, and which I believe could be applicable to a diverse
set of problems.

\paragraph{The higher-order infinitesimal jackknife for the bootstrap.}

In the preprint \citet{giordano:2019:hoij}, I extend \citet{giordano:2019:ij} to
higher-order Taylor series approximations, providing a family of estimators
which I collectively call the higher-order infinitesimal jackknife (HOIJ).  In
addition to providing higher-quality approximations to CV and extending the
results to k-fold CV, the higher-order approach promises to provide a scalable
alternative to the bootstrap, a procedure that estimates frequentist variability
by repeatedly re-evaluating a model at datasets drawn with replacement from the
observed data. The bootstrap is known to enjoy higher-order accuracy in certain
circumstances \citep{hall:2013:bootstrap}, and the HOIJ can approach the
bootstrap at a rate faster than the bootstrap approaches the truth.  The HOIJ
thus promises to make bootstrap inference available to models which are
differentiable but too expensive to re-evaluate (e.g. simulation-based models
\citep[Section 2.6]{baker:2019:workshop}), but also to allow efficient
bootstrap-after-bootstrap procedures which that are currently out of reach for
all but the simplest statistics \citep{efron:1994:bootstrap}.


% \paragraph{Sensitivity for non-differentiable preprocessing.}
%
% Analyses in genomics often begin with a pre-processing step in observation units
% are clustered together according to ad-hoc measures of similarity across a large
% number of feature vectors \citep{xu:2015:identification,
% stuart:2019:comprehensive}.  Quickly assessing the sensitivity of such
% procedures to the inclusion or exclusion of individual features would allow the
% researcher to identify high-leverage observations and avoid imposing structure
% via arbitrary modeling assumptions.  However, ordinary sensitivity analysis
% cannot be applied directly to the clustering step, which is typically
% non-differentiable.  With a colleague from biology, I am currently investigating
% an importance sampling technique that would allow us to apply sensitivity
% analysis using a continuous relaxation only of the distance measure, while
% retaining the non-differentiable clustering step. Ideally, this sensitivity
% analysis would allow for quick exploration of the high-dimensional space of
% feature inclusion, similar to our work in \citet{giordano:2020:amip}.

% With a colleague from biology, I am currently investigating a technique that
% would use importance sampling to compute the sensitivity of such a
% non-differentiable pre-processing step.  We first form a probabilistic
% relaxation only of the similarity measures, and then run the non-differentiable
% clustering for an ensemble of Monte Carlo samples of the similarities.  Removing
% a feature would change the probabilities of the similarity measure draws.  By
% differentiating the importance sampling estimate of the effect of the changing
% probabilities, we can form black-box sensitivity measures with little extra
% computation other than clustering the original similarity ensemble.  Ideally,
% this sensitivity analysis would allow for quick exploration of the
% high-dimensional space of feature inclusion, similar to our work in
% \citet{giordano:2020:amip}.

\paragraph{Scaling sensitivity measures.}

Sensitivity analysis typically avoids the expense of re-fitting a model, but
incurs the expense of solving a large linear system.  Thus, extending
the benefits of the sensitivity analysis to increasingly large scientific
problems requires developing methods to efficiently solve correspondingly large
linear systems.  Stochastic second-order methods are currently an active
research topic in optimization \citep{agarwal:2017:secondorder,
berahas:2020:newtonsketch}, and methods developed therein should apply
directly to sensitivity analysis.

\paragraph{Partitioned Bayesian inference.}

The ideas of \citep{giordano:2018:covariances} can be naturally extended to
approximately propagate uncertainty among separately estimated components of an
inference problem.  For example, astronomical catalogs are customarily produced
with MFVB-like algorithms \citep{lang:2016:tractor, regier:2019:cataloging},
which take inputs such as the sky background and optical point spread function
as fixed inputs, though these quantities are themselves inferred with
uncertainty.  Viewing all the separate inference procedures as a sequential
quasi-MFVB objective, one could directly apply the techniques of
\citep{giordano:2018:covariances} to propagate the uncertainty from the modeling
inputs to the astronomical catalog's uncertainty.



\newpage

\bibliography{references}
\bibliographystyle{plainnat}

\end{document}
