\input{_headers.tex}
\usepackage{enumitem}
\setlist{nolistsep}

\title{Ryan Giordano Research Statement}

\author{
  Ryan Giordano \\ \texttt{rgiordan@mit.edu }
}

\begin{document}

\begin{minipage}[t]{0.5\textwidth}
\hspace{-2em} % Easier than doing it right!
{\bf \LARGE Reseach Statement}\\
\end{minipage}
\begin{minipage}[t]{0.5\textwidth}
%    \begin{flushright}
        \hspace{4.5em} % Easier than doing it right!
        {\LARGE Ryan Giordano}
%    \end{flushright}
\end{minipage}



In order to address the needs of twenty-first century scientific computing,
statistical models must often be able to quantify uncertainty, interrogate
modeling modeling assumptions, and evaluate predictive performance. However,
classical techniques to provied these desiderata can can become quite expensive,
especially in in high-dimensional machine learning and Bayesian models.
Consider, as motivating examples, the following ubiquitous data science tasks.
%
\begin{itemize}
%
        \item Cross validation (CV) is a fundamental tool in machine learning to
        evaluate model predictive performance and tune hyperparameters, but
        requires fitting a model multiple times with different data subsets left
        out.

        \item Uncertainty propagation, i.e., allowing the inferential
        uncertainty in one modeling quantity to inform the inferential
        uncertainty in another, is a key advantage of Bayesian statistics.
        However, the classical tool for Bayesian estimation, Markov Chain Monte
        Carlo (MCMC), requires evaluating a statistical model many times, and so
        can be computationally expensive.

        \item  Prior specification encodes key assumptions in Bayesian
        statistics, a paradigm for coherently solving statistical
        inverse problems.  But Bayesian inference can be sensitive to prior
        specification, particularly in high-dimensional models, and solving the
        inverse problem for multiple plausible prior choices can be
        computationally prohibitive.
%
\end{itemize}
%

% The more complicated a model is, the more likely it is that
% it may overfit the data (remedied by CV), the more difficult it is
% to reason subjectively about the prior (remedied by experimenting with
% a range of priors), and the more likely it is to model multiple, mutually
% dependent, uncertain quantities (remedied by uncertainty propagation).

These three central data science problems share the following commonality:
their computational demands are driven by the evaluation or estimation of a
statistical model multiple times for inputs that are, in some sense,
``close'' to one another.  Datasets chosen for cross-validation, for example,
are typically not too different from the original dataset.

In my research, I exploit this commonality to circumvent the computational
difficulties of these and other core tasks in data science by using {\em
sensitivity analysis}, by which I mean differential approximations to the
dependence of statistical models on their inputs.  In its most straightforward
form, sensitivity analysis amounts to forming {\em Taylor series approximations}
to extrapolate to nearby counterfactual model inputs, such as a new dataset with
some points left out for cross-validation \citep{giordano:2018:bnpsensitivity,
giordano:2019:ij}.  Other connections, e.g. between sensitivity analysis and
Bayesian posterior covariances \citep{giordano:2018:covariances}, are less
direct, but rely on the same conceptual and computational tools.

By evaluating the derivatives necessary to perform sensitivity analysis {\em
single model estimate}, I avoid the re-estimation or re-evaluation that makes
the above procedures computationally prohibitive.  In exchange, evaluating the
derivatives typically requires solving a large but sparse linear system, a
tradeoff that can be quite favorable in practice, providing good accuracy orders
of magnitude faster than the corresponding classical procedures.

The idea of sensitivity analysis is a venerable one, though the breadth of its
potential for contemporary data science problems is arguably underappreciated.
My work advances existing research by providing practical implementations of
classical methods, particularly using automatic differentiation
\citep{baydin:2015:automatic}, by updating classical theory to apply in finite
sample and under more realistic conditions, and by demonstrating unifying ideas
underlying superficially disparate applications.

For the remainder of the statement, I will discuss in more detail my
contributions, both in practice and in theory, to the above three data science
tasks and more.


\newpage

\paragraph{Approximate cross validation.}

The error or variability of machine learning algorithms is often assessed by
repeatedly re-fitting a model with different weighted versions of the observed
data; cross-validation (CV) and the bootstrap can be thought of as examples of
this technique.

In \citet{giordano:2019:ij}, I use a linear approximation to the
dependence of the fitting procedure on the weights, producing results that can
be faster than repeated re-fitting by an order of magnitude. I provide explicit
finite-sample error bounds for the approximation in terms of a small number of
simple, verifiable assumptions.  My results apply whether the weights and data
are stochastic or deterministic, and so can be used as a tool for proving the
accuracy of the infinitesimal jackknife on a wide variety of problems. As a
corollary, I state mild regularity conditions under which the approximation
consistently estimates true leave-$k$-out cross-validation for any fixed $k$. I
demonstrate the accuracy of the approximation on a range of simulated and real
datasets, including an unsupervised clustering problem from genomics
\citep{Luan:2003:clustering, shoemaker:2015:ultrasensitive}.


\paragraph{Prior sensitivity for Markov Chain Monte Carlo.}

MCMC is arguably the most commonly used computational tool to estimate Bayesian
posteriors, which is made still easier by modern black-box MCMC tools such as
\texttt{Stan} \citep{carpenter:2017:stan, rstan}.  However, a single run of MCMC
typically remains time-consuming, and systematically exploring alternative prior
parameterizations by re-running MCMC would be computationally prohibitive for
all but the simplest models.

My software package, \texttt{rstansensitivity},
\citep{giordano:2020:rstansensitivity, giordano:2018:mcmchyper}, takes advantage
of the automatic differentiation capacities of \texttt{Stan}
\citep{carpenter:2015:stanmath} together with a classical result from  Bayesian
robustness \citep{gustafson:1996:localposterior, basu:1996:local,
giordano:2018:covariances} to provide automatic hyperparameter sensitivity for
generic \texttt{Stan} models from only a single MCMC run.  I demonstrate the
speed and utility of the package in detecting excess prior sensitivity,
particularly in a social sciences model taken from \citet[Chapter
13.5]{gelman:2006:arm}.


\paragraph{Prior sensitivity for discrete Bayesian nonparametrics.}

% From BNP_sensitivity/writing/NIPS_2018_BNP_workshop
A central question in many probabilistic clustering problems is how many
distinct clusters are present in a particular dataset. A Bayesian nonparametric
(BNP) model addresses this question by placing a generative process on cluster
assignment, making the number of distinct clusters present amenable to Bayesian
inference.  However, like all Bayesian approaches, BNP requires the
specification of a prior, and this prior may favor a greater or lesser number of
distinct clusters.
% In practice, it is important to quantitatively establish that
% the prior is not too informative, particularly when---as is often the case in
% BNP---the particular form of the prior is chosen for mathematical convenience
% rather than because of a considered subjective belief.

In \citep{giordano:2018:bnpsensitivity}, I derive prior sensitivity measures for
a truncated variational Bayes approximation using ideas from
\citep{gustafson:1996:localposterior, giordano:2018:covariances}. Unlike
previous work on local Bayesian sensitivity for BNP
\citep{Basu:2000:BNP_robustness}, I pay special attention to the ability of the
sensitivity measures to \emph{extrapolate} to different priors, rather than
treating the sensitivity as a measure of robustness \textit{per se}. In work
currently in progress \citep{liu:2020:bnpjisba}, my co-author and I apply the
approximation from \citep{giordano:2018:bnpsensitivity} to an unsupervised
clustering problem on a human genome dataset \citep{huang:2011:haplotype,
raj:2014:faststructure}, demonstrating that the approximate is accurate, orders
of magnitude faster than re-fitting, and capable of detecting meaningful prior
sensitivity.


\paragraph{Uncertainty propagation in mean-field variational Bayes.}

Mean-field Variational Bayes (MFVB) is an approximate Bayesian posterior
inference technique that is increasingly popular due to its fast runtimes on
large-scale scientific data sets (e.g., \citet{raj:2014:faststructure,
kucukelbir:2017:advi, regier:2019:cataloging}). However, even when MFVB provides
accurate posterior means for certain parameters, it often mis-estimates
variances and covariances \citep{wang:2005:inadequacy, turner:2011:two} due to
its inability to propagate Bayesian uncertainty between statistical parameters.

In \citet{giordano:2015:linear, giordano:2018:covariances}, I derive a simple
formula for the effect of infinitesimal model perturbations on MFVB posterior
means, thus providing improved covariance estimates and greatly expanding the
practical usefulness of MFVB posterior approximations. The estimates for MFVB
posterior covariances rely on a result from the classical Bayesian robustness
literature that relates derivatives of posterior expectations to posterior
covariances and includes the Laplace approximation as a special case.
% The key condition is that the MFVB
% approximation provides good estimates of a select subset of posterior means---an
% assumption that has been shown to hold in many practical settings.
In the experiments, I demonstrate that my methods are simple, general, and
fast, providing accurate posterior uncertainty estimates and robustness measures
with runtimes that can be an order of magnitude faster than MCMC, including
models from ecology \citep{kery:2011:bayesian}, the social sciences
\citep{gelman:2006:arm}, and on a massive internet advertising dataset
\citep{criteo:2014:dataset}.



\paragraph{Data ablation.}

In \citet{giordano:2020:amip}, I propose a method to assess the sensitivity of
statistical analyses to the removal of a small fraction of the sample. Analyzing
all possible data subsets of a certain size is computationally prohibitive, so I
provide a finite-sample metric to approximately compute the number (or fraction)
of observations that has the greatest influence on a given result when dropped.
I provide explicit finite-sample error bounds on my approximation for linear
and instrumental variables regressions.
% At minimal computational cost, the
% metric provides an exact finite-sample lower bound on sensitivity for any
% estimator, so any non-robustness my metric finds is conclusive.
I demonstrate
that non-robustness to data ablation is driven by a low signal-to-noise ratio in
the inference problem, is not reflected in standard errors, does not disappear
asymptotically, and is not a product of misspecification.

The approximation is automatically computable and works for common estimators
(including OLS, IV, GMM, MLE, and variational Bayes), and I provide an
easy-to-use \texttt{R} package to compute the approximation
\citep{zaminfluence}. Several empirical applications based on published
econometric analyses \citep{angelucci:2009:indirect, finkelstein:2012:oregon,
meager:2019:microcredit} show that even 2-parameter linear regression analyses
of randomized trials can be highly sensitive. While I find some applications are
robust, in others the sign of a treatment effect can be changed by dropping less
than 1\% of the sample even when standard errors are small.


\paragraph{Frequentist variability of Bayesian posteriors.}

The frequentist (i.e., sampling) variance of Bayesian posterior expectations
differs in general from the posterior variance even for large datasets,
particularly when the model is misspecified or contains many latent variables
\citep{kleijn:2006:misspecification}.
Knowing the frequentist variance of a posterior expectation can be useful even
to a committed Bayesian, particularly when the data is known to arise from
random sampling and there is a possibility of model misspecification
\citep{waddell:2002:bayesphyloboot}.  However, the
principal existing approach for computing the frequentist variability from MCMC
procedures is the bootstrap, which can be extremely computationally intensive
due to the need to run hundreds of extra MCMC procedures
\citep{huggins:2019:bayesbag}.

In \citep{giordano:2020:bayesij, giordano:2020:stanconbayesij}, I propose an
efficient alternative to bootstrapping an MCMC procedure which is based on the
influence function from sensitivity analysis.  Using results from
\citep{giordano:2018:covariances, giordano:2019:ij}, I show that the influence
function for posterior expectations can be easily computed from the posterior
samples of a single MCMC procedure and consistently estimates the bootstrap
variance. I demonstrate the accuracy and computational benefits of the influence
function variance estimates on array of experiments including an election
forecasting model \citep{economist:2020:election}, the Cormack-Jolly-Seber model
from ecology \citep{kery:2011:bayesian}, and a large collection of models and
datasets from the social sciences \citep{gelman:2006:arm}.



\subsection*{Selected Future work}

My research is ideally driven by the needs of my scientific and industry
collaborators, and so I expect my future work will be determined to a large part
by my colleagues.  However, I will now discuss a few directions that I find
promising and interesting, and which I believe could be applicable to a diverse
set of problems.

\paragraph{The higher-order infinitesimal jackknife for the bootstrap.}

In the preprint \citet{giordano2019:hoij}, I extend \citet{giordano:2019:ij} to
higher-order Taylor series approximations, providing a family of estimators
which I collectively call the higher-order infinitesimal jackknife (HOIJ).  In
addition to providing higher-quality approximations to CV and extending the
results to k-fold CV, the higher-order approach promises to provide a scalable
alternative to the bootstrap, a procedure that estimates frequentist variability
by repeatedly re-evaluating a model at datasets drawn with replacement from the
observed data. The bootstrap is known to enjoy higher-order accuracy in certain
circumstances \citet{hall:2013:bootstrap}, and the HOIJ can approach the
bootstrap at a rate faster than the bootstrap approaches the truth.  The HOIJ
thus promises to make bootstrap inference available to models which are
differentiable but too expensive to re-evaluate (e.g. simulation-based models
\citep[Section 2.6]{baker:2019:workshop}), but also to allow efficient
bootstrap-after-bootstrap procedures which that are currently out of reach for
all but the simplest statistics \citep{efron:1994:bootstrap}.


% \paragraph{Sensitivity for non-differentiable preprocessing.}
%
% Analyses in genomics often begin with a pre-processing step in observation units
% are clustered together according to ad-hoc measures of similarity across a large
% number of feature vectors \citep{xu:2015:identification,
% stuart:2019:comprehensive}.  Quickly assessing the sensitivity of such
% procedures to the inclusion or exclusion of individual features would allow the
% researcher to identify high-leverage observations and avoid imposing structure
% via arbitrary modeling assumptions.  However, ordinary sensitivity analysis
% cannot be applied directly to the clustering step, which is typically
% non-differentiable.  With a colleague from biology, I am currently investigating
% an importance sampling technique that would allow us to apply sensitivity
% analysis using a continuous relaxation only of the distance measure, while
% retaining the non-differentiable clustering step. Ideally, this sensitivity
% analysis would allow for quick exploration of the high-dimensional space of
% feature inclusion, similar to our work in \citet{giordano:2020:amip}.

% With a colleague from biology, I am currently investigating a technique that
% would use importance sampling to compute the sensitivity of such a
% non-differentiable pre-processing step.  We first form a probabilistic
% relaxation only of the similarity measures, and then run the non-differentiable
% clustering for an ensemble of Monte Carlo samples of the similarities.  Removing
% a feature would change the probabilities of the similarity measure draws.  By
% differentiating the importance sampling estimate of the effect of the changing
% probabilities, we can form black-box sensitivity measures with little extra
% computation other than clustering the original similarity ensemble.  Ideally,
% this sensitivity analysis would allow for quick exploration of the
% high-dimensional space of feature inclusion, similar to our work in
% \citet{giordano:2020:amip}.

\paragraph{Scaling sensitivity measures.}

Sensitivity analysis typically avoids the expense of re-fitting a model, but
incurs the expense of solving one or several linear systems.  Thus, extending
the benefits of the sensitivity analysis to increasingly large scientific
problems requires developing methods to efficiently solve correspondingly large
linear systems.  Stochastic second-order methods are currently an active
research topic in optimization \citep{agarwal:2017:secondorder,
berahas:2020:newtonsketch}, and methods developed therein should apply
directly to sensitivity analysis.  I believe these methods would be most
fruitfully explored in the context of a particular application, e.g.
the production of astronomical catalogs, which I will now discuss.

\paragraph{Partitioned Bayesian inference.}

The ideas of \citep{giordano:2018:covariances} can be naturally extended to
approximately propagate uncertainty among separately estimated components of an
inference problem.  For example, astronomical catalogs are customarily produced
with MFVB-like algorithms \citep{lang:2016:tractor, regier:2019:cataloging},
which take inputs such as the sky background and optical point spread function
as fixed inputs, though these quantities are themselves inferred with
uncertainty.  Viewing all the separate inference procedures as a sequential
quasi-MFVB objective, one could directly apply the techniques of
\citep{giordano:2018:covariances} to propagate the uncertainty from the modeling
inputs to the astronomical catalog's uncertainty.



\newpage

\bibliography{references}
\bibliographystyle{plainnat}

\end{document}
