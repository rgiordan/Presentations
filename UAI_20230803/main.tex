\documentclass[8pt]{beamer}\usepackage[]{graphicx}\usepackage[]{color}

\input{_headers}

% \usepackage{emoji}


\def\red#1{\textcolor{red}{#1}}
\def\blue#1{\textcolor{blue}{#1}}
\def\green#1{\textcolor{violet}{#1}}

\def\undernote#1#2{\underbrace{#1}_{\mathclap{\substack{#2}}}}
\def\overnote#1#2{\overbrace{#1}^{\mathclap{\substack{#2}}}}

\DeclareMathOperator*{\argmax}{\mathrm{argmax}}
\DeclareMathOperator*{\argmin}{\mathrm{argmin}}
\DeclareMathOperator*{\esssup}{\mathrm{esssup}}
\DeclareMathOperator*{\essinf}{\mathrm{essinf}}
\DeclareMathOperator*{\argsup}{\mathrm{argsup}}
\DeclareMathOperator*{\arginf}{\mathrm{arginf}}

\newcommand{\fracat}[3]{\left. \frac{#1}{#2} \right|_{#3}}

\def\expect#1#2{\underset{#1}{\mathbb{E}}\left[#2\right]}
\def\sumn{\sum_{n=1}^{N}}
\def\meann{\frac{1}{N}\sumn}
\def\var#1#2{\underset{#1}{\mathrm{Var}}\left(#2\right)}
\def\kl#1#2{\mathrm{KL}\left(#1 || #2\right)}

\def\qdom{\mathcal{Q}}
\def\qstar{q^*}
\def\q{q}
\def\p{p}

\setbeamercolor{block title}{bg=cyan, fg=white}
\setbeamercolor{block body}{bg=cyan!10}
\setbeamertemplate{blocks}[rounded][shadow=true]

\begin{document}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Discussion of ``The Shrinkage-Delinkage Trade-off''}

Restricted variational families (mean field) can lead to poor posterior approximations.

Two very common responses to this problem are:
%
\begin{itemize}
\item ``Machine learning'': Ignore it (evaluate using some other criteria, like prediction)
\item ``Modern VI'': Use more expressive families (at a computational cost)
\end{itemize}
%
This paper tries to understand \emph{how} the restricted family goes wrong.  *fire emoji*

\pause
% \vspace{1em}
% \hrulefill
\hrulefill
% \vspace{1em}

This paper's most (initially) surprising conclusion is probably this:

\begin{block}{Theorem 3.6, paraphrased}
%
Let the target distribution have the constant $\varepsilon$-correlation matrix.\\
% $\Sigma_\varepsilon$.\\
As the dimension $n$ of the matrix goes to infinity:
%
\begin{itemize}
\item Each marginal mean field variance is wrong by $\varepsilon$
\item The per-component entropy gap $\rightarrow 0$
\end{itemize}
%
\end{block}

% Initial reaction:
% \textbf{How can we possibly estimate the entropy but not the variance?}

\pause
The key is ``per-component entropy gap'' means \textbf{entropy difference / $n$}.

% It does not mean \textbf{entropy of every marginal}.

In fact, the entropy gap is $O(\log n) \rightarrow \infty$.
\textbf{Why is $n$ the ``right'' scaling?}
% $\Sigma_\varepsilon$ give
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% \begin{align*}
% % $
% \textrm{Entropy gap } = \log \left( \varepsilon (n - 1) + 1 \right) - \log(1 - \varepsilon)
%  = O(\log n)
% %  $
% \end{align*}
%

\pause
Why do the relative values across dimensions of the entropy gap matter?

% \textbf{Comparing entropies across dimensions is not (typically) meaningful.}
\textbf{It's clear why variance is useful.  Less so the entropy gap,
especially as $n$ changes.}

% \textbf{Comparing entropies across dimensions is usually taboo for good reason!}



\end{frame}





% \begin{frame}{References}

%     \footnotesize
    
%     \bibliographystyle{plainnat}
%     % Hide the references header
%     % https://tex.stackexchange.com/questions/22645/hiding-the-title-of-the-bibliography/370784
%     \begingroup
%     \renewcommand{\section}[2]{}%
%     \bibliography{references}
%     \endgroup
    
%     %
% \end{frame}

\end{document}
